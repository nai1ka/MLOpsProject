[[34m2024-07-05T22:04:56.228+0000[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2024-07-05T22:04:56.231+0000[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-07-05T22:04:56.241+0000[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 1563[0m
[[34m2024-07-05T22:04:56.252+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:04:56.274+0000[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-07-05T22:04:56.432+0000] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-07-05T22:05:02.717+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:13:00+00:00, run_after=2022-01-01T00:14:00+00:00[0m
[[34m2024-07-05T22:05:02.810+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:10:00+00:00: scheduled__2022-01-01T00:10:00+00:00, state:running, queued_at: 2024-07-05 21:56:29.095523+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:05:02.812+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:10:00+00:00, run_id=scheduled__2022-01-01T00:10:00+00:00, run_start_date=2024-07-05 21:56:29.138287+00:00, run_end_date=2024-07-05 22:05:02.812260+00:00, run_duration=513.673973, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:10:00+00:00, data_interval_end=2022-01-01 00:11:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:05:02.820+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:11:00+00:00, run_after=2022-01-01T00:12:00+00:00[0m
[[34m2024-07-05T22:05:02.854+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:11:00+00:00 [scheduled]>
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:12:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:02.855+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG hello_world has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:05:02.855+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG hello_world has 1/16 running and queued tasks[0m
[[34m2024-07-05T22:05:02.856+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:11:00+00:00 [scheduled]>
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:12:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:02.861+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:05:02.862+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:11:00+00:00', try_number=2, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:05:02.863+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:11:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:02.863+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:12:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:05:02.863+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:12:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:02.872+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:11:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:04.401+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/hello_dag.py[0m
[[34m2024-07-05T22:05:08.756+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:11:00+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:05:10.387+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:12:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:11.899+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/hello_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=hello_world/run_id=scheduled__2022-01-01T00:12:00+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:05:16.147+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:12:00+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:05:17.682+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:11:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-07-05T22:05:17.682+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:12:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:05:17.697+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=validate_data, run_id=scheduled__2022-01-01T00:11:00+00:00, map_index=-1, run_start_date=2024-07-05 22:05:08.857707+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=2, max_tries=0, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:05:02.857226+00:00, queued_by_job_id=33, pid=1618[0m
[[34m2024-07-05T22:05:17.698+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=validate_data, run_id=scheduled__2022-01-01T00:12:00+00:00, map_index=-1, run_start_date=2024-07-05 22:05:16.243024+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:05:02.857226+00:00, queued_by_job_id=33, pid=1630[0m
[[34m2024-07-05T22:05:17.733+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (2) jobs without heartbeat after 2024-07-05 22:00:17.723993+00:00[0m
[[34m2024-07-05T22:05:17.734+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/hello_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'hello_world', 'Task Id': 'validate_data', 'Run Id': 'scheduled__2022-01-01T00:11:00+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff784673610>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:05:17.741+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/hello_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'hello_world', 'Task Id': 'validate_data', 'Run Id': 'scheduled__2022-01-01T00:12:00+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff78417af10>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:05:22.115+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:14:00+00:00, run_after=2022-01-01T00:15:00+00:00[0m
[[34m2024-07-05T22:05:22.239+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:13:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:22.245+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG hello_world has 1/16 running and queued tasks[0m
[[34m2024-07-05T22:05:22.246+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:13:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:22.251+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:05:22.252+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:13:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:05:22.252+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:13:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:22.257+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:13:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:25.753+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/hello_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=hello_world/run_id=scheduled__2022-01-01T00:13:00+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:05:40.204+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:13:00+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:05:45.089+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:13:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:05:45.129+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=validate_data, run_id=scheduled__2022-01-01T00:13:00+00:00, map_index=-1, run_start_date=2024-07-05 22:05:40.517244+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=36, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:05:22.247287+00:00, queued_by_job_id=33, pid=1725[0m
[[34m2024-07-05T22:05:45.194+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:00:45.181251+00:00[0m
[[34m2024-07-05T22:05:45.197+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/hello_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'hello_world', 'Task Id': 'validate_data', 'Run Id': 'scheduled__2022-01-01T00:13:00+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff78407dc10>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:05:47.499+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:15:00+00:00, run_after=2022-01-01T00:16:00+00:00[0m
[[34m2024-07-05T22:05:47.813+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:11:00+00:00: scheduled__2022-01-01T00:11:00+00:00, state:running, queued_at: 2024-07-05 21:56:50.867933+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:05:47.814+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:11:00+00:00, run_id=scheduled__2022-01-01T00:11:00+00:00, run_start_date=2024-07-05 21:56:51.040005+00:00, run_end_date=2024-07-05 22:05:47.814726+00:00, run_duration=536.774721, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:11:00+00:00, data_interval_end=2022-01-01 00:12:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:05:47.862+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:12:00+00:00, run_after=2022-01-01T00:13:00+00:00[0m
[[34m2024-07-05T22:05:47.970+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:14:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:47.972+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG hello_world has 1/16 running and queued tasks[0m
[[34m2024-07-05T22:05:47.972+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:14:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:05:47.996+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:05:47.997+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:14:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:05:48.010+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:14:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:48.027+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:14:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:05:53.334+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/hello_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=hello_world/run_id=scheduled__2022-01-01T00:14:00+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:06:05.870+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:14:00+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:06:07.556+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:14:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:06:07.578+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=validate_data, run_id=scheduled__2022-01-01T00:14:00+00:00, map_index=-1, run_start_date=2024-07-05 22:06:05.972212+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:05:47.985860+00:00, queued_by_job_id=33, pid=1818[0m
[[34m2024-07-05T22:06:07.644+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:01:07.635140+00:00[0m
[[34m2024-07-05T22:06:07.649+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/hello_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'hello_world', 'Task Id': 'validate_data', 'Run Id': 'scheduled__2022-01-01T00:14:00+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff78417af10>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:06:15.141+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:16:00+00:00, run_after=2022-01-01T00:17:00+00:00[0m
[[34m2024-07-05T22:06:15.196+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:12:00+00:00: scheduled__2022-01-01T00:12:00+00:00, state:running, queued_at: 2024-07-05 22:05:02.686679+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:06:15.197+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:12:00+00:00, run_id=scheduled__2022-01-01T00:12:00+00:00, run_start_date=2024-07-05 22:05:02.743384+00:00, run_end_date=2024-07-05 22:06:15.197578+00:00, run_duration=72.454194, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:12:00+00:00, data_interval_end=2022-01-01 00:13:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:06:15.206+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for hello_world to 2022-01-01T00:13:00+00:00, run_after=2022-01-01T00:14:00+00:00[0m
[[34m2024-07-05T22:06:15.224+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:15:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:06:15.226+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG hello_world has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:06:15.226+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:15:00+00:00 [scheduled]>[0m
[[34m2024-07-05T22:06:15.230+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:06:15.231+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:15:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:06:15.231+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:06:15.236+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'hello_world', 'validate_data', 'scheduled__2022-01-01T00:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/hello_dag.py'][0m
[[34m2024-07-05T22:06:18.625+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/hello_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=hello_world/run_id=scheduled__2022-01-01T00:15:00+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:06:23.710+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: hello_world.validate_data scheduled__2022-01-01T00:15:00+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:06:25.467+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='hello_world', task_id='validate_data', run_id='scheduled__2022-01-01T00:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:06:25.479+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=hello_world, task_id=validate_data, run_id=scheduled__2022-01-01T00:15:00+00:00, map_index=-1, run_start_date=2024-07-05 22:06:23.912354+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:06:15.227387+00:00, queued_by_job_id=33, pid=1853[0m
[[34m2024-07-05T22:06:25.513+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:01:25.509165+00:00[0m
[[34m2024-07-05T22:06:25.515+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/hello_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'hello_world', 'Task Id': 'validate_data', 'Run Id': 'scheduled__2022-01-01T00:15:00+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff784090910>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:07:12.422+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:13:00+00:00: scheduled__2022-01-01T00:13:00+00:00, state:running, queued_at: 2024-07-05 22:05:22.094881+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:07:12.423+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:13:00+00:00, run_id=scheduled__2022-01-01T00:13:00+00:00, run_start_date=2024-07-05 22:05:22.137800+00:00, run_end_date=2024-07-05 22:07:12.423832+00:00, run_duration=110.286032, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:13:00+00:00, data_interval_end=2022-01-01 00:14:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:07:12.434+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:14:00+00:00: scheduled__2022-01-01T00:14:00+00:00, state:running, queued_at: 2024-07-05 22:05:47.464942+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:07:12.435+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:14:00+00:00, run_id=scheduled__2022-01-01T00:14:00+00:00, run_start_date=2024-07-05 22:05:47.559901+00:00, run_end_date=2024-07-05 22:07:12.435457+00:00, run_duration=84.875556, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:14:00+00:00, data_interval_end=2022-01-01 00:15:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:08:20.743+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun hello_world @ 2022-01-01 00:15:00+00:00: scheduled__2022-01-01T00:15:00+00:00, state:running, queued_at: 2024-07-05 22:06:15.125948+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:08:20.744+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=hello_world, execution_date=2022-01-01 00:15:00+00:00, run_id=scheduled__2022-01-01T00:15:00+00:00, run_start_date=2024-07-05 22:06:15.158699+00:00, run_end_date=2024-07-05 22:08:20.744485+00:00, run_duration=125.585786, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2022-01-01 00:15:00+00:00, data_interval_end=2022-01-01 00:16:00+00:00, dag_hash=a22d0d46f293d3c12025135c072bdde6[0m
[[34m2024-07-05T22:08:28.331+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:08:20.378919+00:00, run_after=2024-07-05T22:13:20.378919+00:00[0m
[[34m2024-07-05T22:08:28.415+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:03:20.378919+00:00 [scheduled]>
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:08:20.161913+00:00 [scheduled]>[0m
[[34m2024-07-05T22:08:28.416+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:08:28.417+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 1/16 running and queued tasks[0m
[[34m2024-07-05T22:08:28.417+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:03:20.378919+00:00 [scheduled]>
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:08:20.161913+00:00 [scheduled]>[0m
[[34m2024-07-05T22:08:28.423+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:08:28.423+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:08:28.424+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:03:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:08:28.424+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:03:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:28.426+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:08:20.161913+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:08:28.427+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:08:20.161913+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:28.434+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:03:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:31.007+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:03:20.378919+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:08:36.311+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:03:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:08:38.298+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:08:20.161913+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:40.343+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:08:20.161913+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:08:45.858+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:08:20.161913+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:08:48.372+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:03:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:08:48.374+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:08:20.161913+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:08:48.385+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:08:20.161913+00:00, map_index=-1, run_start_date=2024-07-05 22:08:45.958294+00:00, run_end_date=2024-07-05 22:08:46.483841+00:00, run_duration=0.525547, state=success, executor_state=success, try_number=1, max_tries=0, job_id=40, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:08:28.419053+00:00, queued_by_job_id=33, pid=2157[0m
[[34m2024-07-05T22:08:48.386+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:03:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:08:36.429741+00:00, run_end_date=2024-07-05 22:08:36.715593+00:00, run_duration=0.285852, state=success, executor_state=success, try_number=1, max_tries=0, job_id=39, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:08:28.419053+00:00, queued_by_job_id=33, pid=2146[0m
[[34m2024-07-05T22:08:48.484+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:03:20.378919+00:00 [scheduled]>
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:08:20.161913+00:00 [scheduled]>[0m
[[34m2024-07-05T22:08:48.485+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:08:48.485+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 1/16 running and queued tasks[0m
[[34m2024-07-05T22:08:48.486+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:03:20.378919+00:00 [scheduled]>
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:08:20.161913+00:00 [scheduled]>[0m
[[34m2024-07-05T22:08:48.491+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:08:48.491+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:08:48.492+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:03:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:08:48.492+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:03:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:48.493+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:08:20.161913+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:08:48.493+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:08:20.161913+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:48.500+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:03:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:08:50.613+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:03:20.378919+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:08:56.589+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:03:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:08:58.793+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:08:20.161913+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:09:00.519+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:08:20.161913+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:09:06.246+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:08:20.161913+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:09:08.579+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:03:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:09:08.581+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:08:20.161913+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:09:08.591+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=manual__2024-07-05T22:08:20.161913+00:00, map_index=-1, run_start_date=2024-07-05 22:09:06.342969+00:00, run_end_date=2024-07-05 22:09:06.775035+00:00, run_duration=0.432066, state=success, executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:08:48.487505+00:00, queued_by_job_id=33, pid=2244[0m
[[34m2024-07-05T22:09:08.592+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:03:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:08:56.682572+00:00, run_end_date=2024-07-05 22:08:56.954714+00:00, run_duration=0.272142, state=success, executor_state=success, try_number=1, max_tries=0, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:08:48.487505+00:00, queued_by_job_id=33, pid=2195[0m
[[34m2024-07-05T22:09:14.363+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:03:20.378919+00:00: scheduled__2024-07-05T22:03:20.378919+00:00, state:running, queued_at: 2024-07-05 22:08:28.322624+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:09:14.364+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:03:20.378919+00:00, run_id=scheduled__2024-07-05T22:03:20.378919+00:00, run_start_date=2024-07-05 22:08:28.351187+00:00, run_end_date=2024-07-05 22:09:14.364664+00:00, run_duration=46.013477, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:03:20.378919+00:00, data_interval_end=2024-07-05 22:08:20.378919+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:09:14.371+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:08:20.378919+00:00, run_after=2024-07-05T22:13:20.378919+00:00[0m
[[34m2024-07-05T22:09:14.378+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:08:20.161913+00:00: manual__2024-07-05T22:08:20.161913+00:00, state:running, queued_at: 2024-07-05 22:08:20.294673+00:00. externally triggered: True> successful[0m
[[34m2024-07-05T22:09:14.379+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:08:20.161913+00:00, run_id=manual__2024-07-05T22:08:20.161913+00:00, run_start_date=2024-07-05 22:08:28.351553+00:00, run_end_date=2024-07-05 22:09:14.379319+00:00, run_duration=46.027766, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:03:20.161913+00:00, data_interval_end=2024-07-05 22:08:20.161913+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:11:19.579+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:11:19.587+0000[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2024-07-05T22:11:20.619+0000] {manager.py:543} INFO - DAG hello_world is missing and will be deactivated.
[2024-07-05T22:11:20.620+0000] {manager.py:543} INFO - DAG data_extract_dag is missing and will be deactivated.
[2024-07-05T22:11:20.629+0000] {manager.py:553} INFO - Deactivated 2 DAGs which are no longer present in file.
[2024-07-05T22:11:20.638+0000] {manager.py:557} INFO - Deleted DAG hello_world in serialized_dag table
[2024-07-05T22:11:20.647+0000] {manager.py:557} INFO - Deleted DAG data_extract_dag in serialized_dag table
[[34m2024-07-05T22:13:21.341+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:13:20.378919+00:00, run_after=2024-07-05T22:18:20.378919+00:00[0m
[[34m2024-07-05T22:13:21.418+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:08:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:13:21.419+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:13:21.420+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:08:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:13:21.425+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:13:21.427+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:08:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:13:21.427+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:08:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:13:21.434+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:08:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:13:23.949+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:08:20.378919+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:13:28.976+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:08:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:13:30.987+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:08:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:13:31.003+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:08:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:13:29.110078+00:00, run_end_date=2024-07-05 22:13:29.392855+00:00, run_duration=0.282777, state=success, executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:13:21.421338+00:00, queued_by_job_id=33, pid=2609[0m
[[34m2024-07-05T22:13:31.101+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:08:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:13:31.101+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:13:31.102+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:08:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:13:31.107+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:13:31.108+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:08:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:13:31.109+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:08:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:13:31.115+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:08:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:13:32.790+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:08:20.378919+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:13:37.974+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:08:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:13:39.740+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:08:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:13:39.754+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:08:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:13:38.094938+00:00, run_end_date=2024-07-05 22:13:38.356420+00:00, run_duration=0.261482, state=success, executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:13:31.103499+00:00, queued_by_job_id=33, pid=2614[0m
[[34m2024-07-05T22:13:44.786+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:08:20.378919+00:00: scheduled__2024-07-05T22:08:20.378919+00:00, state:running, queued_at: 2024-07-05 22:13:21.326755+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:13:44.787+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:08:20.378919+00:00, run_id=scheduled__2024-07-05T22:08:20.378919+00:00, run_start_date=2024-07-05 22:13:21.357824+00:00, run_end_date=2024-07-05 22:13:44.787785+00:00, run_duration=23.429961, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:08:20.378919+00:00, data_interval_end=2024-07-05 22:13:20.378919+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:13:44.794+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:13:20.378919+00:00, run_after=2024-07-05T22:18:20.378919+00:00[0m
[[34m2024-07-05T22:16:19.638+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:18:26.060+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:18:20.378919+00:00, run_after=2024-07-05T22:23:20.378919+00:00[0m
[[34m2024-07-05T22:18:26.126+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:13:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:26.128+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:18:26.134+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:13:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:26.140+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:18:26.140+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:13:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:18:26.141+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:13:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:26.146+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:13:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:28.552+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:13:20.378919+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:18:33.970+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:13:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:18:36.052+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:13:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:18:36.084+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:13:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:18:34.096562+00:00, run_end_date=2024-07-05 22:18:34.479468+00:00, run_duration=0.382906, state=success, executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:18:26.135966+00:00, queued_by_job_id=33, pid=3124[0m
[[34m2024-07-05T22:18:41.056+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:13:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:41.057+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:18:41.058+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:13:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:41.063+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:18:41.065+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:13:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:18:41.065+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:13:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:41.072+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:13:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:43.219+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:13:20.378919+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:18:48.058+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:13:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:18:51.886+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:13:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:18:51.914+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:13:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:18:48.183910+00:00, run_end_date=2024-07-05 22:18:48.442619+00:00, run_duration=0.258709, state=success, executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:18:41.059537+00:00, queued_by_job_id=33, pid=3149[0m
[[34m2024-07-05T22:18:52.008+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:13:20.378919+00:00: scheduled__2024-07-05T22:13:20.378919+00:00, state:running, queued_at: 2024-07-05 22:18:26.042886+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:18:52.010+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:13:20.378919+00:00, run_id=scheduled__2024-07-05T22:13:20.378919+00:00, run_start_date=2024-07-05 22:18:26.080668+00:00, run_end_date=2024-07-05 22:18:52.010050+00:00, run_duration=25.929382, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:13:20.378919+00:00, data_interval_end=2024-07-05 22:18:20.378919+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:18:52.016+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:18:20.378919+00:00, run_after=2024-07-05T22:23:20.378919+00:00[0m
[[34m2024-07-05T22:18:55.405+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:18:54.709676+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:55.406+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:18:55.409+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:18:54.709676+00:00 [scheduled]>[0m
[[34m2024-07-05T22:18:55.443+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:18:55.444+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:18:54.709676+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:18:55.444+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:18:54.709676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:55.462+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:18:54.709676+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:18:58.386+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:18:54.709676+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:19:03.928+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:18:54.709676+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:19:05.756+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:18:54.709676+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:19:05.771+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:18:54.709676+00:00, map_index=-1, run_start_date=2024-07-05 22:19:04.090545+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:18:55.422270+00:00, queued_by_job_id=33, pid=3164[0m
[[34m2024-07-05T22:19:05.806+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:14:05.801936+00:00[0m
[[34m2024-07-05T22:19:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/data_extract_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'data_extract_dag', 'Task Id': 'extract_data', 'Run Id': 'manual__2024-07-05T22:18:54.709676+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff783f4ef50>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:19:16.649+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:18:54.709676+00:00: manual__2024-07-05T22:18:54.709676+00:00, state:running, queued_at: 2024-07-05 22:18:54.726874+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:19:16.651+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:18:54.709676+00:00, run_id=manual__2024-07-05T22:18:54.709676+00:00, run_start_date=2024-07-05 22:18:55.225908+00:00, run_end_date=2024-07-05 22:19:16.651115+00:00, run_duration=21.425207, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:13:54.709676+00:00, data_interval_end=2024-07-05 22:18:54.709676+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:19:42.312+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:19:38.667872+00:00 [scheduled]>[0m
[[34m2024-07-05T22:19:42.315+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:19:42.315+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:19:38.667872+00:00 [scheduled]>[0m
[[34m2024-07-05T22:19:42.320+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:19:42.321+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:19:38.667872+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:19:42.322+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:19:38.667872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:19:42.327+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:19:38.667872+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:19:44.863+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:19:38.667872+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:19:49.878+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:19:38.667872+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:19:51.960+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:19:38.667872+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:19:51.981+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:19:38.667872+00:00, map_index=-1, run_start_date=2024-07-05 22:19:50.050158+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:19:42.316780+00:00, queued_by_job_id=33, pid=3214[0m
[[34m2024-07-05T22:19:52.019+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:14:52.014365+00:00[0m
[[34m2024-07-05T22:19:52.021+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/data_extract_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'data_extract_dag', 'Task Id': 'extract_data', 'Run Id': 'manual__2024-07-05T22:19:38.667872+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff784090810>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:20:02.910+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:19:38.667872+00:00: manual__2024-07-05T22:19:38.667872+00:00, state:running, queued_at: 2024-07-05 22:19:38.700369+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:20:02.911+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:19:38.667872+00:00, run_id=manual__2024-07-05T22:19:38.667872+00:00, run_start_date=2024-07-05 22:19:42.254893+00:00, run_end_date=2024-07-05 22:20:02.911577+00:00, run_duration=20.656684, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:14:38.667872+00:00, data_interval_end=2024-07-05 22:19:38.667872+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:21:19.690+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:23:22.058+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:23:20.378919+00:00, run_after=2024-07-05T22:28:20.378919+00:00[0m
[[34m2024-07-05T22:23:22.116+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:18:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:23:22.116+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:23:22.117+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:18:20.378919+00:00 [scheduled]>[0m
[[34m2024-07-05T22:23:22.122+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:23:22.122+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:18:20.378919+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:23:22.123+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:18:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:23:22.134+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:18:20.378919+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:23:24.395+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:18:20.378919+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:23:29.403+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:18:20.378919+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:23:31.150+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:18:20.378919+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:23:31.163+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:18:20.378919+00:00, map_index=-1, run_start_date=2024-07-05 22:23:29.522768+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:23:22.118521+00:00, queued_by_job_id=33, pid=3474[0m
[[34m2024-07-05T22:23:31.197+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:18:31.193372+00:00[0m
[[34m2024-07-05T22:23:31.198+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/data_extract_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'data_extract_dag', 'Task Id': 'extract_data', 'Run Id': 'scheduled__2024-07-05T22:18:20.378919+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff784011390>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:23:40.475+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:18:20.378919+00:00: scheduled__2024-07-05T22:18:20.378919+00:00, state:running, queued_at: 2024-07-05 22:23:22.046345+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:23:40.477+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:18:20.378919+00:00, run_id=scheduled__2024-07-05T22:18:20.378919+00:00, run_start_date=2024-07-05 22:23:22.075129+00:00, run_end_date=2024-07-05 22:23:40.477210+00:00, run_duration=18.402081, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:18:20.378919+00:00, data_interval_end=2024-07-05 22:23:20.378919+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:23:40.483+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:23:20.378919+00:00, run_after=2024-07-05T22:28:20.378919+00:00[0m
[[34m2024-07-05T22:24:38.004+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:24:36.430331+00:00 [scheduled]>[0m
[[34m2024-07-05T22:24:38.005+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:24:38.006+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:24:36.430331+00:00 [scheduled]>[0m
[[34m2024-07-05T22:24:38.011+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:24:38.012+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:24:36.430331+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:24:38.012+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:24:36.430331+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:24:38.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:24:36.430331+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:24:40.270+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:24:36.430331+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:24:45.723+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:24:36.430331+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:24:47.935+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:24:36.430331+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:24:47.961+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:24:36.430331+00:00, map_index=-1, run_start_date=2024-07-05 22:24:45.823399+00:00, run_end_date=2024-07-05 22:24:46.201124+00:00, run_duration=0.377725, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:24:38.007519+00:00, queued_by_job_id=33, pid=3585[0m
[[34m2024-07-05T22:24:53.848+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:24:36.430331+00:00: manual__2024-07-05T22:24:36.430331+00:00, state:running, queued_at: 2024-07-05 22:24:36.453489+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:24:53.850+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:24:36.430331+00:00, run_id=manual__2024-07-05T22:24:36.430331+00:00, run_start_date=2024-07-05 22:24:37.955493+00:00, run_end_date=2024-07-05 22:24:53.850750+00:00, run_duration=15.895257, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:19:36.430331+00:00, data_interval_end=2024-07-05 22:24:36.430331+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:25:09.556+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:25:05.436003+00:00 [scheduled]>[0m
[[34m2024-07-05T22:25:09.558+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:25:09.558+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:25:05.436003+00:00 [scheduled]>[0m
[[34m2024-07-05T22:25:09.563+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:25:09.564+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:25:05.436003+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:25:09.565+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:25:05.436003+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:25:09.571+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:25:05.436003+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:25:11.756+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:25:05.436003+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:25:17.471+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:25:05.436003+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:25:20.068+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:25:05.436003+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:25:20.094+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:25:05.436003+00:00, map_index=-1, run_start_date=2024-07-05 22:25:17.584708+00:00, run_end_date=2024-07-05 22:25:18.347772+00:00, run_duration=0.763064, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:25:09.559541+00:00, queued_by_job_id=33, pid=3632[0m
[[34m2024-07-05T22:25:20.263+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:25:05.436003+00:00: manual__2024-07-05T22:25:05.436003+00:00, state:running, queued_at: 2024-07-05 22:25:05.495349+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:25:20.264+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:25:05.436003+00:00, run_id=manual__2024-07-05T22:25:05.436003+00:00, run_start_date=2024-07-05 22:25:09.443910+00:00, run_end_date=2024-07-05 22:25:20.264206+00:00, run_duration=10.820296, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:20:05.436003+00:00, data_interval_end=2024-07-05 22:25:05.436003+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:26:22.221+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:27:46.973+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:27:43.371323+00:00 [scheduled]>[0m
[[34m2024-07-05T22:27:46.974+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:27:46.975+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:27:43.371323+00:00 [scheduled]>[0m
[[34m2024-07-05T22:27:46.980+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:27:46.982+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:27:43.371323+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:27:46.982+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:27:43.371323+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:27:46.988+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:27:43.371323+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:27:49.258+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:27:43.371323+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:27:54.153+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:27:43.371323+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:27:56.093+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:27:43.371323+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:27:56.106+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:27:43.371323+00:00, map_index=-1, run_start_date=2024-07-05 22:27:54.274845+00:00, run_end_date=2024-07-05 22:27:54.584067+00:00, run_duration=0.309222, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:27:46.976064+00:00, queued_by_job_id=33, pid=3912[0m
[[34m2024-07-05T22:28:01.143+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:27:43.371323+00:00: manual__2024-07-05T22:27:43.371323+00:00, state:running, queued_at: 2024-07-05 22:27:43.423319+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:28:01.144+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:27:43.371323+00:00, run_id=manual__2024-07-05T22:27:43.371323+00:00, run_start_date=2024-07-05 22:27:46.927800+00:00, run_end_date=2024-07-05 22:28:01.144327+00:00, run_duration=14.216527, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:22:43.371323+00:00, data_interval_end=2024-07-05 22:27:43.371323+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:28:23.616+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:28:22.411869+00:00, run_after=2024-07-05T22:33:22.411869+00:00[0m
[[34m2024-07-05T22:28:23.685+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:23:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:28:23.690+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:28:23.691+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:23:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:28:23.696+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:28:23.697+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:23:22.411869+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:28:23.697+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:23:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:28:23.703+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:23:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:28:25.954+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:23:22.411869+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:28:30.938+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:23:22.411869+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:28:32.928+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:23:22.411869+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:28:32.955+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:23:22.411869+00:00, map_index=-1, run_start_date=2024-07-05 22:28:31.052878+00:00, run_end_date=2024-07-05 22:28:31.415884+00:00, run_duration=0.363006, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:28:23.692287+00:00, queued_by_job_id=33, pid=3968[0m
[[34m2024-07-05T22:28:37.392+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:23:22.411869+00:00: scheduled__2024-07-05T22:23:22.411869+00:00, state:running, queued_at: 2024-07-05 22:28:23.608546+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:28:37.393+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:23:22.411869+00:00, run_id=scheduled__2024-07-05T22:23:22.411869+00:00, run_start_date=2024-07-05 22:28:23.643379+00:00, run_end_date=2024-07-05 22:28:37.393420+00:00, run_duration=13.750041, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:23:22.411869+00:00, data_interval_end=2024-07-05 22:28:22.411869+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:28:37.399+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:28:22.411869+00:00, run_after=2024-07-05T22:33:22.411869+00:00[0m
[[34m2024-07-05T22:31:24.884+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:31:32.915+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:31:31.024410+00:00 [scheduled]>[0m
[[34m2024-07-05T22:31:32.921+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:31:32.922+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:31:31.024410+00:00 [scheduled]>[0m
[[34m2024-07-05T22:31:32.927+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:31:32.928+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:31:31.024410+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:31:32.928+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:31:31.024410+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:31:32.937+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:31:31.024410+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:31:35.555+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:31:31.024410+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:31:40.727+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:31:31.024410+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:31:42.951+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:31:31.024410+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:31:42.965+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:31:31.024410+00:00, map_index=-1, run_start_date=2024-07-05 22:31:40.837635+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:31:32.923716+00:00, queued_by_job_id=33, pid=4189[0m
[[34m2024-07-05T22:31:42.995+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:26:42.990359+00:00[0m
[[34m2024-07-05T22:31:42.996+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/data_extract_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'data_extract_dag', 'Task Id': 'extract_data', 'Run Id': 'manual__2024-07-05T22:31:31.024410+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff78419f890>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:31:49.495+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:31:31.024410+00:00: manual__2024-07-05T22:31:31.024410+00:00, state:running, queued_at: 2024-07-05 22:31:31.059231+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:31:49.497+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:31:31.024410+00:00, run_id=manual__2024-07-05T22:31:31.024410+00:00, run_start_date=2024-07-05 22:31:32.830145+00:00, run_end_date=2024-07-05 22:31:49.497529+00:00, run_duration=16.667384, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:26:31.024410+00:00, data_interval_end=2024-07-05 22:31:31.024410+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:33:23.948+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:33:22.411869+00:00, run_after=2024-07-05T22:38:22.411869+00:00[0m
[[34m2024-07-05T22:33:24.032+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:28:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:33:24.033+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:33:24.034+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:28:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:33:24.039+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:33:24.041+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:28:22.411869+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:33:24.041+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:28:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:33:24.047+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:28:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:33:26.016+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:28:22.411869+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:33:31.463+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:28:22.411869+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:33:35.250+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:28:22.411869+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:33:35.275+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:28:22.411869+00:00, map_index=-1, run_start_date=2024-07-05 22:33:31.731495+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:33:24.035403+00:00, queued_by_job_id=33, pid=4520[0m
[[34m2024-07-05T22:33:35.315+0000[0m] {[34mscheduler_job_runner.py:[0m1736} WARNING[0m - Failing (1) jobs without heartbeat after 2024-07-05 22:28:35.311256+00:00[0m
[[34m2024-07-05T22:33:35.322+0000[0m] {[34mscheduler_job_runner.py:[0m1746} ERROR[0m - Detected zombie job: {'full_filepath': '/root/MLOpsProject/services/airflow/dags/data_extract_dag.py', 'processor_subdir': '/root/MLOpsProject/services/airflow/dags', 'msg': "{'DAG Id': 'data_extract_dag', 'Task Id': 'extract_data', 'Run Id': 'scheduled__2024-07-05T22:28:22.411869+00:00', 'Hostname': 'ice-lazurite960'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7ff783f77fd0>, 'is_failure_callback': True}[0m
[[34m2024-07-05T22:33:42.825+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:28:22.411869+00:00: scheduled__2024-07-05T22:28:22.411869+00:00, state:running, queued_at: 2024-07-05 22:33:23.935704+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:33:42.827+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:28:22.411869+00:00, run_id=scheduled__2024-07-05T22:28:22.411869+00:00, run_start_date=2024-07-05 22:33:23.967994+00:00, run_end_date=2024-07-05 22:33:42.827696+00:00, run_duration=18.859702, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:28:22.411869+00:00, data_interval_end=2024-07-05 22:33:22.411869+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:33:42.834+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:33:22.411869+00:00, run_after=2024-07-05T22:38:22.411869+00:00[0m
[[34m2024-07-05T22:36:24.940+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:36:33.289+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:36:32.189637+00:00 [scheduled]>[0m
[[34m2024-07-05T22:36:33.297+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:36:33.297+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:36:32.189637+00:00 [scheduled]>[0m
[[34m2024-07-05T22:36:33.320+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:36:33.321+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:36:32.189637+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:36:33.322+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:36:32.189637+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:36:33.330+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:36:32.189637+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:36:35.544+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:36:32.189637+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:36:40.859+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:36:32.189637+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:36:42.783+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:36:32.189637+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:36:42.801+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:36:32.189637+00:00, map_index=-1, run_start_date=2024-07-05 22:36:41.007373+00:00, run_end_date=2024-07-05 22:36:41.300711+00:00, run_duration=0.293338, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=56, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:36:33.298991+00:00, queued_by_job_id=33, pid=4863[0m
[[34m2024-07-05T22:36:42.896+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:36:32.189637+00:00: manual__2024-07-05T22:36:32.189637+00:00, state:running, queued_at: 2024-07-05 22:36:32.222952+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:36:42.898+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:36:32.189637+00:00, run_id=manual__2024-07-05T22:36:32.189637+00:00, run_start_date=2024-07-05 22:36:33.043983+00:00, run_end_date=2024-07-05 22:36:42.897857+00:00, run_duration=9.853874, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:31:32.189637+00:00, data_interval_end=2024-07-05 22:36:32.189637+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:37:38.946+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:37:33.304505+00:00 [scheduled]>[0m
[[34m2024-07-05T22:37:38.948+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:37:38.948+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:37:33.304505+00:00 [scheduled]>[0m
[[34m2024-07-05T22:37:38.953+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:37:38.955+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:37:33.304505+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:37:38.956+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:37:33.304505+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:37:38.962+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:37:33.304505+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:37:40.970+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:37:33.304505+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:37:45.858+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:37:33.304505+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:37:47.788+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:37:33.304505+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:37:47.803+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:37:33.304505+00:00, map_index=-1, run_start_date=2024-07-05 22:37:45.961592+00:00, run_end_date=2024-07-05 22:37:46.293545+00:00, run_duration=0.331953, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=57, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:37:38.949722+00:00, queued_by_job_id=33, pid=4948[0m
[[34m2024-07-05T22:37:47.886+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:37:33.304505+00:00: manual__2024-07-05T22:37:33.304505+00:00, state:running, queued_at: 2024-07-05 22:37:33.387300+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:37:47.887+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:37:33.304505+00:00, run_id=manual__2024-07-05T22:37:33.304505+00:00, run_start_date=2024-07-05 22:37:38.897018+00:00, run_end_date=2024-07-05 22:37:47.887680+00:00, run_duration=8.990662, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:32:33.304505+00:00, data_interval_end=2024-07-05 22:37:33.304505+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:38:23.066+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:38:22.411869+00:00, run_after=2024-07-05T22:43:22.411869+00:00[0m
[[34m2024-07-05T22:38:23.184+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:33:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:38:23.185+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:38:23.186+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:33:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:38:23.190+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:38:23.191+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:33:22.411869+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:38:23.192+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:33:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:38:23.197+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:33:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:38:25.209+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:33:22.411869+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:38:30.248+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:33:22.411869+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:38:32.125+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:33:22.411869+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:38:32.142+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:33:22.411869+00:00, map_index=-1, run_start_date=2024-07-05 22:38:30.362380+00:00, run_end_date=2024-07-05 22:38:30.692931+00:00, run_duration=0.330551, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=58, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:38:23.187229+00:00, queued_by_job_id=33, pid=5013[0m
[[34m2024-07-05T22:38:37.245+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:33:22.411869+00:00: scheduled__2024-07-05T22:33:22.411869+00:00, state:running, queued_at: 2024-07-05 22:38:23.038398+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:38:37.247+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:33:22.411869+00:00, run_id=scheduled__2024-07-05T22:33:22.411869+00:00, run_start_date=2024-07-05 22:38:23.102968+00:00, run_end_date=2024-07-05 22:38:37.247062+00:00, run_duration=14.144094, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:33:22.411869+00:00, data_interval_end=2024-07-05 22:38:22.411869+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:38:37.253+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:38:22.411869+00:00, run_after=2024-07-05T22:43:22.411869+00:00[0m
[[34m2024-07-05T22:38:45.119+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:38:40.198767+00:00 [scheduled]>[0m
[[34m2024-07-05T22:38:45.120+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:38:45.121+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:38:40.198767+00:00 [scheduled]>[0m
[[34m2024-07-05T22:38:45.126+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:38:45.127+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:38:40.198767+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:38:45.127+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:38:40.198767+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:38:45.134+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:38:40.198767+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:38:47.183+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:38:40.198767+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:38:52.525+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:38:40.198767+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:38:54.418+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:38:40.198767+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:38:54.432+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:38:40.198767+00:00, map_index=-1, run_start_date=2024-07-05 22:38:52.642349+00:00, run_end_date=2024-07-05 22:38:52.978347+00:00, run_duration=0.335998, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=59, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:38:45.122394+00:00, queued_by_job_id=33, pid=5048[0m
[[34m2024-07-05T22:38:59.178+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:38:40.198767+00:00: manual__2024-07-05T22:38:40.198767+00:00, state:running, queued_at: 2024-07-05 22:38:40.246359+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:38:59.180+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:38:40.198767+00:00, run_id=manual__2024-07-05T22:38:40.198767+00:00, run_start_date=2024-07-05 22:38:45.072516+00:00, run_end_date=2024-07-05 22:38:59.179921+00:00, run_duration=14.107405, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:33:40.198767+00:00, data_interval_end=2024-07-05 22:38:40.198767+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:39:30.758+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:39:26.508418+00:00 [scheduled]>[0m
[[34m2024-07-05T22:39:30.759+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:39:30.760+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:39:26.508418+00:00 [scheduled]>[0m
[[34m2024-07-05T22:39:30.765+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:39:30.766+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:39:26.508418+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:39:30.766+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:39:26.508418+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:39:30.773+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:39:26.508418+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:39:32.818+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:39:26.508418+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:39:37.940+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:39:26.508418+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:39:40.155+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:39:26.508418+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:39:40.169+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:39:26.508418+00:00, map_index=-1, run_start_date=2024-07-05 22:39:38.058192+00:00, run_end_date=2024-07-05 22:39:38.706525+00:00, run_duration=0.648333, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=60, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:39:30.761293+00:00, queued_by_job_id=33, pid=5106[0m
[[34m2024-07-05T22:39:45.183+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:39:26.508418+00:00: manual__2024-07-05T22:39:26.508418+00:00, state:running, queued_at: 2024-07-05 22:39:26.542450+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:39:45.184+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:39:26.508418+00:00, run_id=manual__2024-07-05T22:39:26.508418+00:00, run_start_date=2024-07-05 22:39:30.703887+00:00, run_end_date=2024-07-05 22:39:45.184624+00:00, run_duration=14.480737, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:34:26.508418+00:00, data_interval_end=2024-07-05 22:39:26.508418+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:40:35.041+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:40:33.321333+00:00 [scheduled]>[0m
[[34m2024-07-05T22:40:35.042+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:40:35.043+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:40:33.321333+00:00 [scheduled]>[0m
[[34m2024-07-05T22:40:35.047+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:40:35.049+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:40:33.321333+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:40:35.049+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:40:33.321333+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:40:35.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:40:33.321333+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:40:37.046+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:40:33.321333+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:40:42.228+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:40:33.321333+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:40:44.083+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:40:33.321333+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:40:44.101+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:40:33.321333+00:00, map_index=-1, run_start_date=2024-07-05 22:40:42.333685+00:00, run_end_date=2024-07-05 22:40:42.653021+00:00, run_duration=0.319336, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=61, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:40:35.043972+00:00, queued_by_job_id=33, pid=5193[0m
[[34m2024-07-05T22:40:44.183+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:40:33.321333+00:00: manual__2024-07-05T22:40:33.321333+00:00, state:running, queued_at: 2024-07-05 22:40:33.354362+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:40:44.184+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:40:33.321333+00:00, run_id=manual__2024-07-05T22:40:33.321333+00:00, run_start_date=2024-07-05 22:40:34.993438+00:00, run_end_date=2024-07-05 22:40:44.184076+00:00, run_duration=9.190638, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:35:33.321333+00:00, data_interval_end=2024-07-05 22:40:33.321333+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:41:24.993+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:42:00.418+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:41:59.755712+00:00 [scheduled]>[0m
[[34m2024-07-05T22:42:00.424+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:42:00.427+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:41:59.755712+00:00 [scheduled]>[0m
[[34m2024-07-05T22:42:00.464+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:42:00.470+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:41:59.755712+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:42:00.470+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:41:59.755712+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:42:00.484+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:41:59.755712+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:42:02.975+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:41:59.755712+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:42:08.001+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:41:59.755712+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:42:10.462+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:41:59.755712+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:42:10.497+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:41:59.755712+00:00, map_index=-1, run_start_date=2024-07-05 22:42:08.122972+00:00, run_end_date=2024-07-05 22:42:08.984363+00:00, run_duration=0.861391, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=62, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:42:00.441672+00:00, queued_by_job_id=33, pid=5302[0m
[[34m2024-07-05T22:42:16.416+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:41:59.755712+00:00: manual__2024-07-05T22:41:59.755712+00:00, state:running, queued_at: 2024-07-05 22:41:59.771401+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:42:16.418+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:41:59.755712+00:00, run_id=manual__2024-07-05T22:41:59.755712+00:00, run_start_date=2024-07-05 22:42:00.127132+00:00, run_end_date=2024-07-05 22:42:16.418036+00:00, run_duration=16.290904, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:36:59.755712+00:00, data_interval_end=2024-07-05 22:41:59.755712+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:43:26.972+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:43:22.411869+00:00, run_after=2024-07-05T22:48:22.411869+00:00[0m
[[34m2024-07-05T22:43:27.048+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:38:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:43:27.049+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:43:27.050+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:38:22.411869+00:00 [scheduled]>[0m
[[34m2024-07-05T22:43:27.055+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:43:27.056+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:38:22.411869+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:43:27.057+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:38:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:43:27.063+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:38:22.411869+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:43:29.309+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:38:22.411869+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:43:34.105+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:38:22.411869+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:43:36.327+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:38:22.411869+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:43:36.347+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:38:22.411869+00:00, map_index=-1, run_start_date=2024-07-05 22:43:34.277284+00:00, run_end_date=2024-07-05 22:43:34.972074+00:00, run_duration=0.69479, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=63, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:43:27.051589+00:00, queued_by_job_id=33, pid=5426[0m
[[34m2024-07-05T22:43:41.578+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:38:22.411869+00:00: scheduled__2024-07-05T22:38:22.411869+00:00, state:running, queued_at: 2024-07-05 22:43:26.957664+00:00. externally triggered: False> failed[0m
[[34m2024-07-05T22:43:41.579+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:38:22.411869+00:00, run_id=scheduled__2024-07-05T22:38:22.411869+00:00, run_start_date=2024-07-05 22:43:26.994951+00:00, run_end_date=2024-07-05 22:43:41.579679+00:00, run_duration=14.584728, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:38:22.411869+00:00, data_interval_end=2024-07-05 22:43:22.411869+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:43:41.586+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:43:22.411869+00:00, run_after=2024-07-05T22:48:22.411869+00:00[0m
[[34m2024-07-05T22:44:16.856+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:44:15.020807+00:00 [scheduled]>[0m
[[34m2024-07-05T22:44:16.858+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:44:16.858+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:44:15.020807+00:00 [scheduled]>[0m
[[34m2024-07-05T22:44:16.864+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:44:16.865+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:44:15.020807+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:44:16.866+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:44:15.020807+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:44:16.871+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:44:15.020807+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:44:19.116+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:44:15.020807+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:44:24.333+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:44:15.020807+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:44:26.762+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:44:15.020807+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:44:26.779+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:44:15.020807+00:00, map_index=-1, run_start_date=2024-07-05 22:44:24.457129+00:00, run_end_date=2024-07-05 22:44:25.143985+00:00, run_duration=0.686856, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=64, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:44:16.860155+00:00, queued_by_job_id=33, pid=5495[0m
[[34m2024-07-05T22:44:26.859+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:44:15.020807+00:00: manual__2024-07-05T22:44:15.020807+00:00, state:running, queued_at: 2024-07-05 22:44:15.035931+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:44:26.860+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:44:15.020807+00:00, run_id=manual__2024-07-05T22:44:15.020807+00:00, run_start_date=2024-07-05 22:44:16.811349+00:00, run_end_date=2024-07-05 22:44:26.860471+00:00, run_duration=10.049122, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:39:15.020807+00:00, data_interval_end=2024-07-05 22:44:15.020807+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:45:51.155+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:45:48.246504+00:00 [scheduled]>[0m
[[34m2024-07-05T22:45:51.156+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:45:51.157+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:45:48.246504+00:00 [scheduled]>[0m
[[34m2024-07-05T22:45:51.162+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:45:51.163+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:45:48.246504+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:45:51.164+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:45:48.246504+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:45:51.170+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:45:48.246504+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:45:54.052+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:45:48.246504+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:46:00.356+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:45:48.246504+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:46:02.538+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:45:48.246504+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:46:02.555+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:45:48.246504+00:00, map_index=-1, run_start_date=2024-07-05 22:46:00.550994+00:00, run_end_date=2024-07-05 22:46:00.905471+00:00, run_duration=0.354477, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=65, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:45:51.158378+00:00, queued_by_job_id=33, pid=5646[0m
[[34m2024-07-05T22:46:02.660+0000[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:45:48.246504+00:00: manual__2024-07-05T22:45:48.246504+00:00, state:running, queued_at: 2024-07-05 22:45:48.285517+00:00. externally triggered: True> failed[0m
[[34m2024-07-05T22:46:02.661+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:45:48.246504+00:00, run_id=manual__2024-07-05T22:45:48.246504+00:00, run_start_date=2024-07-05 22:45:51.103615+00:00, run_end_date=2024-07-05 22:46:02.661077+00:00, run_duration=11.557462, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:40:48.246504+00:00, data_interval_end=2024-07-05 22:45:48.246504+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:46:19.902+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:46:18.872227+00:00 [scheduled]>[0m
[[34m2024-07-05T22:46:19.923+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:46:19.923+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:46:18.872227+00:00 [scheduled]>[0m
[[34m2024-07-05T22:46:19.944+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:46:19.946+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:46:18.872227+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:46:19.950+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:46:18.872227+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:46:19.964+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:46:18.872227+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:46:23.498+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:46:18.872227+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:46:29.675+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:46:18.872227+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:46:32.060+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:46:18.872227+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:46:32.091+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:46:18.872227+00:00, map_index=-1, run_start_date=2024-07-05 22:46:29.780113+00:00, run_end_date=2024-07-05 22:46:30.467117+00:00, run_duration=0.687004, state=success, executor_state=success, try_number=1, max_tries=0, job_id=66, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:46:19.931174+00:00, queued_by_job_id=33, pid=5686[0m
[[34m2024-07-05T22:46:32.126+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:46:37.937+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:46:18.872227+00:00 [scheduled]>[0m
[[34m2024-07-05T22:46:37.939+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:46:37.939+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:46:18.872227+00:00 [scheduled]>[0m
[[34m2024-07-05T22:46:37.945+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:46:37.946+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:46:18.872227+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:46:37.947+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:46:18.872227+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:46:37.953+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:46:18.872227+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:46:40.884+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:46:18.872227+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:46:48.173+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:46:18.872227+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:46:51.641+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:46:18.872227+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:46:51.665+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=manual__2024-07-05T22:46:18.872227+00:00, map_index=-1, run_start_date=2024-07-05 22:46:48.288530+00:00, run_end_date=2024-07-05 22:46:49.801728+00:00, run_duration=1.513198, state=success, executor_state=success, try_number=1, max_tries=0, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:46:37.940980+00:00, queued_by_job_id=33, pid=5712[0m
[[34m2024-07-05T22:46:57.288+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:46:18.872227+00:00: manual__2024-07-05T22:46:18.872227+00:00, state:running, queued_at: 2024-07-05 22:46:18.924495+00:00. externally triggered: True> successful[0m
[[34m2024-07-05T22:46:57.290+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:46:18.872227+00:00, run_id=manual__2024-07-05T22:46:18.872227+00:00, run_start_date=2024-07-05 22:46:19.699505+00:00, run_end_date=2024-07-05 22:46:57.290066+00:00, run_duration=37.590561, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:41:18.872227+00:00, data_interval_end=2024-07-05 22:46:18.872227+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:48:00.649+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:47:59.955306+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:00.650+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:48:00.651+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:47:59.955306+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:00.674+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:48:00.675+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:47:59.955306+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:48:00.675+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:47:59.955306+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:00.685+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'manual__2024-07-05T22:47:59.955306+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:03.531+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:47:59.955306+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:48:08.261+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data manual__2024-07-05T22:47:59.955306+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:48:10.639+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='manual__2024-07-05T22:47:59.955306+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:48:10.660+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=manual__2024-07-05T22:47:59.955306+00:00, map_index=-1, run_start_date=2024-07-05 22:48:08.391100+00:00, run_end_date=2024-07-05 22:48:09.062395+00:00, run_duration=0.671295, state=success, executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:48:00.652164+00:00, queued_by_job_id=33, pid=5821[0m
[[34m2024-07-05T22:48:10.770+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:47:59.955306+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:10.771+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:48:10.771+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:47:59.955306+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:10.776+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:48:10.778+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:47:59.955306+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:48:10.778+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:47:59.955306+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:10.783+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'manual__2024-07-05T22:47:59.955306+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:12.843+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=manual__2024-07-05T22:47:59.955306+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:48:17.932+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data manual__2024-07-05T22:47:59.955306+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:48:22.018+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='manual__2024-07-05T22:47:59.955306+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:48:22.032+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=manual__2024-07-05T22:47:59.955306+00:00, map_index=-1, run_start_date=2024-07-05 22:48:18.044403+00:00, run_end_date=2024-07-05 22:48:20.379655+00:00, run_duration=2.335252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:48:10.772675+00:00, queued_by_job_id=33, pid=5833[0m
[[34m2024-07-05T22:48:27.969+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:47:59.955306+00:00: manual__2024-07-05T22:47:59.955306+00:00, state:running, queued_at: 2024-07-05 22:47:59.981658+00:00. externally triggered: True> successful[0m
[[34m2024-07-05T22:48:27.971+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:47:59.955306+00:00, run_id=manual__2024-07-05T22:47:59.955306+00:00, run_start_date=2024-07-05 22:48:00.405486+00:00, run_end_date=2024-07-05 22:48:27.971536+00:00, run_duration=27.56605, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-07-05 22:42:59.955306+00:00, data_interval_end=2024-07-05 22:47:59.955306+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:48:33.829+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:48:27.793710+00:00, run_after=2024-07-05T22:53:27.793710+00:00[0m
[[34m2024-07-05T22:48:33.893+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:43:27.793710+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:33.894+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:48:33.895+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:43:27.793710+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:33.900+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:48:33.901+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:43:27.793710+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:48:33.902+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:43:27.793710+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:33.907+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:43:27.793710+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:36.101+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:43:27.793710+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:48:41.296+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:43:27.793710+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:48:43.491+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:43:27.793710+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:48:43.574+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:43:27.793710+00:00, map_index=-1, run_start_date=2024-07-05 22:48:41.411789+00:00, run_end_date=2024-07-05 22:48:41.977850+00:00, run_duration=0.566061, state=success, executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:48:33.896303+00:00, queued_by_job_id=33, pid=5871[0m
[[34m2024-07-05T22:48:43.757+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:43:27.793710+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:43.757+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:48:43.758+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:43:27.793710+00:00 [scheduled]>[0m
[[34m2024-07-05T22:48:43.763+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:48:43.764+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:43:27.793710+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:48:43.764+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:43:27.793710+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:43.773+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:43:27.793710+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:48:45.831+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:43:27.793710+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:48:50.611+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:43:27.793710+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:48:54.386+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:43:27.793710+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:48:54.396+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:43:27.793710+00:00, map_index=-1, run_start_date=2024-07-05 22:48:50.717291+00:00, run_end_date=2024-07-05 22:48:52.695412+00:00, run_duration=1.978121, state=success, executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:48:43.759429+00:00, queued_by_job_id=33, pid=5885[0m
[[34m2024-07-05T22:48:54.455+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:43:27.793710+00:00: scheduled__2024-07-05T22:43:27.793710+00:00, state:running, queued_at: 2024-07-05 22:48:33.813457+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:48:54.456+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:43:27.793710+00:00, run_id=scheduled__2024-07-05T22:43:27.793710+00:00, run_start_date=2024-07-05 22:48:33.848739+00:00, run_end_date=2024-07-05 22:48:54.456682+00:00, run_duration=20.607943, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:43:27.793710+00:00, data_interval_end=2024-07-05 22:48:27.793710+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:48:54.465+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:48:27.793710+00:00, run_after=2024-07-05T22:53:27.793710+00:00[0m
[[34m2024-07-05T22:51:36.719+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:53:34.766+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:53:33.592049+00:00, run_after=2024-07-05T22:58:33.592049+00:00[0m
[[34m2024-07-05T22:53:34.820+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:48:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:53:34.820+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:53:34.821+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:48:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:53:34.823+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:53:34.823+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:48:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:53:34.824+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:48:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:53:34.828+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:48:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:53:37.225+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:48:33.592049+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:53:42.090+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:48:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:53:44.292+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:48:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:53:44.308+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:48:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 22:53:42.211169+00:00, run_end_date=2024-07-05 22:53:42.734926+00:00, run_duration=0.523757, state=success, executor_state=success, try_number=1, max_tries=0, job_id=72, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:53:34.821707+00:00, queued_by_job_id=33, pid=6853[0m
[[34m2024-07-05T22:53:44.395+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:48:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:53:44.395+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:53:44.396+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:48:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:53:44.398+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:53:44.398+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:48:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:53:44.399+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:48:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:53:44.402+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:48:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:53:46.559+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:48:33.592049+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:53:51.217+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:48:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:53:54.535+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:48:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:53:54.552+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:48:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 22:53:51.307911+00:00, run_end_date=2024-07-05 22:53:53.026545+00:00, run_duration=1.718634, state=success, executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:53:44.396739+00:00, queued_by_job_id=33, pid=6856[0m
[[34m2024-07-05T22:53:59.268+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:48:33.592049+00:00: scheduled__2024-07-05T22:48:33.592049+00:00, state:running, queued_at: 2024-07-05 22:53:34.752703+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:53:59.269+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:48:33.592049+00:00, run_id=scheduled__2024-07-05T22:48:33.592049+00:00, run_start_date=2024-07-05 22:53:34.782262+00:00, run_end_date=2024-07-05 22:53:59.269385+00:00, run_duration=24.487123, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:48:33.592049+00:00, data_interval_end=2024-07-05 22:53:33.592049+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:53:59.275+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:53:33.592049+00:00, run_after=2024-07-05T22:58:33.592049+00:00[0m
[[34m2024-07-05T22:56:36.776+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T22:58:36.618+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:58:33.592049+00:00, run_after=2024-07-05T23:03:33.592049+00:00[0m
[[34m2024-07-05T22:58:36.670+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:53:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:58:36.670+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:58:36.670+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:53:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:58:36.672+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:58:36.673+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:53:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T22:58:36.674+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:53:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:58:36.678+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:53:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:58:38.347+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:53:33.592049+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T22:58:42.985+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:53:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:58:45.045+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:53:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:58:45.062+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:53:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 22:58:43.114035+00:00, run_end_date=2024-07-05 22:58:43.635929+00:00, run_duration=0.521894, state=success, executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 22:58:36.671421+00:00, queued_by_job_id=33, pid=6929[0m
[[34m2024-07-05T22:58:49.800+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:53:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:58:49.801+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T22:58:49.802+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:53:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T22:58:49.804+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T22:58:49.804+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:53:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T22:58:49.805+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:53:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:58:49.808+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:53:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T22:58:51.438+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:53:33.592049+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T22:58:56.014+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:53:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T22:58:59.129+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:53:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T22:58:59.138+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:53:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 22:58:56.106369+00:00, run_end_date=2024-07-05 22:58:57.877496+00:00, run_duration=1.771127, state=success, executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 22:58:49.802740+00:00, queued_by_job_id=33, pid=6936[0m
[[34m2024-07-05T22:58:59.207+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:53:33.592049+00:00: scheduled__2024-07-05T22:53:33.592049+00:00, state:running, queued_at: 2024-07-05 22:58:36.610282+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T22:58:59.208+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:53:33.592049+00:00, run_id=scheduled__2024-07-05T22:53:33.592049+00:00, run_start_date=2024-07-05 22:58:36.634411+00:00, run_end_date=2024-07-05 22:58:59.208229+00:00, run_duration=22.573818, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:53:33.592049+00:00, data_interval_end=2024-07-05 22:58:33.592049+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T22:58:59.211+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T22:58:33.592049+00:00, run_after=2024-07-05T23:03:33.592049+00:00[0m
[[34m2024-07-05T23:01:36.826+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:03:34.194+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:03:33.592049+00:00, run_after=2024-07-05T23:08:33.592049+00:00[0m
[[34m2024-07-05T23:03:34.240+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:58:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:03:34.240+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:03:34.241+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:58:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:03:34.242+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:03:34.243+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:58:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:03:34.243+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:58:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:03:34.246+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T22:58:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:03:35.966+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:58:33.592049+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:03:40.234+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T22:58:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:03:42.016+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T22:58:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:03:42.031+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T22:58:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:03:40.324395+00:00, run_end_date=2024-07-05 23:03:40.767476+00:00, run_duration=0.443081, state=success, executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:03:34.241583+00:00, queued_by_job_id=33, pid=6994[0m
[[34m2024-07-05T23:03:42.111+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:58:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:03:42.111+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:03:42.111+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:58:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:03:42.113+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:03:42.114+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:58:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:03:42.114+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:58:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:03:42.118+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T22:58:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:03:43.754+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T22:58:33.592049+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:03:48.340+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T22:58:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:03:51.379+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T22:58:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:03:51.395+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T22:58:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:03:48.435550+00:00, run_end_date=2024-07-05 23:03:50.116478+00:00, run_duration=1.680928, state=success, executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:03:42.112176+00:00, queued_by_job_id=33, pid=6999[0m
[[34m2024-07-05T23:03:55.891+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 22:58:33.592049+00:00: scheduled__2024-07-05T22:58:33.592049+00:00, state:running, queued_at: 2024-07-05 23:03:34.189090+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:03:55.892+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 22:58:33.592049+00:00, run_id=scheduled__2024-07-05T22:58:33.592049+00:00, run_start_date=2024-07-05 23:03:34.209794+00:00, run_end_date=2024-07-05 23:03:55.892036+00:00, run_duration=21.682242, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 22:58:33.592049+00:00, data_interval_end=2024-07-05 23:03:33.592049+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:03:55.895+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:03:33.592049+00:00, run_after=2024-07-05T23:08:33.592049+00:00[0m
[[34m2024-07-05T23:06:36.878+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:08:34.520+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:08:33.592049+00:00, run_after=2024-07-05T23:13:33.592049+00:00[0m
[[34m2024-07-05T23:08:34.574+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:03:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:08:34.575+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:08:34.575+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:03:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:08:34.577+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:08:34.578+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:03:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:08:34.578+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:03:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:08:34.583+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:03:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:08:36.162+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:03:33.592049+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:08:40.604+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:03:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:08:42.402+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:03:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:08:42.416+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:03:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:08:40.693076+00:00, run_end_date=2024-07-05 23:08:41.151802+00:00, run_duration=0.458726, state=success, executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:08:34.576178+00:00, queued_by_job_id=33, pid=7066[0m
[[34m2024-07-05T23:08:46.845+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:03:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:08:46.846+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:08:46.846+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:03:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:08:46.848+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:08:46.849+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:03:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:08:46.850+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:03:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:08:46.854+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:03:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:08:48.460+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:03:33.592049+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:08:52.963+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:03:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:08:55.981+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:03:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:08:55.994+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:03:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:08:53.058307+00:00, run_end_date=2024-07-05 23:08:54.746369+00:00, run_duration=1.688062, state=success, executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:08:46.847162+00:00, queued_by_job_id=33, pid=7073[0m
[[34m2024-07-05T23:08:56.069+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:03:33.592049+00:00: scheduled__2024-07-05T23:03:33.592049+00:00, state:running, queued_at: 2024-07-05 23:08:34.514984+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:08:56.070+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:03:33.592049+00:00, run_id=scheduled__2024-07-05T23:03:33.592049+00:00, run_start_date=2024-07-05 23:08:34.536229+00:00, run_end_date=2024-07-05 23:08:56.070230+00:00, run_duration=21.534001, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:03:33.592049+00:00, data_interval_end=2024-07-05 23:08:33.592049+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:08:56.074+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:08:33.592049+00:00, run_after=2024-07-05T23:13:33.592049+00:00[0m
[[34m2024-07-05T23:11:36.929+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:13:34.189+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:13:33.592049+00:00, run_after=2024-07-05T23:18:33.592049+00:00[0m
[[34m2024-07-05T23:13:34.237+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:08:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:13:34.237+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:13:34.238+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:08:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:13:34.239+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:13:34.240+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:08:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:13:34.240+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:08:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:13:34.243+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:08:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:13:35.841+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:08:33.592049+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:13:40.202+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:08:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:13:41.995+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:08:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:13:42.010+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:08:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:13:40.297203+00:00, run_end_date=2024-07-05 23:13:40.722008+00:00, run_duration=0.424805, state=success, executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:13:34.238592+00:00, queued_by_job_id=33, pid=7128[0m
[[34m2024-07-05T23:13:46.218+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:08:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:13:46.218+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:13:46.218+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:08:33.592049+00:00 [scheduled]>[0m
[[34m2024-07-05T23:13:46.220+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:13:46.221+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:08:33.592049+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:13:46.221+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:08:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:13:46.225+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:08:33.592049+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:13:47.761+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:08:33.592049+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:13:51.864+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:08:33.592049+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:13:55.040+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:08:33.592049+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:13:55.047+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:08:33.592049+00:00, map_index=-1, run_start_date=2024-07-05 23:13:51.965945+00:00, run_end_date=2024-07-05 23:13:53.585084+00:00, run_duration=1.619139, state=success, executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:13:46.219363+00:00, queued_by_job_id=33, pid=7133[0m
[[34m2024-07-05T23:13:55.110+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:08:33.592049+00:00: scheduled__2024-07-05T23:08:33.592049+00:00, state:running, queued_at: 2024-07-05 23:13:34.181838+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:13:55.111+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:08:33.592049+00:00, run_id=scheduled__2024-07-05T23:08:33.592049+00:00, run_start_date=2024-07-05 23:13:34.203815+00:00, run_end_date=2024-07-05 23:13:55.110875+00:00, run_duration=20.90706, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:08:33.592049+00:00, data_interval_end=2024-07-05 23:13:33.592049+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:13:55.114+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:13:33.592049+00:00, run_after=2024-07-05T23:18:33.592049+00:00[0m
[[34m2024-07-05T23:16:39.028+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:18:38.215+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:18:37.053506+00:00, run_after=2024-07-05T23:23:37.053506+00:00[0m
[[34m2024-07-05T23:18:38.269+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:13:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:18:38.270+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:18:38.270+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:13:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:18:38.272+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:18:38.273+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:13:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:18:38.273+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:13:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:18:38.278+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:13:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:18:39.884+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:13:37.053506+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:18:44.287+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:13:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:18:46.127+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:13:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:18:46.142+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:13:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:18:44.400191+00:00, run_end_date=2024-07-05 23:18:44.831645+00:00, run_duration=0.431454, state=success, executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:18:38.271000+00:00, queued_by_job_id=33, pid=7191[0m
[[34m2024-07-05T23:18:46.244+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:13:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:18:46.244+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:18:46.244+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:13:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:18:46.246+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:18:46.247+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:13:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:18:46.248+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:13:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:18:46.253+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:13:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:18:47.956+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:13:37.053506+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:18:52.176+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:13:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:18:55.169+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:13:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:18:55.186+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:13:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:18:52.271071+00:00, run_end_date=2024-07-05 23:18:53.930582+00:00, run_duration=1.659511, state=success, executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:18:46.245419+00:00, queued_by_job_id=33, pid=7194[0m
[[34m2024-07-05T23:18:59.586+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:13:37.053506+00:00: scheduled__2024-07-05T23:13:37.053506+00:00, state:running, queued_at: 2024-07-05 23:18:38.206803+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:18:59.587+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:13:37.053506+00:00, run_id=scheduled__2024-07-05T23:13:37.053506+00:00, run_start_date=2024-07-05 23:18:38.234026+00:00, run_end_date=2024-07-05 23:18:59.587706+00:00, run_duration=21.35368, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:13:37.053506+00:00, data_interval_end=2024-07-05 23:18:37.053506+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:18:59.592+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:18:37.053506+00:00, run_after=2024-07-05T23:23:37.053506+00:00[0m
[[34m2024-07-05T23:21:39.075+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:23:38.496+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:23:37.053506+00:00, run_after=2024-07-05T23:28:37.053506+00:00[0m
[[34m2024-07-05T23:23:38.567+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:18:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:23:38.568+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:23:38.568+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:18:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:23:38.570+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:23:38.571+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:18:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:23:38.571+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:18:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:23:38.575+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:18:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:23:40.267+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:18:37.053506+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:23:44.691+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:18:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:23:46.489+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:18:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:23:46.505+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:18:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:23:44.788161+00:00, run_end_date=2024-07-05 23:23:45.224728+00:00, run_duration=0.436567, state=success, executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:23:38.568979+00:00, queued_by_job_id=33, pid=7255[0m
[[34m2024-07-05T23:23:51.047+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:18:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:23:51.048+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:23:51.049+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:18:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:23:51.051+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:23:51.052+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:18:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:23:51.052+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:18:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:23:51.056+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:18:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:23:52.601+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:18:37.053506+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:23:56.907+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:18:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:23:59.889+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:18:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:23:59.895+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:18:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:23:57.002757+00:00, run_end_date=2024-07-05 23:23:58.665708+00:00, run_duration=1.662951, state=success, executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:23:51.049943+00:00, queued_by_job_id=33, pid=7262[0m
[[34m2024-07-05T23:23:59.975+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:18:37.053506+00:00: scheduled__2024-07-05T23:18:37.053506+00:00, state:running, queued_at: 2024-07-05 23:23:38.489876+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:23:59.976+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:18:37.053506+00:00, run_id=scheduled__2024-07-05T23:18:37.053506+00:00, run_start_date=2024-07-05 23:23:38.523064+00:00, run_end_date=2024-07-05 23:23:59.976430+00:00, run_duration=21.453366, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:18:37.053506+00:00, data_interval_end=2024-07-05 23:23:37.053506+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:23:59.979+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:23:37.053506+00:00, run_after=2024-07-05T23:28:37.053506+00:00[0m
[[34m2024-07-05T23:26:39.131+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:28:38.975+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:28:37.053506+00:00, run_after=2024-07-05T23:33:37.053506+00:00[0m
[[34m2024-07-05T23:28:39.021+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:23:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:28:39.022+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:28:39.022+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:23:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:28:39.024+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:28:39.024+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:23:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:28:39.025+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:23:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:28:39.028+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:23:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:28:40.624+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:23:37.053506+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:28:44.869+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:23:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:28:46.654+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:23:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:28:46.671+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:23:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:28:44.959881+00:00, run_end_date=2024-07-05 23:28:45.354118+00:00, run_duration=0.394237, state=success, executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:28:39.022852+00:00, queued_by_job_id=33, pid=7333[0m
[[34m2024-07-05T23:28:50.994+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:23:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:28:50.995+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:28:50.996+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:23:37.053506+00:00 [scheduled]>[0m
[[34m2024-07-05T23:28:50.998+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:28:50.999+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:23:37.053506+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:28:50.999+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:23:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:28:51.003+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:23:37.053506+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:28:52.512+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:23:37.053506+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:28:56.755+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:23:37.053506+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:28:59.710+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:23:37.053506+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:28:59.724+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:23:37.053506+00:00, map_index=-1, run_start_date=2024-07-05 23:28:56.877359+00:00, run_end_date=2024-07-05 23:28:58.511173+00:00, run_duration=1.633814, state=success, executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:28:50.996759+00:00, queued_by_job_id=33, pid=7342[0m
[[34m2024-07-05T23:28:59.806+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:23:37.053506+00:00: scheduled__2024-07-05T23:23:37.053506+00:00, state:running, queued_at: 2024-07-05 23:28:38.968745+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:28:59.806+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:23:37.053506+00:00, run_id=scheduled__2024-07-05T23:23:37.053506+00:00, run_start_date=2024-07-05 23:28:38.987883+00:00, run_end_date=2024-07-05 23:28:59.806680+00:00, run_duration=20.818797, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:23:37.053506+00:00, data_interval_end=2024-07-05 23:28:37.053506+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:28:59.810+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:28:37.053506+00:00, run_after=2024-07-05T23:33:37.053506+00:00[0m
[[34m2024-07-05T23:31:39.182+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:33:42.239+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:33:41.081035+00:00, run_after=2024-07-05T23:38:41.081035+00:00[0m
[[34m2024-07-05T23:33:42.283+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:28:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:33:42.284+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:33:42.284+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:28:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:33:42.286+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:33:42.286+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:28:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:33:42.287+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:28:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:33:42.291+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:28:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:33:43.837+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:28:41.081035+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:33:47.894+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:28:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:33:49.674+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:28:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:33:49.689+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:28:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:33:47.986277+00:00, run_end_date=2024-07-05 23:33:48.397382+00:00, run_duration=0.411105, state=success, executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:33:42.284982+00:00, queued_by_job_id=33, pid=7400[0m
[[34m2024-07-05T23:33:49.772+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:28:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:33:49.773+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:33:49.773+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:28:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:33:49.775+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:33:49.776+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:28:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:33:49.776+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:28:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:33:49.781+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:28:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:33:51.327+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:28:41.081035+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:33:55.655+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:28:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:33:58.591+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:28:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:33:58.609+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:28:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:33:55.743244+00:00, run_end_date=2024-07-05 23:33:57.307809+00:00, run_duration=1.564565, state=success, executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:33:49.774148+00:00, queued_by_job_id=33, pid=7403[0m
[[34m2024-07-05T23:34:02.926+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:28:41.081035+00:00: scheduled__2024-07-05T23:28:41.081035+00:00, state:running, queued_at: 2024-07-05 23:33:42.234120+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:34:02.927+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:28:41.081035+00:00, run_id=scheduled__2024-07-05T23:28:41.081035+00:00, run_start_date=2024-07-05 23:33:42.251777+00:00, run_end_date=2024-07-05 23:34:02.927566+00:00, run_duration=20.675789, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:28:41.081035+00:00, data_interval_end=2024-07-05 23:33:41.081035+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:34:02.931+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:33:41.081035+00:00, run_after=2024-07-05T23:38:41.081035+00:00[0m
[[34m2024-07-05T23:36:39.233+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:38:42.581+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:38:41.081035+00:00, run_after=2024-07-05T23:43:41.081035+00:00[0m
[[34m2024-07-05T23:38:42.635+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:33:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:38:42.636+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:38:42.636+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:33:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:38:42.638+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:38:42.639+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:33:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:38:42.639+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:33:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:38:42.643+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:33:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:38:44.435+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:33:41.081035+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:38:49.215+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:33:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:38:51.176+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:33:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:38:51.195+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:33:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:38:49.308305+00:00, run_end_date=2024-07-05 23:38:49.719275+00:00, run_duration=0.41097, state=success, executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:38:42.637255+00:00, queued_by_job_id=33, pid=7471[0m
[[34m2024-07-05T23:38:56.137+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:33:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:38:56.138+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:38:56.138+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:33:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:38:56.140+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:38:56.141+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:33:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:38:56.141+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:33:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:38:56.145+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:33:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:38:57.857+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:33:41.081035+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:39:02.836+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:33:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:39:06.330+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:33:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:39:06.346+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:33:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:39:02.966374+00:00, run_end_date=2024-07-05 23:39:04.714023+00:00, run_duration=1.747649, state=success, executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:38:56.139017+00:00, queued_by_job_id=33, pid=7476[0m
[[34m2024-07-05T23:39:06.429+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:33:41.081035+00:00: scheduled__2024-07-05T23:33:41.081035+00:00, state:running, queued_at: 2024-07-05 23:38:42.575166+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:39:06.430+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:33:41.081035+00:00, run_id=scheduled__2024-07-05T23:33:41.081035+00:00, run_start_date=2024-07-05 23:38:42.594081+00:00, run_end_date=2024-07-05 23:39:06.430412+00:00, run_duration=23.836331, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:33:41.081035+00:00, data_interval_end=2024-07-05 23:38:41.081035+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:39:06.434+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:38:41.081035+00:00, run_after=2024-07-05T23:43:41.081035+00:00[0m
[[34m2024-07-05T23:41:39.303+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:43:42.304+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:43:41.081035+00:00, run_after=2024-07-05T23:48:41.081035+00:00[0m
[[34m2024-07-05T23:43:42.353+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:38:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:43:42.354+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:43:42.354+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:38:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:43:42.356+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:43:42.356+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:38:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:43:42.357+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:38:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:43:42.361+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:38:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:43:43.953+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:38:41.081035+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:43:48.141+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:38:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:43:49.853+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:38:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:43:49.867+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:38:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:43:48.235339+00:00, run_end_date=2024-07-05 23:43:48.652790+00:00, run_duration=0.417451, state=success, executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:43:42.355121+00:00, queued_by_job_id=33, pid=7536[0m
[[34m2024-07-05T23:43:53.968+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:38:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:43:53.969+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:43:53.969+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:38:41.081035+00:00 [scheduled]>[0m
[[34m2024-07-05T23:43:53.971+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:43:53.971+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:38:41.081035+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:43:53.972+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:38:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:43:53.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:38:41.081035+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:43:55.473+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:38:41.081035+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:43:59.662+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:38:41.081035+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:44:02.598+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:38:41.081035+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:44:02.612+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:38:41.081035+00:00, map_index=-1, run_start_date=2024-07-05 23:43:59.754569+00:00, run_end_date=2024-07-05 23:44:01.371315+00:00, run_duration=1.616746, state=success, executor_state=success, try_number=1, max_tries=0, job_id=93, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:43:53.970061+00:00, queued_by_job_id=33, pid=7542[0m
[[34m2024-07-05T23:44:02.682+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:38:41.081035+00:00: scheduled__2024-07-05T23:38:41.081035+00:00, state:running, queued_at: 2024-07-05 23:43:42.298500+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:44:02.682+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:38:41.081035+00:00, run_id=scheduled__2024-07-05T23:38:41.081035+00:00, run_start_date=2024-07-05 23:43:42.323009+00:00, run_end_date=2024-07-05 23:44:02.682539+00:00, run_duration=20.35953, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:38:41.081035+00:00, data_interval_end=2024-07-05 23:43:41.081035+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:44:02.686+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:43:41.081035+00:00, run_after=2024-07-05T23:48:41.081035+00:00[0m
[[34m2024-07-05T23:46:39.358+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:48:44.287+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:48:43.076850+00:00, run_after=2024-07-05T23:53:43.076850+00:00[0m
[[34m2024-07-05T23:48:44.334+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:43:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:48:44.335+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:48:44.335+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:43:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:48:44.337+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:48:44.337+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:43:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:48:44.338+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:43:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:48:44.341+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:43:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:48:45.959+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:43:43.076850+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:48:50.340+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:43:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:48:52.167+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:43:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:48:52.181+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:43:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:48:50.431444+00:00, run_end_date=2024-07-05 23:48:50.851367+00:00, run_duration=0.419923, state=success, executor_state=success, try_number=1, max_tries=0, job_id=94, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:48:44.336004+00:00, queued_by_job_id=33, pid=7606[0m
[[34m2024-07-05T23:48:52.260+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:43:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:48:52.260+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:48:52.261+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:43:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:48:52.263+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:48:52.263+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:43:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:48:52.264+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:43:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:48:52.267+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:43:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:48:53.776+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:43:43.076850+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:48:58.061+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:43:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:49:01.064+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:43:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:49:01.078+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:43:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:48:58.179041+00:00, run_end_date=2024-07-05 23:48:59.807668+00:00, run_duration=1.628627, state=success, executor_state=success, try_number=1, max_tries=0, job_id=95, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:48:52.261798+00:00, queued_by_job_id=33, pid=7611[0m
[[34m2024-07-05T23:49:05.299+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:43:43.076850+00:00: scheduled__2024-07-05T23:43:43.076850+00:00, state:running, queued_at: 2024-07-05 23:48:44.282262+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:49:05.300+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:43:43.076850+00:00, run_id=scheduled__2024-07-05T23:43:43.076850+00:00, run_start_date=2024-07-05 23:48:44.304064+00:00, run_end_date=2024-07-05 23:49:05.300529+00:00, run_duration=20.996465, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:43:43.076850+00:00, data_interval_end=2024-07-05 23:48:43.076850+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:49:05.304+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:48:43.076850+00:00, run_after=2024-07-05T23:53:43.076850+00:00[0m
[[34m2024-07-05T23:51:39.408+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:53:44.067+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:53:43.076850+00:00, run_after=2024-07-05T23:58:43.076850+00:00[0m
[[34m2024-07-05T23:53:44.114+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:48:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:53:44.114+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:53:44.115+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:48:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:53:44.117+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:53:44.117+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:48:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:53:44.118+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:48:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:53:44.121+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:48:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:53:45.656+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:48:43.076850+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:53:49.911+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:48:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:53:51.644+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:48:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:53:51.659+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:48:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:53:49.999660+00:00, run_end_date=2024-07-05 23:53:50.415496+00:00, run_duration=0.415836, state=success, executor_state=success, try_number=1, max_tries=0, job_id=96, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:53:44.115816+00:00, queued_by_job_id=33, pid=7677[0m
[[34m2024-07-05T23:53:55.863+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:48:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:53:55.864+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:53:55.864+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:48:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:53:55.866+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:53:55.866+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:48:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:53:55.866+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:48:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:53:55.869+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:48:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:53:57.443+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:48:43.076850+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:54:01.708+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:48:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:54:04.630+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:48:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:54:04.643+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:48:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:54:01.799017+00:00, run_end_date=2024-07-05 23:54:03.413832+00:00, run_duration=1.614815, state=success, executor_state=success, try_number=1, max_tries=0, job_id=97, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:53:55.864865+00:00, queued_by_job_id=33, pid=7682[0m
[[34m2024-07-05T23:54:04.713+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:48:43.076850+00:00: scheduled__2024-07-05T23:48:43.076850+00:00, state:running, queued_at: 2024-07-05 23:53:44.061197+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:54:04.714+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:48:43.076850+00:00, run_id=scheduled__2024-07-05T23:48:43.076850+00:00, run_start_date=2024-07-05 23:53:44.082460+00:00, run_end_date=2024-07-05 23:54:04.713980+00:00, run_duration=20.63152, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:48:43.076850+00:00, data_interval_end=2024-07-05 23:53:43.076850+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:54:04.717+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:53:43.076850+00:00, run_after=2024-07-05T23:58:43.076850+00:00[0m
[[34m2024-07-05T23:56:39.460+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-05T23:58:44.412+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:58:43.076850+00:00, run_after=2024-07-06T00:03:43.076850+00:00[0m
[[34m2024-07-05T23:58:44.460+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:53:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:58:44.461+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:58:44.461+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:53:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:58:44.463+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:58:44.464+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:53:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-05T23:58:44.464+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:53:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:58:44.468+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:53:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:58:46.078+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:53:43.076850+00:00/task_id=extract_data permission to 509
[[34m2024-07-05T23:58:50.338+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:53:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:58:52.074+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:53:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:58:52.087+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:53:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:58:50.436660+00:00, run_end_date=2024-07-05 23:58:50.843033+00:00, run_duration=0.406373, state=success, executor_state=success, try_number=1, max_tries=0, job_id=98, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-05 23:58:44.461967+00:00, queued_by_job_id=33, pid=7734[0m
[[34m2024-07-05T23:58:56.184+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:53:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:58:56.184+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-05T23:58:56.184+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:53:43.076850+00:00 [scheduled]>[0m
[[34m2024-07-05T23:58:56.186+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-05T23:58:56.187+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:53:43.076850+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-05T23:58:56.187+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:53:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:58:56.190+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:53:43.076850+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-05T23:58:57.698+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:53:43.076850+00:00/task_id=validate_data permission to 509
[[34m2024-07-05T23:59:01.999+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:53:43.076850+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-05T23:59:04.901+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:53:43.076850+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-05T23:59:04.914+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:53:43.076850+00:00, map_index=-1, run_start_date=2024-07-05 23:59:02.090010+00:00, run_end_date=2024-07-05 23:59:03.706629+00:00, run_duration=1.616619, state=success, executor_state=success, try_number=1, max_tries=0, job_id=99, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-05 23:58:56.185459+00:00, queued_by_job_id=33, pid=7739[0m
[[34m2024-07-05T23:59:09.201+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:53:43.076850+00:00: scheduled__2024-07-05T23:53:43.076850+00:00, state:running, queued_at: 2024-07-05 23:58:44.405958+00:00. externally triggered: False> successful[0m
[[34m2024-07-05T23:59:09.202+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:53:43.076850+00:00, run_id=scheduled__2024-07-05T23:53:43.076850+00:00, run_start_date=2024-07-05 23:58:44.425819+00:00, run_end_date=2024-07-05 23:59:09.202277+00:00, run_duration=24.776458, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:53:43.076850+00:00, data_interval_end=2024-07-05 23:58:43.076850+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-05T23:59:09.205+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-05T23:58:43.076850+00:00, run_after=2024-07-06T00:03:43.076850+00:00[0m
[[34m2024-07-06T00:01:39.509+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:03:46.390+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:03:45.013649+00:00, run_after=2024-07-06T00:08:45.013649+00:00[0m
[[34m2024-07-06T00:03:46.456+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:58:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:03:46.457+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:03:46.457+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:58:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:03:46.461+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:03:46.462+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:58:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:03:46.463+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:58:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:03:46.467+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-05T23:58:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:03:48.276+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:58:45.013649+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:03:52.618+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-05T23:58:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:03:54.368+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-05T23:58:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:03:54.382+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-05T23:58:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:03:52.706798+00:00, run_end_date=2024-07-06 00:03:53.133191+00:00, run_duration=0.426393, state=success, executor_state=success, try_number=1, max_tries=0, job_id=100, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:03:46.458317+00:00, queued_by_job_id=33, pid=7876[0m
[[34m2024-07-06T00:03:54.456+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:58:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:03:54.456+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:03:54.456+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:58:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:03:54.458+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:03:54.459+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:58:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:03:54.459+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:58:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:03:54.462+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-05T23:58:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:03:56.043+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-05T23:58:45.013649+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:04:00.401+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-05T23:58:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:04:03.445+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-05T23:58:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:04:03.461+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-05T23:58:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:04:00.494500+00:00, run_end_date=2024-07-06 00:04:02.169519+00:00, run_duration=1.675019, state=success, executor_state=success, try_number=1, max_tries=0, job_id=101, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:03:54.457280+00:00, queued_by_job_id=33, pid=7879[0m
[[34m2024-07-06T00:04:03.528+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-05 23:58:45.013649+00:00: scheduled__2024-07-05T23:58:45.013649+00:00, state:running, queued_at: 2024-07-06 00:03:46.382898+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:04:03.529+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-05 23:58:45.013649+00:00, run_id=scheduled__2024-07-05T23:58:45.013649+00:00, run_start_date=2024-07-06 00:03:46.406646+00:00, run_end_date=2024-07-06 00:04:03.528936+00:00, run_duration=17.12229, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 23:58:45.013649+00:00, data_interval_end=2024-07-06 00:03:45.013649+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:04:03.532+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:03:45.013649+00:00, run_after=2024-07-06T00:08:45.013649+00:00[0m
[[34m2024-07-06T00:06:39.579+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:08:47.611+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:08:45.013649+00:00, run_after=2024-07-06T00:13:45.013649+00:00[0m
[[34m2024-07-06T00:08:47.661+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:03:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:08:47.662+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:08:47.662+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:03:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:08:47.664+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:08:47.664+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:03:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:08:47.665+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:03:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:08:47.669+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:03:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:08:49.243+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:03:45.013649+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:08:53.611+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:03:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:08:55.301+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:03:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:08:55.313+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:03:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:08:53.697945+00:00, run_end_date=2024-07-06 00:08:54.100354+00:00, run_duration=0.402409, state=success, executor_state=success, try_number=1, max_tries=0, job_id=102, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:08:47.662910+00:00, queued_by_job_id=33, pid=7954[0m
[[34m2024-07-06T00:08:59.729+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:03:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:08:59.730+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:08:59.730+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:03:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:08:59.732+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:08:59.732+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:03:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:08:59.733+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:03:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:08:59.736+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:03:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:09:01.266+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:03:45.013649+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:09:05.415+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:03:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:09:08.479+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:03:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:09:08.494+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:03:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:09:05.507487+00:00, run_end_date=2024-07-06 00:09:07.170174+00:00, run_duration=1.662687, state=success, executor_state=success, try_number=1, max_tries=0, job_id=103, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:08:59.731014+00:00, queued_by_job_id=33, pid=7959[0m
[[34m2024-07-06T00:09:08.564+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:03:45.013649+00:00: scheduled__2024-07-06T00:03:45.013649+00:00, state:running, queued_at: 2024-07-06 00:08:47.604039+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:09:08.564+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:03:45.013649+00:00, run_id=scheduled__2024-07-06T00:03:45.013649+00:00, run_start_date=2024-07-06 00:08:47.628207+00:00, run_end_date=2024-07-06 00:09:08.564660+00:00, run_duration=20.936453, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:03:45.013649+00:00, data_interval_end=2024-07-06 00:08:45.013649+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:09:08.568+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:08:45.013649+00:00, run_after=2024-07-06T00:13:45.013649+00:00[0m
[[34m2024-07-06T00:11:40.264+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:13:46.374+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:13:45.013649+00:00, run_after=2024-07-06T00:18:45.013649+00:00[0m
[[34m2024-07-06T00:13:46.421+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:08:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:13:46.422+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:13:46.422+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:08:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:13:46.424+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:13:46.425+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:08:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:13:46.425+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:08:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:13:46.428+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:08:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:13:48.028+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:08:45.013649+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:13:52.384+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:08:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:13:54.177+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:08:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:13:54.190+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:08:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:13:52.473001+00:00, run_end_date=2024-07-06 00:13:52.903304+00:00, run_duration=0.430303, state=success, executor_state=success, try_number=1, max_tries=0, job_id=104, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:13:46.422877+00:00, queued_by_job_id=33, pid=8027[0m
[[34m2024-07-06T00:13:58.503+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:08:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:13:58.504+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:13:58.504+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:08:45.013649+00:00 [scheduled]>[0m
[[34m2024-07-06T00:13:58.506+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:13:58.507+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:08:45.013649+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:13:58.507+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:08:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:13:58.515+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:08:45.013649+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:14:00.082+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:08:45.013649+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:14:04.434+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:08:45.013649+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:14:07.440+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:08:45.013649+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:14:07.453+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:08:45.013649+00:00, map_index=-1, run_start_date=2024-07-06 00:14:04.525493+00:00, run_end_date=2024-07-06 00:14:06.182851+00:00, run_duration=1.657358, state=success, executor_state=success, try_number=1, max_tries=0, job_id=105, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:13:58.505201+00:00, queued_by_job_id=33, pid=8034[0m
[[34m2024-07-06T00:14:07.519+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:08:45.013649+00:00: scheduled__2024-07-06T00:08:45.013649+00:00, state:running, queued_at: 2024-07-06 00:13:46.368062+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:14:07.520+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:08:45.013649+00:00, run_id=scheduled__2024-07-06T00:08:45.013649+00:00, run_start_date=2024-07-06 00:13:46.387756+00:00, run_end_date=2024-07-06 00:14:07.520507+00:00, run_duration=21.132751, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:08:45.013649+00:00, data_interval_end=2024-07-06 00:13:45.013649+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:14:07.524+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:13:45.013649+00:00, run_after=2024-07-06T00:18:45.013649+00:00[0m
[[34m2024-07-06T00:16:40.311+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:18:50.388+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:18:49.223032+00:00, run_after=2024-07-06T00:23:49.223032+00:00[0m
[[34m2024-07-06T00:18:50.435+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:13:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:18:50.436+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:18:50.436+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:13:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:18:50.438+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:18:50.439+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:13:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:18:50.439+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:13:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:18:50.442+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:13:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:18:52.067+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:13:49.223032+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:18:56.400+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:13:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:18:58.297+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:13:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:18:58.310+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:13:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:18:56.489166+00:00, run_end_date=2024-07-06 00:18:56.897368+00:00, run_duration=0.408202, state=success, executor_state=success, try_number=1, max_tries=0, job_id=106, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:18:50.437191+00:00, queued_by_job_id=33, pid=8094[0m
[[34m2024-07-06T00:18:58.388+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:13:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:18:58.388+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:18:58.388+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:13:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:18:58.390+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:18:58.391+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:13:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:18:58.391+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:13:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:18:58.395+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:13:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:18:59.945+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:13:49.223032+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:19:04.255+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:13:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:19:07.225+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:13:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:19:07.237+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:13:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:19:04.361061+00:00, run_end_date=2024-07-06 00:19:06.002296+00:00, run_duration=1.641235, state=success, executor_state=success, try_number=1, max_tries=0, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:18:58.389555+00:00, queued_by_job_id=33, pid=8097[0m
[[34m2024-07-06T00:19:07.309+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:13:49.223032+00:00: scheduled__2024-07-06T00:13:49.223032+00:00, state:running, queued_at: 2024-07-06 00:18:50.382064+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:19:07.309+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:13:49.223032+00:00, run_id=scheduled__2024-07-06T00:13:49.223032+00:00, run_start_date=2024-07-06 00:18:50.402254+00:00, run_end_date=2024-07-06 00:19:07.309800+00:00, run_duration=16.907546, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:13:49.223032+00:00, data_interval_end=2024-07-06 00:18:49.223032+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:19:07.313+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:18:49.223032+00:00, run_after=2024-07-06T00:23:49.223032+00:00[0m
[[34m2024-07-06T00:21:42.887+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:23:52.147+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:23:49.223032+00:00, run_after=2024-07-06T00:28:49.223032+00:00[0m
[[34m2024-07-06T00:23:52.196+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:18:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:23:52.197+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:23:52.197+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:18:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:23:52.199+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:23:52.199+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:18:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:23:52.200+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:18:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:23:52.203+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:18:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:23:53.784+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:18:49.223032+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:23:58.094+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:18:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:23:59.916+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:18:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:23:59.928+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:18:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:23:58.185105+00:00, run_end_date=2024-07-06 00:23:58.599617+00:00, run_duration=0.414512, state=success, executor_state=success, try_number=1, max_tries=0, job_id=108, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:23:52.197820+00:00, queued_by_job_id=33, pid=8153[0m
[[34m2024-07-06T00:24:04.133+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:18:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:24:04.134+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:24:04.134+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:18:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:24:04.136+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:24:04.137+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:18:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:24:04.137+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:18:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:24:04.140+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:18:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:24:05.654+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:18:49.223032+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:24:09.877+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:18:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:24:13.032+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:18:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:24:13.045+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:18:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:24:09.996274+00:00, run_end_date=2024-07-06 00:24:11.620227+00:00, run_duration=1.623953, state=success, executor_state=success, try_number=1, max_tries=0, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:24:04.135044+00:00, queued_by_job_id=33, pid=8159[0m
[[34m2024-07-06T00:24:13.118+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:18:49.223032+00:00: scheduled__2024-07-06T00:18:49.223032+00:00, state:running, queued_at: 2024-07-06 00:23:52.140686+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:24:13.119+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:18:49.223032+00:00, run_id=scheduled__2024-07-06T00:18:49.223032+00:00, run_start_date=2024-07-06 00:23:52.161768+00:00, run_end_date=2024-07-06 00:24:13.119469+00:00, run_duration=20.957701, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:18:49.223032+00:00, data_interval_end=2024-07-06 00:23:49.223032+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:24:13.122+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:23:49.223032+00:00, run_after=2024-07-06T00:28:49.223032+00:00[0m
[[34m2024-07-06T00:26:44.496+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:28:50.328+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:28:49.223032+00:00, run_after=2024-07-06T00:33:49.223032+00:00[0m
[[34m2024-07-06T00:28:50.373+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:23:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:28:50.374+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:28:50.374+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:23:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:28:50.376+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:28:50.376+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:23:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:28:50.377+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:23:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:28:50.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:23:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:28:51.906+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:23:49.223032+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:28:56.197+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:23:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:28:57.931+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:23:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:28:57.952+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:23:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:28:56.291606+00:00, run_end_date=2024-07-06 00:28:56.703663+00:00, run_duration=0.412057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=110, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:28:50.374951+00:00, queued_by_job_id=33, pid=8222[0m
[[34m2024-07-06T00:29:02.270+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:23:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:29:02.271+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:29:02.271+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:23:49.223032+00:00 [scheduled]>[0m
[[34m2024-07-06T00:29:02.273+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:29:02.273+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:23:49.223032+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:29:02.274+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:23:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:29:02.277+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:23:49.223032+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:29:03.762+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:23:49.223032+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:29:07.891+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:23:49.223032+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:29:10.925+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:23:49.223032+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:29:10.931+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:23:49.223032+00:00, map_index=-1, run_start_date=2024-07-06 00:29:07.976868+00:00, run_end_date=2024-07-06 00:29:09.598343+00:00, run_duration=1.621475, state=success, executor_state=success, try_number=1, max_tries=0, job_id=111, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:29:02.272023+00:00, queued_by_job_id=33, pid=8228[0m
[[34m2024-07-06T00:29:10.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:23:49.223032+00:00: scheduled__2024-07-06T00:23:49.223032+00:00, state:running, queued_at: 2024-07-06 00:28:50.323263+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:29:10.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:23:49.223032+00:00, run_id=scheduled__2024-07-06T00:23:49.223032+00:00, run_start_date=2024-07-06 00:28:50.342149+00:00, run_end_date=2024-07-06 00:29:10.993658+00:00, run_duration=20.651509, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:23:49.223032+00:00, data_interval_end=2024-07-06 00:28:49.223032+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:29:10.996+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:28:49.223032+00:00, run_after=2024-07-06T00:33:49.223032+00:00[0m
[[34m2024-07-06T00:31:44.551+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:33:53.982+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:33:52.807561+00:00, run_after=2024-07-06T00:38:52.807561+00:00[0m
[[34m2024-07-06T00:33:54.029+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:28:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:33:54.030+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:33:54.030+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:28:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:33:54.032+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:33:54.033+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:28:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:33:54.033+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:28:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:33:54.037+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:28:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:33:55.702+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:28:52.807561+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:34:00.259+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:28:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:34:02.181+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:28:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:34:02.196+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:28:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:34:00.351631+00:00, run_end_date=2024-07-06 00:34:00.809377+00:00, run_duration=0.457746, state=success, executor_state=success, try_number=1, max_tries=0, job_id=112, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:33:54.031043+00:00, queued_by_job_id=33, pid=8288[0m
[[34m2024-07-06T00:34:02.271+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:28:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:34:02.272+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:34:02.272+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:28:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:34:02.274+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:34:02.275+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:28:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:34:02.275+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:28:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:34:02.279+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:28:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:34:03.864+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:28:52.807561+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:34:08.108+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:28:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:34:10.941+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:28:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:34:10.953+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:28:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:34:08.194932+00:00, run_end_date=2024-07-06 00:34:09.677742+00:00, run_duration=1.48281, state=success, executor_state=success, try_number=1, max_tries=0, job_id=113, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:34:02.273197+00:00, queued_by_job_id=33, pid=8291[0m
[[34m2024-07-06T00:34:15.201+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:28:52.807561+00:00: scheduled__2024-07-06T00:28:52.807561+00:00, state:running, queued_at: 2024-07-06 00:33:53.976480+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:34:15.202+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:28:52.807561+00:00, run_id=scheduled__2024-07-06T00:28:52.807561+00:00, run_start_date=2024-07-06 00:33:53.995462+00:00, run_end_date=2024-07-06 00:34:15.202368+00:00, run_duration=21.206906, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:28:52.807561+00:00, data_interval_end=2024-07-06 00:33:52.807561+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:34:15.206+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:33:52.807561+00:00, run_after=2024-07-06T00:38:52.807561+00:00[0m
[[34m2024-07-06T00:36:45.371+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:38:53.628+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:38:52.807561+00:00, run_after=2024-07-06T00:43:52.807561+00:00[0m
[[34m2024-07-06T00:38:53.671+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:33:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:38:53.671+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:38:53.672+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:33:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:38:53.673+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:38:53.674+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:33:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:38:53.674+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:33:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:38:53.678+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:33:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:38:55.158+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:33:52.807561+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:38:59.374+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:33:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:39:01.284+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:33:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:39:01.299+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:33:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:38:59.472448+00:00, run_end_date=2024-07-06 00:38:59.887063+00:00, run_duration=0.414615, state=success, executor_state=success, try_number=1, max_tries=0, job_id=114, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:38:53.672548+00:00, queued_by_job_id=33, pid=8347[0m
[[34m2024-07-06T00:39:05.717+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:33:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:39:05.718+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:39:05.718+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:33:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:39:05.720+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:39:05.721+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:33:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:39:05.721+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:33:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:39:05.725+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:33:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:39:07.320+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:33:52.807561+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:39:11.433+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:33:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:39:14.459+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:33:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:39:14.469+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:33:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:39:11.519853+00:00, run_end_date=2024-07-06 00:39:13.082940+00:00, run_duration=1.563087, state=success, executor_state=success, try_number=1, max_tries=0, job_id=115, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:39:05.719433+00:00, queued_by_job_id=33, pid=8355[0m
[[34m2024-07-06T00:39:14.535+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:33:52.807561+00:00: scheduled__2024-07-06T00:33:52.807561+00:00, state:running, queued_at: 2024-07-06 00:38:53.623177+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:39:14.536+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:33:52.807561+00:00, run_id=scheduled__2024-07-06T00:33:52.807561+00:00, run_start_date=2024-07-06 00:38:53.641375+00:00, run_end_date=2024-07-06 00:39:14.536063+00:00, run_duration=20.894688, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:33:52.807561+00:00, data_interval_end=2024-07-06 00:38:52.807561+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:39:14.539+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:38:52.807561+00:00, run_after=2024-07-06T00:43:52.807561+00:00[0m
[[34m2024-07-06T00:41:45.424+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:43:53.574+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:43:52.807561+00:00, run_after=2024-07-06T00:48:52.807561+00:00[0m
[[34m2024-07-06T00:43:53.617+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:38:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:43:53.617+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:43:53.617+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:38:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:43:53.619+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:43:53.620+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:38:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:43:53.620+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:38:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:43:53.623+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:38:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:43:55.310+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:38:52.807561+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:43:59.726+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:38:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:44:01.512+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:38:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:44:01.525+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:38:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:43:59.816153+00:00, run_end_date=2024-07-06 00:44:00.279340+00:00, run_duration=0.463187, state=success, executor_state=success, try_number=1, max_tries=0, job_id=116, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:43:53.618258+00:00, queued_by_job_id=33, pid=8414[0m
[[34m2024-07-06T00:44:05.839+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:38:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:44:05.840+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:44:05.840+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:38:52.807561+00:00 [scheduled]>[0m
[[34m2024-07-06T00:44:05.842+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:44:05.843+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:38:52.807561+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:44:05.843+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:38:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:44:05.847+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:38:52.807561+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:44:07.384+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:38:52.807561+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:44:11.656+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:38:52.807561+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:44:14.684+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:38:52.807561+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:44:14.692+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:38:52.807561+00:00, map_index=-1, run_start_date=2024-07-06 00:44:11.752910+00:00, run_end_date=2024-07-06 00:44:13.422789+00:00, run_duration=1.669879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=117, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:44:05.841454+00:00, queued_by_job_id=33, pid=8422[0m
[[34m2024-07-06T00:44:18.889+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:38:52.807561+00:00: scheduled__2024-07-06T00:38:52.807561+00:00, state:running, queued_at: 2024-07-06 00:43:53.569556+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:44:18.890+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:38:52.807561+00:00, run_id=scheduled__2024-07-06T00:38:52.807561+00:00, run_start_date=2024-07-06 00:43:53.586955+00:00, run_end_date=2024-07-06 00:44:18.889903+00:00, run_duration=25.302948, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:38:52.807561+00:00, data_interval_end=2024-07-06 00:43:52.807561+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:44:18.899+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:43:52.807561+00:00, run_after=2024-07-06T00:48:52.807561+00:00[0m
[[34m2024-07-06T00:46:45.470+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:48:55.286+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:48:54.068315+00:00, run_after=2024-07-06T00:53:54.068315+00:00[0m
[[34m2024-07-06T00:48:55.331+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:43:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:48:55.331+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:48:55.332+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:43:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:48:55.333+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:48:55.334+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:43:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:48:55.334+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:43:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:48:55.337+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:43:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:48:56.938+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:43:54.068315+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:49:01.205+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:43:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:49:02.897+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:43:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:49:02.909+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:43:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:49:01.291475+00:00, run_end_date=2024-07-06 00:49:01.726396+00:00, run_duration=0.434921, state=success, executor_state=success, try_number=1, max_tries=0, job_id=118, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:48:55.332634+00:00, queued_by_job_id=33, pid=8485[0m
[[34m2024-07-06T00:49:02.992+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:43:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:49:02.992+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:49:02.992+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:43:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:49:02.994+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:49:02.995+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:43:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:49:02.995+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:43:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:49:02.998+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:43:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:49:04.482+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:43:54.068315+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:49:08.730+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:43:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:49:11.642+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:43:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:49:11.656+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:43:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:49:08.821115+00:00, run_end_date=2024-07-06 00:49:10.455369+00:00, run_duration=1.634254, state=success, executor_state=success, try_number=1, max_tries=0, job_id=119, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:49:02.993200+00:00, queued_by_job_id=33, pid=8489[0m
[[34m2024-07-06T00:49:11.741+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:43:54.068315+00:00: scheduled__2024-07-06T00:43:54.068315+00:00, state:running, queued_at: 2024-07-06 00:48:55.281265+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:49:11.742+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:43:54.068315+00:00, run_id=scheduled__2024-07-06T00:43:54.068315+00:00, run_start_date=2024-07-06 00:48:55.301965+00:00, run_end_date=2024-07-06 00:49:11.742302+00:00, run_duration=16.440337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:43:54.068315+00:00, data_interval_end=2024-07-06 00:48:54.068315+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:49:11.745+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:48:54.068315+00:00, run_after=2024-07-06T00:53:54.068315+00:00[0m
[[34m2024-07-06T00:51:47.641+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:53:55.201+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:53:54.068315+00:00, run_after=2024-07-06T00:58:54.068315+00:00[0m
[[34m2024-07-06T00:53:55.249+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:48:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:53:55.250+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:53:55.250+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:48:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:53:55.253+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:53:55.254+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:48:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:53:55.255+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:48:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:53:55.258+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:48:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:53:56.755+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:48:54.068315+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:54:01.049+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:48:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:54:02.749+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:48:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:54:02.761+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:48:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:54:01.158647+00:00, run_end_date=2024-07-06 00:54:01.561221+00:00, run_duration=0.402574, state=success, executor_state=success, try_number=1, max_tries=0, job_id=120, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:53:55.251693+00:00, queued_by_job_id=33, pid=8552[0m
[[34m2024-07-06T00:54:07.014+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:48:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:54:07.015+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:54:07.015+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:48:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:54:07.017+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:54:07.018+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:48:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:54:07.018+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:48:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:54:07.022+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:48:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:54:08.615+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:48:54.068315+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:54:12.774+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:48:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:54:15.653+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:48:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:54:15.667+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:48:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:54:12.863530+00:00, run_end_date=2024-07-06 00:54:14.274398+00:00, run_duration=1.410868, state=success, executor_state=success, try_number=1, max_tries=0, job_id=121, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:54:07.016216+00:00, queued_by_job_id=33, pid=8557[0m
[[34m2024-07-06T00:54:15.740+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:48:54.068315+00:00: scheduled__2024-07-06T00:48:54.068315+00:00, state:running, queued_at: 2024-07-06 00:53:55.195202+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:54:15.741+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:48:54.068315+00:00, run_id=scheduled__2024-07-06T00:48:54.068315+00:00, run_start_date=2024-07-06 00:53:55.214678+00:00, run_end_date=2024-07-06 00:54:15.741040+00:00, run_duration=20.526362, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:48:54.068315+00:00, data_interval_end=2024-07-06 00:53:54.068315+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:54:15.744+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:53:54.068315+00:00, run_after=2024-07-06T00:58:54.068315+00:00[0m
[[34m2024-07-06T00:56:48.552+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T00:58:55.577+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:58:54.068315+00:00, run_after=2024-07-06T01:03:54.068315+00:00[0m
[[34m2024-07-06T00:58:55.624+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:53:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:58:55.625+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:58:55.625+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:53:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:58:55.627+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:58:55.628+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:53:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T00:58:55.628+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:53:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:58:55.632+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:53:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:58:57.308+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:53:54.068315+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T00:59:01.609+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:53:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:59:03.412+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:53:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:59:03.425+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:53:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:59:01.731230+00:00, run_end_date=2024-07-06 00:59:02.147269+00:00, run_duration=0.416039, state=success, executor_state=success, try_number=1, max_tries=0, job_id=122, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 00:58:55.626061+00:00, queued_by_job_id=33, pid=8617[0m
[[34m2024-07-06T00:59:07.661+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:53:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:59:07.662+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T00:59:07.662+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:53:54.068315+00:00 [scheduled]>[0m
[[34m2024-07-06T00:59:07.664+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T00:59:07.665+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:53:54.068315+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T00:59:07.665+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:53:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:59:07.668+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:53:54.068315+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T00:59:09.231+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:53:54.068315+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T00:59:13.469+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:53:54.068315+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T00:59:16.518+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:53:54.068315+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T00:59:16.526+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:53:54.068315+00:00, map_index=-1, run_start_date=2024-07-06 00:59:13.559625+00:00, run_end_date=2024-07-06 00:59:15.248749+00:00, run_duration=1.689124, state=success, executor_state=success, try_number=1, max_tries=0, job_id=123, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 00:59:07.663096+00:00, queued_by_job_id=33, pid=8628[0m
[[34m2024-07-06T00:59:20.820+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:53:54.068315+00:00: scheduled__2024-07-06T00:53:54.068315+00:00, state:running, queued_at: 2024-07-06 00:58:55.567612+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T00:59:20.821+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:53:54.068315+00:00, run_id=scheduled__2024-07-06T00:53:54.068315+00:00, run_start_date=2024-07-06 00:58:55.591216+00:00, run_end_date=2024-07-06 00:59:20.821676+00:00, run_duration=25.23046, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:53:54.068315+00:00, data_interval_end=2024-07-06 00:58:54.068315+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T00:59:20.826+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T00:58:54.068315+00:00, run_after=2024-07-06T01:03:54.068315+00:00[0m
[[34m2024-07-06T01:01:48.597+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:03:58.021+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:03:56.850587+00:00, run_after=2024-07-06T01:08:56.850587+00:00[0m
[[34m2024-07-06T01:03:58.071+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:58:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:03:58.071+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:03:58.072+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:58:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:03:58.074+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:03:58.081+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:58:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:03:58.082+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:58:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:03:58.087+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T00:58:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:03:59.563+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:58:56.850587+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:04:03.646+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T00:58:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:04:05.510+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T00:58:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:04:05.516+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T00:58:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:04:03.731602+00:00, run_end_date=2024-07-06 01:04:04.140464+00:00, run_duration=0.408862, state=success, executor_state=success, try_number=1, max_tries=0, job_id=124, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:03:58.072772+00:00, queued_by_job_id=33, pid=8682[0m
[[34m2024-07-06T01:04:05.582+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:58:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:04:05.583+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:04:05.583+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:58:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:04:05.585+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:04:05.586+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:58:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:04:05.586+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:58:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:04:05.589+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T00:58:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:04:07.097+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T00:58:56.850587+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:04:11.195+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T00:58:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:04:14.242+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T00:58:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:04:14.256+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T00:58:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:04:11.293640+00:00, run_end_date=2024-07-06 01:04:12.930426+00:00, run_duration=1.636786, state=success, executor_state=success, try_number=1, max_tries=0, job_id=125, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:04:05.584174+00:00, queued_by_job_id=33, pid=8685[0m
[[34m2024-07-06T01:04:14.336+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 00:58:56.850587+00:00: scheduled__2024-07-06T00:58:56.850587+00:00, state:running, queued_at: 2024-07-06 01:03:58.003104+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:04:14.338+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 00:58:56.850587+00:00, run_id=scheduled__2024-07-06T00:58:56.850587+00:00, run_start_date=2024-07-06 01:03:58.039277+00:00, run_end_date=2024-07-06 01:04:14.338381+00:00, run_duration=16.299104, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:58:56.850587+00:00, data_interval_end=2024-07-06 01:03:56.850587+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:04:14.341+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:03:56.850587+00:00, run_after=2024-07-06T01:08:56.850587+00:00[0m
[[34m2024-07-06T01:06:48.895+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:08:57.137+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:08:56.850587+00:00, run_after=2024-07-06T01:13:56.850587+00:00[0m
[[34m2024-07-06T01:08:57.191+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:03:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:08:57.191+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:08:57.191+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:03:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:08:57.193+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:08:57.194+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:03:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:08:57.194+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:03:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:08:57.197+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:03:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:08:58.762+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:03:56.850587+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:09:03.133+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:03:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:09:04.892+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:03:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:09:04.905+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:03:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:09:03.221374+00:00, run_end_date=2024-07-06 01:09:03.647824+00:00, run_duration=0.42645, state=success, executor_state=success, try_number=1, max_tries=0, job_id=126, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:08:57.192370+00:00, queued_by_job_id=33, pid=8753[0m
[[34m2024-07-06T01:09:09.226+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:03:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:09:09.227+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:09:09.227+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:03:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:09:09.229+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:09:09.230+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:03:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:09:09.230+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:03:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:09:09.233+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:03:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:09:10.738+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:03:56.850587+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:09:14.881+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:03:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:09:17.850+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:03:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:09:17.863+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:03:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:09:14.968072+00:00, run_end_date=2024-07-06 01:09:16.572814+00:00, run_duration=1.604742, state=success, executor_state=success, try_number=1, max_tries=0, job_id=127, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:09:09.228094+00:00, queued_by_job_id=33, pid=8758[0m
[[34m2024-07-06T01:09:17.932+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:03:56.850587+00:00: scheduled__2024-07-06T01:03:56.850587+00:00, state:running, queued_at: 2024-07-06 01:08:57.132044+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:09:17.933+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:03:56.850587+00:00, run_id=scheduled__2024-07-06T01:03:56.850587+00:00, run_start_date=2024-07-06 01:08:57.154584+00:00, run_end_date=2024-07-06 01:09:17.932881+00:00, run_duration=20.778297, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:03:56.850587+00:00, data_interval_end=2024-07-06 01:08:56.850587+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:09:17.936+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:08:56.850587+00:00, run_after=2024-07-06T01:13:56.850587+00:00[0m
[[34m2024-07-06T01:11:50.340+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:13:57.171+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:13:56.850587+00:00, run_after=2024-07-06T01:18:56.850587+00:00[0m
[[34m2024-07-06T01:13:57.218+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:08:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:13:57.219+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:13:57.219+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:08:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:13:57.221+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:13:57.221+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:08:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:13:57.222+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:08:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:13:57.225+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:08:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:13:58.839+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:08:56.850587+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:14:03.202+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:08:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:14:04.969+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:08:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:14:04.983+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:08:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:14:03.299154+00:00, run_end_date=2024-07-06 01:14:03.735654+00:00, run_duration=0.4365, state=success, executor_state=success, try_number=1, max_tries=0, job_id=128, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:13:57.219896+00:00, queued_by_job_id=33, pid=8820[0m
[[34m2024-07-06T01:14:09.181+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:08:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:14:09.182+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:14:09.182+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:08:56.850587+00:00 [scheduled]>[0m
[[34m2024-07-06T01:14:09.184+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:14:09.185+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:08:56.850587+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:14:09.185+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:08:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:14:09.189+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:08:56.850587+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:14:10.679+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:08:56.850587+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:14:14.827+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:08:56.850587+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:14:17.655+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:08:56.850587+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:14:17.669+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:08:56.850587+00:00, map_index=-1, run_start_date=2024-07-06 01:14:14.944636+00:00, run_end_date=2024-07-06 01:14:16.315412+00:00, run_duration=1.370776, state=success, executor_state=success, try_number=1, max_tries=0, job_id=129, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:14:09.183025+00:00, queued_by_job_id=33, pid=8825[0m
[[34m2024-07-06T01:14:17.740+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:08:56.850587+00:00: scheduled__2024-07-06T01:08:56.850587+00:00, state:running, queued_at: 2024-07-06 01:13:57.165924+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:14:17.740+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:08:56.850587+00:00, run_id=scheduled__2024-07-06T01:08:56.850587+00:00, run_start_date=2024-07-06 01:13:57.186783+00:00, run_end_date=2024-07-06 01:14:17.740555+00:00, run_duration=20.553772, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:08:56.850587+00:00, data_interval_end=2024-07-06 01:13:56.850587+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:14:17.743+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:13:56.850587+00:00, run_after=2024-07-06T01:18:56.850587+00:00[0m
[[34m2024-07-06T01:16:50.411+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:18:59.072+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:18:58.924873+00:00, run_after=2024-07-06T01:23:58.924873+00:00[0m
[[34m2024-07-06T01:18:59.115+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:13:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:18:59.115+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:18:59.115+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:13:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:18:59.117+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:18:59.118+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:13:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:18:59.118+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:13:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:18:59.121+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:13:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:19:00.615+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:13:58.924873+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:19:04.911+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:13:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:19:06.635+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:13:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:19:06.648+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:13:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:19:05.015680+00:00, run_end_date=2024-07-06 01:19:05.402918+00:00, run_duration=0.387238, state=success, executor_state=success, try_number=1, max_tries=0, job_id=130, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:18:59.116172+00:00, queued_by_job_id=33, pid=8890[0m
[[34m2024-07-06T01:19:06.737+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:13:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:19:06.738+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:19:06.738+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:13:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:19:06.739+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:19:06.740+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:13:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:19:06.741+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:13:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:19:06.744+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:13:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:19:08.356+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:13:58.924873+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:19:12.665+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:13:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:19:15.625+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:13:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:19:15.638+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:13:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:19:12.753111+00:00, run_end_date=2024-07-06 01:19:14.370592+00:00, run_duration=1.617481, state=success, executor_state=success, try_number=1, max_tries=0, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:19:06.738778+00:00, queued_by_job_id=33, pid=8893[0m
[[34m2024-07-06T01:19:15.696+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:13:58.924873+00:00: scheduled__2024-07-06T01:13:58.924873+00:00, state:running, queued_at: 2024-07-06 01:18:59.065889+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:19:15.697+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:13:58.924873+00:00, run_id=scheduled__2024-07-06T01:13:58.924873+00:00, run_start_date=2024-07-06 01:18:59.084875+00:00, run_end_date=2024-07-06 01:19:15.697017+00:00, run_duration=16.612142, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:13:58.924873+00:00, data_interval_end=2024-07-06 01:18:58.924873+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:19:15.700+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:18:58.924873+00:00, run_after=2024-07-06T01:23:58.924873+00:00[0m
[[34m2024-07-06T01:21:53.064+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:23:59.801+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:23:58.924873+00:00, run_after=2024-07-06T01:28:58.924873+00:00[0m
[[34m2024-07-06T01:23:59.852+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:18:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:23:59.853+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:23:59.853+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:18:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:23:59.855+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:23:59.856+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:18:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:23:59.856+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:18:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:23:59.860+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:18:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:24:01.530+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:18:58.924873+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:24:06.483+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:18:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:24:08.305+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:18:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:24:08.319+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:18:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:24:06.620694+00:00, run_end_date=2024-07-06 01:24:07.036573+00:00, run_duration=0.415879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=132, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:23:59.854191+00:00, queued_by_job_id=33, pid=8957[0m
[[34m2024-07-06T01:24:12.620+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:18:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:24:12.621+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:24:12.621+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:18:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:24:12.627+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:24:12.627+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:18:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:24:12.628+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:18:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:24:12.635+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:18:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:24:14.182+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:18:58.924873+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:24:18.416+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:18:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:24:21.119+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:18:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:24:21.132+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:18:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:24:18.508713+00:00, run_end_date=2024-07-06 01:24:19.937023+00:00, run_duration=1.42831, state=success, executor_state=success, try_number=1, max_tries=0, job_id=133, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:24:12.622037+00:00, queued_by_job_id=33, pid=8965[0m
[[34m2024-07-06T01:24:21.207+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:18:58.924873+00:00: scheduled__2024-07-06T01:18:58.924873+00:00, state:running, queued_at: 2024-07-06 01:23:59.794632+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:24:21.208+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:18:58.924873+00:00, run_id=scheduled__2024-07-06T01:18:58.924873+00:00, run_start_date=2024-07-06 01:23:59.817125+00:00, run_end_date=2024-07-06 01:24:21.208232+00:00, run_duration=21.391107, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:18:58.924873+00:00, data_interval_end=2024-07-06 01:23:58.924873+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:24:21.212+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:23:58.924873+00:00, run_after=2024-07-06T01:28:58.924873+00:00[0m
[[34m2024-07-06T01:26:53.110+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:28:59.630+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:28:58.924873+00:00, run_after=2024-07-06T01:33:58.924873+00:00[0m
[[34m2024-07-06T01:28:59.676+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:23:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:28:59.677+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:28:59.677+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:23:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:28:59.679+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:28:59.680+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:23:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:28:59.680+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:23:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:28:59.684+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:23:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:29:01.259+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:23:58.924873+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:29:05.563+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:23:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:29:07.425+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:23:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:29:07.440+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:23:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:29:05.653194+00:00, run_end_date=2024-07-06 01:29:06.079174+00:00, run_duration=0.42598, state=success, executor_state=success, try_number=1, max_tries=0, job_id=134, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:28:59.678176+00:00, queued_by_job_id=33, pid=9020[0m
[[34m2024-07-06T01:29:11.864+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:23:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:29:11.865+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:29:11.865+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:23:58.924873+00:00 [scheduled]>[0m
[[34m2024-07-06T01:29:11.867+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:29:11.868+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:23:58.924873+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:29:11.868+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:23:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:29:11.871+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:23:58.924873+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:29:13.459+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:23:58.924873+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:29:17.707+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:23:58.924873+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:29:20.784+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:23:58.924873+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:29:20.797+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:23:58.924873+00:00, map_index=-1, run_start_date=2024-07-06 01:29:17.791510+00:00, run_end_date=2024-07-06 01:29:19.447280+00:00, run_duration=1.65577, state=success, executor_state=success, try_number=1, max_tries=0, job_id=135, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:29:11.866163+00:00, queued_by_job_id=33, pid=9027[0m
[[34m2024-07-06T01:29:20.864+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:23:58.924873+00:00: scheduled__2024-07-06T01:23:58.924873+00:00, state:running, queued_at: 2024-07-06 01:28:59.621587+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:29:20.865+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:23:58.924873+00:00, run_id=scheduled__2024-07-06T01:23:58.924873+00:00, run_start_date=2024-07-06 01:28:59.643882+00:00, run_end_date=2024-07-06 01:29:20.864998+00:00, run_duration=21.221116, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:23:58.924873+00:00, data_interval_end=2024-07-06 01:28:58.924873+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:29:20.868+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:28:58.924873+00:00, run_after=2024-07-06T01:33:58.924873+00:00[0m
[[34m2024-07-06T01:31:53.157+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:34:03.397+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:34:02.220753+00:00, run_after=2024-07-06T01:39:02.220753+00:00[0m
[[34m2024-07-06T01:34:03.448+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:29:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:34:03.449+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:34:03.449+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:29:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:34:03.451+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:34:03.452+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:29:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:34:03.452+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:29:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:34:03.457+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:29:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:34:04.980+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:29:02.220753+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:34:09.405+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:29:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:34:11.225+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:29:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:34:11.238+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:29:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:34:09.498399+00:00, run_end_date=2024-07-06 01:34:09.931253+00:00, run_duration=0.432854, state=success, executor_state=success, try_number=1, max_tries=0, job_id=136, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:34:03.450252+00:00, queued_by_job_id=33, pid=9094[0m
[[34m2024-07-06T01:34:11.321+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:29:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:34:11.321+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:34:11.322+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:29:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:34:11.324+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:34:11.325+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:29:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:34:11.325+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:29:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:34:11.328+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:29:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:34:12.854+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:29:02.220753+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:34:17.031+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:29:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:34:20.190+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:29:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:34:20.203+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:29:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:34:17.122170+00:00, run_end_date=2024-07-06 01:34:18.812329+00:00, run_duration=1.690159, state=success, executor_state=success, try_number=1, max_tries=0, job_id=137, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:34:11.322962+00:00, queued_by_job_id=33, pid=9097[0m
[[34m2024-07-06T01:34:24.315+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:29:02.220753+00:00: scheduled__2024-07-06T01:29:02.220753+00:00, state:running, queued_at: 2024-07-06 01:34:03.392017+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:34:24.315+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:29:02.220753+00:00, run_id=scheduled__2024-07-06T01:29:02.220753+00:00, run_start_date=2024-07-06 01:34:03.415999+00:00, run_end_date=2024-07-06 01:34:24.315493+00:00, run_duration=20.899494, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:29:02.220753+00:00, data_interval_end=2024-07-06 01:34:02.220753+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:34:24.318+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:34:02.220753+00:00, run_after=2024-07-06T01:39:02.220753+00:00[0m
[[34m2024-07-06T01:36:54.635+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:39:03.725+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:39:02.220753+00:00, run_after=2024-07-06T01:44:02.220753+00:00[0m
[[34m2024-07-06T01:39:03.771+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:34:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:39:03.771+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:39:03.772+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:34:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:39:03.773+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:39:03.774+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:34:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:39:03.775+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:34:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:39:03.778+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:34:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:39:05.368+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:34:02.220753+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:39:09.666+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:34:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:39:11.444+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:34:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:39:11.457+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:34:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:39:09.751801+00:00, run_end_date=2024-07-06 01:39:10.167678+00:00, run_duration=0.415877, state=success, executor_state=success, try_number=1, max_tries=0, job_id=138, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:39:03.772581+00:00, queued_by_job_id=33, pid=9160[0m
[[34m2024-07-06T01:39:15.674+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:34:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:39:15.674+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:39:15.675+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:34:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:39:15.677+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:39:15.677+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:34:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:39:15.678+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:34:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:39:15.682+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:34:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:39:17.190+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:34:02.220753+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:39:21.299+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:34:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:39:24.390+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:34:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:39:24.402+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:34:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:39:21.395226+00:00, run_end_date=2024-07-06 01:39:23.088666+00:00, run_duration=1.69344, state=success, executor_state=success, try_number=1, max_tries=0, job_id=139, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:39:15.675575+00:00, queued_by_job_id=33, pid=9165[0m
[[34m2024-07-06T01:39:24.473+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:34:02.220753+00:00: scheduled__2024-07-06T01:34:02.220753+00:00, state:running, queued_at: 2024-07-06 01:39:03.720217+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:39:24.473+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:34:02.220753+00:00, run_id=scheduled__2024-07-06T01:34:02.220753+00:00, run_start_date=2024-07-06 01:39:03.739409+00:00, run_end_date=2024-07-06 01:39:24.473530+00:00, run_duration=20.734121, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:34:02.220753+00:00, data_interval_end=2024-07-06 01:39:02.220753+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:39:24.477+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:39:02.220753+00:00, run_after=2024-07-06T01:44:02.220753+00:00[0m
[[34m2024-07-06T01:41:55.592+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:44:03.179+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:44:02.220753+00:00, run_after=2024-07-06T01:49:02.220753+00:00[0m
[[34m2024-07-06T01:44:03.228+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:39:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:44:03.228+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:44:03.228+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:39:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:44:03.230+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:44:03.231+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:39:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:44:03.231+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:39:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:44:03.234+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:39:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:44:04.867+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:39:02.220753+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:44:09.310+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:39:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:44:11.158+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:39:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:44:11.170+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:39:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:44:09.401877+00:00, run_end_date=2024-07-06 01:44:09.844948+00:00, run_duration=0.443071, state=success, executor_state=success, try_number=1, max_tries=0, job_id=140, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:44:03.229370+00:00, queued_by_job_id=33, pid=9228[0m
[[34m2024-07-06T01:44:15.281+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:39:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:44:15.281+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:44:15.281+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:39:02.220753+00:00 [scheduled]>[0m
[[34m2024-07-06T01:44:15.283+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:44:15.284+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:39:02.220753+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:44:15.284+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:39:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:44:15.287+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:39:02.220753+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:44:16.787+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:39:02.220753+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:44:20.947+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:39:02.220753+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:44:23.953+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:39:02.220753+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:44:23.966+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:39:02.220753+00:00, map_index=-1, run_start_date=2024-07-06 01:44:21.051862+00:00, run_end_date=2024-07-06 01:44:22.650244+00:00, run_duration=1.598382, state=success, executor_state=success, try_number=1, max_tries=0, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:44:15.282439+00:00, queued_by_job_id=33, pid=9233[0m
[[34m2024-07-06T01:44:28.178+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:39:02.220753+00:00: scheduled__2024-07-06T01:39:02.220753+00:00, state:running, queued_at: 2024-07-06 01:44:03.173401+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:44:28.179+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:39:02.220753+00:00, run_id=scheduled__2024-07-06T01:39:02.220753+00:00, run_start_date=2024-07-06 01:44:03.194273+00:00, run_end_date=2024-07-06 01:44:28.179466+00:00, run_duration=24.985193, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:39:02.220753+00:00, data_interval_end=2024-07-06 01:44:02.220753+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:44:28.182+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:44:02.220753+00:00, run_after=2024-07-06T01:49:02.220753+00:00[0m
[[34m2024-07-06T01:46:55.638+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:49:05.480+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:49:04.282546+00:00, run_after=2024-07-06T01:54:04.282546+00:00[0m
[[34m2024-07-06T01:49:05.526+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:44:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:49:05.526+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:49:05.527+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:44:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:49:05.528+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:49:05.529+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:44:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:49:05.529+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:44:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:49:05.533+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:44:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:49:07.091+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:44:04.282546+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:49:11.412+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:44:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:49:13.294+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:44:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:49:13.307+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:44:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:49:11.502582+00:00, run_end_date=2024-07-06 01:49:11.953650+00:00, run_duration=0.451068, state=success, executor_state=success, try_number=1, max_tries=0, job_id=142, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:49:05.527592+00:00, queued_by_job_id=33, pid=9300[0m
[[34m2024-07-06T01:49:13.384+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:44:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:49:13.384+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:49:13.384+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:44:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:49:13.386+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:49:13.387+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:44:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:49:13.387+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:44:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:49:13.391+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:44:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:49:14.905+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:44:04.282546+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:49:19.256+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:44:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:49:22.249+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:44:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:49:22.262+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:44:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:49:19.347546+00:00, run_end_date=2024-07-06 01:49:20.985941+00:00, run_duration=1.638395, state=success, executor_state=success, try_number=1, max_tries=0, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:49:13.385345+00:00, queued_by_job_id=33, pid=9303[0m
[[34m2024-07-06T01:49:26.465+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:44:04.282546+00:00: scheduled__2024-07-06T01:44:04.282546+00:00, state:running, queued_at: 2024-07-06 01:49:05.475185+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:49:26.466+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:44:04.282546+00:00, run_id=scheduled__2024-07-06T01:44:04.282546+00:00, run_start_date=2024-07-06 01:49:05.493360+00:00, run_end_date=2024-07-06 01:49:26.466345+00:00, run_duration=20.972985, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:44:04.282546+00:00, data_interval_end=2024-07-06 01:49:04.282546+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:49:26.469+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:49:04.282546+00:00, run_after=2024-07-06T01:54:04.282546+00:00[0m
[[34m2024-07-06T01:51:56.827+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:54:05.912+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:54:04.282546+00:00, run_after=2024-07-06T01:59:04.282546+00:00[0m
[[34m2024-07-06T01:54:05.957+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:49:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:54:05.957+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:54:05.957+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:49:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:54:05.959+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:54:05.959+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:49:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:54:05.960+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:49:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:54:05.963+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:49:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:54:07.453+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:49:04.282546+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:54:11.706+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:49:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:54:13.418+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:49:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:54:13.450+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:49:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:54:11.792656+00:00, run_end_date=2024-07-06 01:54:12.200911+00:00, run_duration=0.408255, state=success, executor_state=success, try_number=1, max_tries=0, job_id=144, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:54:05.958220+00:00, queued_by_job_id=33, pid=9362[0m
[[34m2024-07-06T01:54:17.656+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:49:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:54:17.656+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:54:17.656+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:49:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:54:17.658+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:54:17.659+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:49:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:54:17.660+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:49:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:54:17.663+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:49:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:54:19.179+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:49:04.282546+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:54:23.326+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:49:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:54:26.523+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:49:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:54:26.536+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:49:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:54:23.416316+00:00, run_end_date=2024-07-06 01:54:25.121824+00:00, run_duration=1.705508, state=success, executor_state=success, try_number=1, max_tries=0, job_id=145, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:54:17.657568+00:00, queued_by_job_id=33, pid=9367[0m
[[34m2024-07-06T01:54:26.611+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:49:04.282546+00:00: scheduled__2024-07-06T01:49:04.282546+00:00, state:running, queued_at: 2024-07-06 01:54:05.908055+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:54:26.611+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:49:04.282546+00:00, run_id=scheduled__2024-07-06T01:49:04.282546+00:00, run_start_date=2024-07-06 01:54:05.927615+00:00, run_end_date=2024-07-06 01:54:26.611698+00:00, run_duration=20.684083, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:49:04.282546+00:00, data_interval_end=2024-07-06 01:54:04.282546+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:54:26.614+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:54:04.282546+00:00, run_after=2024-07-06T01:59:04.282546+00:00[0m
[[34m2024-07-06T01:56:58.301+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T01:59:05.466+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:59:04.282546+00:00, run_after=2024-07-06T02:04:04.282546+00:00[0m
[[34m2024-07-06T01:59:05.512+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:54:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:59:05.512+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:59:05.512+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:54:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:59:05.514+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:59:05.515+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:54:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T01:59:05.515+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:54:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:59:05.519+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:54:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:59:07.111+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:54:04.282546+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T01:59:11.498+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:54:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:59:13.293+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:54:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:59:13.305+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:54:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:59:11.584712+00:00, run_end_date=2024-07-06 01:59:11.992514+00:00, run_duration=0.407802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=146, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 01:59:05.513268+00:00, queued_by_job_id=33, pid=9435[0m
[[34m2024-07-06T01:59:17.511+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:54:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:59:17.511+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T01:59:17.511+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:54:04.282546+00:00 [scheduled]>[0m
[[34m2024-07-06T01:59:17.513+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T01:59:17.514+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:54:04.282546+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T01:59:17.514+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:54:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:59:17.517+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:54:04.282546+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T01:59:18.992+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:54:04.282546+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T01:59:23.175+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:54:04.282546+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T01:59:26.169+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:54:04.282546+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T01:59:26.182+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:54:04.282546+00:00, map_index=-1, run_start_date=2024-07-06 01:59:23.273907+00:00, run_end_date=2024-07-06 01:59:24.903246+00:00, run_duration=1.629339, state=success, executor_state=success, try_number=1, max_tries=0, job_id=147, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 01:59:17.512358+00:00, queued_by_job_id=33, pid=9440[0m
[[34m2024-07-06T01:59:30.272+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:54:04.282546+00:00: scheduled__2024-07-06T01:54:04.282546+00:00, state:running, queued_at: 2024-07-06 01:59:05.461128+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T01:59:30.272+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:54:04.282546+00:00, run_id=scheduled__2024-07-06T01:54:04.282546+00:00, run_start_date=2024-07-06 01:59:05.479401+00:00, run_end_date=2024-07-06 01:59:30.272560+00:00, run_duration=24.793159, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:54:04.282546+00:00, data_interval_end=2024-07-06 01:59:04.282546+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T01:59:30.276+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T01:59:04.282546+00:00, run_after=2024-07-06T02:04:04.282546+00:00[0m
[[34m2024-07-06T02:01:58.348+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:04:06.995+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:04:05.232560+00:00, run_after=2024-07-06T02:09:05.232560+00:00[0m
[[34m2024-07-06T02:04:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:59:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:04:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:04:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:59:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:04:07.046+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:04:07.047+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:59:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:04:07.047+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:59:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:04:07.051+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T01:59:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:04:08.651+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:59:05.232560+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:04:13.040+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T01:59:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:04:14.833+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T01:59:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:04:14.845+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T01:59:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:04:13.130032+00:00, run_end_date=2024-07-06 02:04:13.559263+00:00, run_duration=0.429231, state=success, executor_state=success, try_number=1, max_tries=0, job_id=148, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:04:07.045320+00:00, queued_by_job_id=33, pid=9492[0m
[[34m2024-07-06T02:04:14.919+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:59:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:04:14.920+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:04:14.920+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:59:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:04:14.922+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:04:14.923+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:59:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:04:14.924+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:59:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:04:14.927+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T01:59:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:04:16.481+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T01:59:05.232560+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:04:20.661+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T01:59:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:04:23.623+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T01:59:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:04:23.635+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T01:59:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:04:20.747802+00:00, run_end_date=2024-07-06 02:04:22.379491+00:00, run_duration=1.631689, state=success, executor_state=success, try_number=1, max_tries=0, job_id=149, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:04:14.921495+00:00, queued_by_job_id=33, pid=9495[0m
[[34m2024-07-06T02:04:23.704+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 01:59:05.232560+00:00: scheduled__2024-07-06T01:59:05.232560+00:00, state:running, queued_at: 2024-07-06 02:04:06.990474+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:04:23.705+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 01:59:05.232560+00:00, run_id=scheduled__2024-07-06T01:59:05.232560+00:00, run_start_date=2024-07-06 02:04:07.009041+00:00, run_end_date=2024-07-06 02:04:23.704996+00:00, run_duration=16.695955, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 01:59:05.232560+00:00, data_interval_end=2024-07-06 02:04:05.232560+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:04:23.708+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:04:05.232560+00:00, run_after=2024-07-06T02:09:05.232560+00:00[0m
[[34m2024-07-06T02:06:58.410+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:09:07.609+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:09:05.232560+00:00, run_after=2024-07-06T02:14:05.232560+00:00[0m
[[34m2024-07-06T02:09:07.661+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:04:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:09:07.661+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:09:07.661+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:04:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:09:07.663+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:09:07.664+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:04:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:09:07.664+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:04:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:09:07.668+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:04:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:09:09.196+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:04:05.232560+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:09:13.441+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:04:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:09:15.227+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:04:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:09:15.239+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:04:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:09:13.539823+00:00, run_end_date=2024-07-06 02:09:13.949793+00:00, run_duration=0.40997, state=success, executor_state=success, try_number=1, max_tries=0, job_id=150, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:09:07.662343+00:00, queued_by_job_id=33, pid=9560[0m
[[34m2024-07-06T02:09:19.453+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:04:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:09:19.453+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:09:19.454+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:04:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:09:19.455+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:09:19.456+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:04:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:09:19.456+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:04:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:09:19.460+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:04:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:09:20.985+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:04:05.232560+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:09:25.171+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:04:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:09:28.086+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:04:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:09:28.098+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:04:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:09:25.259202+00:00, run_end_date=2024-07-06 02:09:26.905522+00:00, run_duration=1.64632, state=success, executor_state=success, try_number=1, max_tries=0, job_id=151, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:09:19.454727+00:00, queued_by_job_id=33, pid=9567[0m
[[34m2024-07-06T02:09:28.165+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:04:05.232560+00:00: scheduled__2024-07-06T02:04:05.232560+00:00, state:running, queued_at: 2024-07-06 02:09:07.603172+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:09:28.165+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:04:05.232560+00:00, run_id=scheduled__2024-07-06T02:04:05.232560+00:00, run_start_date=2024-07-06 02:09:07.624081+00:00, run_end_date=2024-07-06 02:09:28.165814+00:00, run_duration=20.541733, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:04:05.232560+00:00, data_interval_end=2024-07-06 02:09:05.232560+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:09:28.169+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:09:05.232560+00:00, run_after=2024-07-06T02:14:05.232560+00:00[0m
[[34m2024-07-06T02:12:01.570+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:14:07.042+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:14:05.232560+00:00, run_after=2024-07-06T02:19:05.232560+00:00[0m
[[34m2024-07-06T02:14:07.093+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:09:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:14:07.093+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:14:07.093+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:09:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:14:07.095+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:14:07.096+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:09:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:14:07.096+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:09:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:14:07.100+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:09:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:14:08.777+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:09:05.232560+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:14:13.116+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:09:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:14:15.034+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:09:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:14:15.047+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:09:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:14:13.213873+00:00, run_end_date=2024-07-06 02:14:13.673426+00:00, run_duration=0.459553, state=success, executor_state=success, try_number=1, max_tries=0, job_id=152, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:14:07.094464+00:00, queued_by_job_id=33, pid=9619[0m
[[34m2024-07-06T02:14:19.358+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:09:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:14:19.358+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:14:19.359+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:09:05.232560+00:00 [scheduled]>[0m
[[34m2024-07-06T02:14:19.360+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:14:19.361+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:09:05.232560+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:14:19.361+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:09:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:14:19.366+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:09:05.232560+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:14:20.861+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:09:05.232560+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:14:25.041+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:09:05.232560+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:14:28.029+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:09:05.232560+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:14:28.042+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:09:05.232560+00:00, map_index=-1, run_start_date=2024-07-06 02:14:25.129035+00:00, run_end_date=2024-07-06 02:14:26.782257+00:00, run_duration=1.653222, state=success, executor_state=success, try_number=1, max_tries=0, job_id=153, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:14:19.359707+00:00, queued_by_job_id=33, pid=9624[0m
[[34m2024-07-06T02:14:28.114+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:09:05.232560+00:00: scheduled__2024-07-06T02:09:05.232560+00:00, state:running, queued_at: 2024-07-06 02:14:07.037486+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:14:28.114+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:09:05.232560+00:00, run_id=scheduled__2024-07-06T02:09:05.232560+00:00, run_start_date=2024-07-06 02:14:07.057605+00:00, run_end_date=2024-07-06 02:14:28.114613+00:00, run_duration=21.057008, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:09:05.232560+00:00, data_interval_end=2024-07-06 02:14:05.232560+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:14:28.117+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:14:05.232560+00:00, run_after=2024-07-06T02:19:05.232560+00:00[0m
[[34m2024-07-06T02:17:01.637+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:19:09.044+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:19:08.952537+00:00, run_after=2024-07-06T02:24:08.952537+00:00[0m
[[34m2024-07-06T02:19:09.090+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:14:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:19:09.091+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:19:09.091+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:14:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:19:09.093+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:19:09.094+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:14:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:19:09.094+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:14:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:19:09.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:14:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:19:10.616+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:14:08.952537+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:19:14.773+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:14:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:19:16.640+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:14:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:19:16.652+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:14:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:19:14.892232+00:00, run_end_date=2024-07-06 02:19:15.342087+00:00, run_duration=0.449855, state=success, executor_state=success, try_number=1, max_tries=0, job_id=154, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:19:09.092164+00:00, queued_by_job_id=33, pid=9692[0m
[[34m2024-07-06T02:19:16.737+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:14:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:19:16.737+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:19:16.737+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:14:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:19:16.739+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:19:16.740+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:14:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:19:16.740+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:14:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:19:16.744+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:14:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:19:18.291+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:14:08.952537+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:19:22.471+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:14:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:19:25.513+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:14:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:19:25.519+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:14:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:19:22.561008+00:00, run_end_date=2024-07-06 02:19:24.256092+00:00, run_duration=1.695084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=155, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:19:16.738363+00:00, queued_by_job_id=33, pid=9695[0m
[[34m2024-07-06T02:19:29.802+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:14:08.952537+00:00: scheduled__2024-07-06T02:14:08.952537+00:00, state:running, queued_at: 2024-07-06 02:19:09.038724+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:19:29.804+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:14:08.952537+00:00, run_id=scheduled__2024-07-06T02:14:08.952537+00:00, run_start_date=2024-07-06 02:19:09.058743+00:00, run_end_date=2024-07-06 02:19:29.803860+00:00, run_duration=20.745117, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:14:08.952537+00:00, data_interval_end=2024-07-06 02:19:08.952537+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:19:29.807+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:19:08.952537+00:00, run_after=2024-07-06T02:24:08.952537+00:00[0m
[[34m2024-07-06T02:22:03.027+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:24:09.886+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:24:08.952537+00:00, run_after=2024-07-06T02:29:08.952537+00:00[0m
[[34m2024-07-06T02:24:09.930+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:19:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:24:09.930+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:24:09.930+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:19:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:24:09.932+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:24:09.933+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:19:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:24:09.933+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:19:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:24:09.936+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:19:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:24:11.567+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:19:08.952537+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:24:15.844+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:19:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:24:17.653+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:19:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:24:17.666+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:19:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:24:15.936237+00:00, run_end_date=2024-07-06 02:24:16.353079+00:00, run_duration=0.416842, state=success, executor_state=success, try_number=1, max_tries=0, job_id=156, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:24:09.931352+00:00, queued_by_job_id=33, pid=9766[0m
[[34m2024-07-06T02:24:21.972+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:19:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:24:21.973+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:24:21.973+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:19:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:24:21.975+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:24:21.976+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:19:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:24:21.976+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:19:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:24:21.980+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:19:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:24:23.462+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:19:08.952537+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:24:27.774+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:19:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:24:30.718+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:19:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:24:30.733+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:19:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:24:27.885446+00:00, run_end_date=2024-07-06 02:24:29.499889+00:00, run_duration=1.614443, state=success, executor_state=success, try_number=1, max_tries=0, job_id=157, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:24:21.974424+00:00, queued_by_job_id=33, pid=9774[0m
[[34m2024-07-06T02:24:30.802+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:19:08.952537+00:00: scheduled__2024-07-06T02:19:08.952537+00:00, state:running, queued_at: 2024-07-06 02:24:09.881221+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:24:30.803+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:19:08.952537+00:00, run_id=scheduled__2024-07-06T02:19:08.952537+00:00, run_start_date=2024-07-06 02:24:09.899746+00:00, run_end_date=2024-07-06 02:24:30.803110+00:00, run_duration=20.903364, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:19:08.952537+00:00, data_interval_end=2024-07-06 02:24:08.952537+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:24:30.806+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:24:08.952537+00:00, run_after=2024-07-06T02:29:08.952537+00:00[0m
[[34m2024-07-06T02:27:03.074+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:29:09.051+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:29:08.952537+00:00, run_after=2024-07-06T02:34:08.952537+00:00[0m
[[34m2024-07-06T02:29:09.118+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:24:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:29:09.118+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:29:09.118+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:24:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:29:09.121+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:29:09.121+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:24:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:29:09.122+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:24:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:29:09.126+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:24:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:29:10.730+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:24:08.952537+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:29:15.091+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:24:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:29:16.936+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:24:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:29:16.951+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:24:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:29:15.201633+00:00, run_end_date=2024-07-06 02:29:15.625660+00:00, run_duration=0.424027, state=success, executor_state=success, try_number=1, max_tries=0, job_id=158, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:29:09.119655+00:00, queued_by_job_id=33, pid=9834[0m
[[34m2024-07-06T02:29:21.071+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:24:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:29:21.071+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:29:21.072+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:24:08.952537+00:00 [scheduled]>[0m
[[34m2024-07-06T02:29:21.074+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:29:21.075+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:24:08.952537+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:29:21.075+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:24:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:29:21.079+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:24:08.952537+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:29:22.633+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:24:08.952537+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:29:26.925+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:24:08.952537+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:29:29.938+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:24:08.952537+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:29:29.952+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:24:08.952537+00:00, map_index=-1, run_start_date=2024-07-06 02:29:27.015921+00:00, run_end_date=2024-07-06 02:29:28.602679+00:00, run_duration=1.586758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=159, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:29:21.072903+00:00, queued_by_job_id=33, pid=9840[0m
[[34m2024-07-06T02:29:34.145+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:24:08.952537+00:00: scheduled__2024-07-06T02:24:08.952537+00:00, state:running, queued_at: 2024-07-06 02:29:09.046054+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:29:34.146+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:24:08.952537+00:00, run_id=scheduled__2024-07-06T02:24:08.952537+00:00, run_start_date=2024-07-06 02:29:09.066161+00:00, run_end_date=2024-07-06 02:29:34.146220+00:00, run_duration=25.080059, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:24:08.952537+00:00, data_interval_end=2024-07-06 02:29:08.952537+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:29:34.149+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:29:08.952537+00:00, run_after=2024-07-06T02:34:08.952537+00:00[0m
[[34m2024-07-06T02:32:03.129+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:34:11.573+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:34:10.304580+00:00, run_after=2024-07-06T02:39:10.304580+00:00[0m
[[34m2024-07-06T02:34:11.623+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:29:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:34:11.624+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:34:11.624+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:29:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:34:11.626+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:34:11.627+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:29:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:34:11.627+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:29:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:34:11.631+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:29:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:34:13.251+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:29:10.304580+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:34:17.693+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:29:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:34:19.503+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:29:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:34:19.516+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:29:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:34:17.789308+00:00, run_end_date=2024-07-06 02:34:18.199806+00:00, run_duration=0.410498, state=success, executor_state=success, try_number=1, max_tries=0, job_id=160, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:34:11.624881+00:00, queued_by_job_id=33, pid=9904[0m
[[34m2024-07-06T02:34:19.593+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:29:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:34:19.593+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:34:19.594+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:29:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:34:19.596+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:34:19.597+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:29:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:34:19.597+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:29:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:34:19.600+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:29:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:34:21.171+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:29:10.304580+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:34:25.323+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:29:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:34:28.376+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:29:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:34:28.389+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:29:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:34:25.415728+00:00, run_end_date=2024-07-06 02:34:27.079653+00:00, run_duration=1.663925, state=success, executor_state=success, try_number=1, max_tries=0, job_id=161, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:34:19.594772+00:00, queued_by_job_id=33, pid=9907[0m
[[34m2024-07-06T02:34:28.456+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:29:10.304580+00:00: scheduled__2024-07-06T02:29:10.304580+00:00, state:running, queued_at: 2024-07-06 02:34:11.567879+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:34:28.457+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:29:10.304580+00:00, run_id=scheduled__2024-07-06T02:29:10.304580+00:00, run_start_date=2024-07-06 02:34:11.588074+00:00, run_end_date=2024-07-06 02:34:28.457495+00:00, run_duration=16.869421, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:29:10.304580+00:00, data_interval_end=2024-07-06 02:34:10.304580+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:34:28.460+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:34:10.304580+00:00, run_after=2024-07-06T02:39:10.304580+00:00[0m
[[34m2024-07-06T02:37:03.563+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:39:12.210+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:39:10.304580+00:00, run_after=2024-07-06T02:44:10.304580+00:00[0m
[[34m2024-07-06T02:39:12.256+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:34:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:39:12.256+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:39:12.256+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:34:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:39:12.258+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:39:12.259+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:34:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:39:12.259+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:34:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:39:12.262+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:34:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:39:13.858+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:34:10.304580+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:39:18.227+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:34:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:39:20.014+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:34:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:39:20.028+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:34:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:39:18.319937+00:00, run_end_date=2024-07-06 02:39:18.735970+00:00, run_duration=0.416033, state=success, executor_state=success, try_number=1, max_tries=0, job_id=162, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:39:12.257430+00:00, queued_by_job_id=33, pid=9974[0m
[[34m2024-07-06T02:39:24.466+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:34:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:39:24.467+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:39:24.467+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:34:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:39:24.469+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:39:24.470+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:34:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:39:24.470+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:34:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:39:24.473+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:34:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:39:25.994+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:34:10.304580+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:39:30.204+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:34:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:39:33.257+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:34:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:39:33.270+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:34:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:39:30.294861+00:00, run_end_date=2024-07-06 02:39:32.048860+00:00, run_duration=1.753999, state=success, executor_state=success, try_number=1, max_tries=0, job_id=163, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:39:24.467952+00:00, queued_by_job_id=33, pid=9981[0m
[[34m2024-07-06T02:39:33.336+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:34:10.304580+00:00: scheduled__2024-07-06T02:34:10.304580+00:00, state:running, queued_at: 2024-07-06 02:39:12.203960+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:39:33.337+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:34:10.304580+00:00, run_id=scheduled__2024-07-06T02:34:10.304580+00:00, run_start_date=2024-07-06 02:39:12.223196+00:00, run_end_date=2024-07-06 02:39:33.337412+00:00, run_duration=21.114216, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:34:10.304580+00:00, data_interval_end=2024-07-06 02:39:10.304580+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:39:33.340+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:39:10.304580+00:00, run_after=2024-07-06T02:44:10.304580+00:00[0m
[[34m2024-07-06T02:42:05.023+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:44:11.934+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:44:10.304580+00:00, run_after=2024-07-06T02:49:10.304580+00:00[0m
[[34m2024-07-06T02:44:11.984+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:39:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:44:11.984+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:44:11.984+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:39:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:44:11.986+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:44:11.987+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:39:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:44:11.987+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:39:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:44:11.991+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:39:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:44:13.664+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:39:10.304580+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:44:18.223+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:39:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:44:20.257+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:39:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:44:20.271+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:39:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:44:18.313976+00:00, run_end_date=2024-07-06 02:44:18.856938+00:00, run_duration=0.542962, state=success, executor_state=success, try_number=1, max_tries=0, job_id=164, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:44:11.985552+00:00, queued_by_job_id=33, pid=10036[0m
[[34m2024-07-06T02:44:24.477+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:39:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:44:24.477+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:44:24.478+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:39:10.304580+00:00 [scheduled]>[0m
[[34m2024-07-06T02:44:24.479+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:44:24.480+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:39:10.304580+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:44:24.480+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:39:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:44:24.483+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:39:10.304580+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:44:26.064+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:39:10.304580+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:44:30.464+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:39:10.304580+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:44:33.605+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:39:10.304580+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:44:33.619+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:39:10.304580+00:00, map_index=-1, run_start_date=2024-07-06 02:44:30.561357+00:00, run_end_date=2024-07-06 02:44:32.281330+00:00, run_duration=1.719973, state=success, executor_state=success, try_number=1, max_tries=0, job_id=165, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:44:24.478654+00:00, queued_by_job_id=33, pid=10042[0m
[[34m2024-07-06T02:44:38.120+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:39:10.304580+00:00: scheduled__2024-07-06T02:39:10.304580+00:00, state:running, queued_at: 2024-07-06 02:44:11.929379+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:44:38.121+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:39:10.304580+00:00, run_id=scheduled__2024-07-06T02:39:10.304580+00:00, run_start_date=2024-07-06 02:44:11.947543+00:00, run_end_date=2024-07-06 02:44:38.121310+00:00, run_duration=26.173767, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:39:10.304580+00:00, data_interval_end=2024-07-06 02:44:10.304580+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:44:38.125+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:44:10.304580+00:00, run_after=2024-07-06T02:49:10.304580+00:00[0m
[[34m2024-07-06T02:47:05.075+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:49:13.069+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:49:12.926291+00:00, run_after=2024-07-06T02:54:12.926291+00:00[0m
[[34m2024-07-06T02:49:13.117+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:44:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:49:13.117+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:49:13.117+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:44:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:49:13.119+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:49:13.120+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:44:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:49:13.120+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:44:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:49:13.124+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:44:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:49:14.713+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:44:12.926291+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:49:18.963+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:44:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:49:20.804+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:44:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:49:20.819+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:44:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:49:19.053975+00:00, run_end_date=2024-07-06 02:49:19.480690+00:00, run_duration=0.426715, state=success, executor_state=success, try_number=1, max_tries=0, job_id=166, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:49:13.118192+00:00, queued_by_job_id=33, pid=10110[0m
[[34m2024-07-06T02:49:20.909+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:44:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:49:20.909+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:49:20.909+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:44:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:49:20.911+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:49:20.912+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:44:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:49:20.912+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:44:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:49:20.916+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:44:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:49:22.456+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:44:12.926291+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:49:26.620+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:44:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:49:29.947+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:44:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:49:29.959+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:44:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:49:26.705301+00:00, run_end_date=2024-07-06 02:49:28.591249+00:00, run_duration=1.885948, state=success, executor_state=success, try_number=1, max_tries=0, job_id=167, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:49:20.910399+00:00, queued_by_job_id=33, pid=10113[0m
[[34m2024-07-06T02:49:30.023+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:44:12.926291+00:00: scheduled__2024-07-06T02:44:12.926291+00:00, state:running, queued_at: 2024-07-06 02:49:13.063019+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:49:30.024+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:44:12.926291+00:00, run_id=scheduled__2024-07-06T02:44:12.926291+00:00, run_start_date=2024-07-06 02:49:13.082120+00:00, run_end_date=2024-07-06 02:49:30.024309+00:00, run_duration=16.942189, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:44:12.926291+00:00, data_interval_end=2024-07-06 02:49:12.926291+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:49:30.027+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:49:12.926291+00:00, run_after=2024-07-06T02:54:12.926291+00:00[0m
[[34m2024-07-06T02:52:06.327+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:54:13.794+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:54:12.926291+00:00, run_after=2024-07-06T02:59:12.926291+00:00[0m
[[34m2024-07-06T02:54:13.840+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:49:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:54:13.840+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:54:13.840+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:49:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:54:13.842+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:54:13.843+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:49:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:54:13.843+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:49:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:54:13.847+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:49:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:54:15.493+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:49:12.926291+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:54:19.910+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:49:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:54:21.701+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:49:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:54:21.713+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:49:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:54:20.006486+00:00, run_end_date=2024-07-06 02:54:20.417310+00:00, run_duration=0.410824, state=success, executor_state=success, try_number=1, max_tries=0, job_id=168, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:54:13.841383+00:00, queued_by_job_id=33, pid=10180[0m
[[34m2024-07-06T02:54:25.832+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:49:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:54:25.832+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:54:25.832+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:49:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:54:25.834+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:54:25.835+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:49:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:54:25.835+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:49:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:54:25.840+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:49:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:54:27.431+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:49:12.926291+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:54:31.638+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:49:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:54:34.710+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:49:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:54:34.725+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:49:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:54:31.730436+00:00, run_end_date=2024-07-06 02:54:33.342095+00:00, run_duration=1.611659, state=success, executor_state=success, try_number=1, max_tries=0, job_id=169, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:54:25.833234+00:00, queued_by_job_id=33, pid=10186[0m
[[34m2024-07-06T02:54:34.800+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:49:12.926291+00:00: scheduled__2024-07-06T02:49:12.926291+00:00, state:running, queued_at: 2024-07-06 02:54:13.788037+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:54:34.801+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:49:12.926291+00:00, run_id=scheduled__2024-07-06T02:49:12.926291+00:00, run_start_date=2024-07-06 02:54:13.807796+00:00, run_end_date=2024-07-06 02:54:34.800983+00:00, run_duration=20.993187, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:49:12.926291+00:00, data_interval_end=2024-07-06 02:54:12.926291+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:54:34.804+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:54:12.926291+00:00, run_after=2024-07-06T02:59:12.926291+00:00[0m
[[34m2024-07-06T02:57:08.105+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T02:59:13.503+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:59:12.926291+00:00, run_after=2024-07-06T03:04:12.926291+00:00[0m
[[34m2024-07-06T02:59:13.548+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:54:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:59:13.548+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:59:13.549+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:54:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:59:13.551+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:59:13.554+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:54:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T02:59:13.555+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:54:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:59:13.560+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:54:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:59:15.123+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:54:12.926291+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T02:59:19.568+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:54:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:59:21.362+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:54:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:59:21.375+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:54:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:59:19.656673+00:00, run_end_date=2024-07-06 02:59:20.070607+00:00, run_duration=0.413934, state=success, executor_state=success, try_number=1, max_tries=0, job_id=170, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 02:59:13.549833+00:00, queued_by_job_id=33, pid=10243[0m
[[34m2024-07-06T02:59:21.452+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:54:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:59:21.452+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T02:59:21.452+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:54:12.926291+00:00 [scheduled]>[0m
[[34m2024-07-06T02:59:21.454+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T02:59:21.455+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:54:12.926291+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T02:59:21.455+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:54:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:59:21.459+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:54:12.926291+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T02:59:22.993+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:54:12.926291+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T02:59:27.252+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:54:12.926291+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T02:59:30.332+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:54:12.926291+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T02:59:30.347+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:54:12.926291+00:00, map_index=-1, run_start_date=2024-07-06 02:59:27.347077+00:00, run_end_date=2024-07-06 02:59:29.030988+00:00, run_duration=1.683911, state=success, executor_state=success, try_number=1, max_tries=0, job_id=171, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 02:59:21.453521+00:00, queued_by_job_id=33, pid=10246[0m
[[34m2024-07-06T02:59:34.638+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:54:12.926291+00:00: scheduled__2024-07-06T02:54:12.926291+00:00, state:running, queued_at: 2024-07-06 02:59:13.498001+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T02:59:34.639+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:54:12.926291+00:00, run_id=scheduled__2024-07-06T02:54:12.926291+00:00, run_start_date=2024-07-06 02:59:13.517288+00:00, run_end_date=2024-07-06 02:59:34.639385+00:00, run_duration=21.122097, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:54:12.926291+00:00, data_interval_end=2024-07-06 02:59:12.926291+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T02:59:34.643+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T02:59:12.926291+00:00, run_after=2024-07-06T03:04:12.926291+00:00[0m
[[34m2024-07-06T03:02:08.151+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:04:17.615+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:04:16.277937+00:00, run_after=2024-07-06T03:09:16.277937+00:00[0m
[[34m2024-07-06T03:04:17.660+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:59:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:04:17.660+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:04:17.661+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:59:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:04:17.662+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:04:17.663+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:59:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:04:17.663+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:59:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:04:17.667+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T02:59:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:04:19.251+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:59:16.277937+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:04:23.869+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T02:59:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:04:25.790+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T02:59:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:04:25.806+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T02:59:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:04:23.958944+00:00, run_end_date=2024-07-06 03:04:24.399036+00:00, run_duration=0.440092, state=success, executor_state=success, try_number=1, max_tries=0, job_id=172, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:04:17.661582+00:00, queued_by_job_id=33, pid=10306[0m
[[34m2024-07-06T03:04:25.895+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:59:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:04:25.895+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:04:25.896+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:59:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:04:25.897+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:04:25.898+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:59:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:04:25.899+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:59:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:04:25.902+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T02:59:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:04:27.418+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T02:59:16.277937+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:04:31.591+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T02:59:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:04:34.686+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T02:59:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:04:34.699+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T02:59:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:04:31.678969+00:00, run_end_date=2024-07-06 03:04:33.320820+00:00, run_duration=1.641851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=173, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:04:25.896641+00:00, queued_by_job_id=33, pid=10310[0m
[[34m2024-07-06T03:04:34.765+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 02:59:16.277937+00:00: scheduled__2024-07-06T02:59:16.277937+00:00, state:running, queued_at: 2024-07-06 03:04:17.610802+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:04:34.766+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 02:59:16.277937+00:00, run_id=scheduled__2024-07-06T02:59:16.277937+00:00, run_start_date=2024-07-06 03:04:17.628439+00:00, run_end_date=2024-07-06 03:04:34.765990+00:00, run_duration=17.137551, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 02:59:16.277937+00:00, data_interval_end=2024-07-06 03:04:16.277937+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:04:34.769+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:04:16.277937+00:00, run_after=2024-07-06T03:09:16.277937+00:00[0m
[[34m2024-07-06T03:07:08.617+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:09:17.690+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:09:16.277937+00:00, run_after=2024-07-06T03:14:16.277937+00:00[0m
[[34m2024-07-06T03:09:17.733+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:04:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:09:17.734+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:09:17.734+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:04:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:09:17.736+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:09:17.736+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:04:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:09:17.737+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:04:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:09:17.740+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:04:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:09:19.332+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:04:16.277937+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:09:23.597+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:04:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:09:25.336+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:04:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:09:25.348+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:04:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:09:23.688351+00:00, run_end_date=2024-07-06 03:09:24.100275+00:00, run_duration=0.411924, state=success, executor_state=success, try_number=1, max_tries=0, job_id=174, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:09:17.735040+00:00, queued_by_job_id=33, pid=10379[0m
[[34m2024-07-06T03:09:29.680+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:04:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:09:29.681+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:09:29.681+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:04:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:09:29.683+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:09:29.684+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:04:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:09:29.684+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:04:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:09:29.688+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:04:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:09:31.255+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:04:16.277937+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:09:35.578+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:04:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:09:38.524+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:04:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:09:38.536+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:04:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:09:35.674942+00:00, run_end_date=2024-07-06 03:09:37.303090+00:00, run_duration=1.628148, state=success, executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:09:29.682061+00:00, queued_by_job_id=33, pid=10385[0m
[[34m2024-07-06T03:09:42.828+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:04:16.277937+00:00: scheduled__2024-07-06T03:04:16.277937+00:00, state:running, queued_at: 2024-07-06 03:09:17.681350+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:09:42.829+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:04:16.277937+00:00, run_id=scheduled__2024-07-06T03:04:16.277937+00:00, run_start_date=2024-07-06 03:09:17.703930+00:00, run_end_date=2024-07-06 03:09:42.829643+00:00, run_duration=25.125713, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:04:16.277937+00:00, data_interval_end=2024-07-06 03:09:16.277937+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:09:42.834+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:09:16.277937+00:00, run_after=2024-07-06T03:14:16.277937+00:00[0m
[[34m2024-07-06T03:12:08.682+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:14:18.028+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:14:16.277937+00:00, run_after=2024-07-06T03:19:16.277937+00:00[0m
[[34m2024-07-06T03:14:18.078+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:09:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:14:18.078+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:14:18.078+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:09:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:14:18.080+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:14:18.081+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:09:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:14:18.081+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:09:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:14:18.084+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:09:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:14:19.587+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:09:16.277937+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:14:23.718+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:09:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:14:25.433+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:09:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:14:25.439+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:09:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:14:23.803480+00:00, run_end_date=2024-07-06 03:14:24.220747+00:00, run_duration=0.417267, state=success, executor_state=success, try_number=1, max_tries=0, job_id=176, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:14:18.079159+00:00, queued_by_job_id=33, pid=10437[0m
[[34m2024-07-06T03:14:25.522+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:09:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:14:25.523+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:14:25.523+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:09:16.277937+00:00 [scheduled]>[0m
[[34m2024-07-06T03:14:25.525+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:14:25.525+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:09:16.277937+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:14:25.526+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:09:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:14:25.529+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:09:16.277937+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:14:27.074+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:09:16.277937+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:14:31.269+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:09:16.277937+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:14:34.057+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:09:16.277937+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:14:34.069+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:09:16.277937+00:00, map_index=-1, run_start_date=2024-07-06 03:14:31.361035+00:00, run_end_date=2024-07-06 03:14:32.791728+00:00, run_duration=1.430693, state=success, executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:14:25.524020+00:00, queued_by_job_id=33, pid=10441[0m
[[34m2024-07-06T03:14:34.126+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:09:16.277937+00:00: scheduled__2024-07-06T03:09:16.277937+00:00, state:running, queued_at: 2024-07-06 03:14:18.020157+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:14:34.127+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:09:16.277937+00:00, run_id=scheduled__2024-07-06T03:09:16.277937+00:00, run_start_date=2024-07-06 03:14:18.042637+00:00, run_end_date=2024-07-06 03:14:34.127293+00:00, run_duration=16.084656, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:09:16.277937+00:00, data_interval_end=2024-07-06 03:14:16.277937+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:14:34.130+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:14:16.277937+00:00, run_after=2024-07-06T03:19:16.277937+00:00[0m
[[34m2024-07-06T03:17:10.360+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:19:19.351+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:19:18.178185+00:00, run_after=2024-07-06T03:24:18.178185+00:00[0m
[[34m2024-07-06T03:19:19.397+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:14:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:19:19.397+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:19:19.398+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:14:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:19:19.399+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:19:19.400+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:14:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:19:19.400+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:14:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:19:19.404+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:14:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:19:20.937+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:14:18.178185+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:19:25.094+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:14:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:19:27.011+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:14:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:19:27.025+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:14:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:19:25.180939+00:00, run_end_date=2024-07-06 03:19:25.611099+00:00, run_duration=0.43016, state=success, executor_state=success, try_number=1, max_tries=0, job_id=178, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:19:19.398588+00:00, queued_by_job_id=33, pid=10514[0m
[[34m2024-07-06T03:19:31.226+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:14:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:19:31.227+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:19:31.227+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:14:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:19:31.229+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:19:31.230+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:14:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:19:31.230+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:14:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:19:31.234+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:14:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:19:32.732+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:14:18.178185+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:19:37.067+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:14:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:19:39.986+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:14:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:19:39.999+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:14:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:19:37.153101+00:00, run_end_date=2024-07-06 03:19:38.789922+00:00, run_duration=1.636821, state=success, executor_state=success, try_number=1, max_tries=0, job_id=179, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:19:31.228227+00:00, queued_by_job_id=33, pid=10520[0m
[[34m2024-07-06T03:19:40.085+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:14:18.178185+00:00: scheduled__2024-07-06T03:14:18.178185+00:00, state:running, queued_at: 2024-07-06 03:19:19.344402+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:19:40.086+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:14:18.178185+00:00, run_id=scheduled__2024-07-06T03:14:18.178185+00:00, run_start_date=2024-07-06 03:19:19.364745+00:00, run_end_date=2024-07-06 03:19:40.086546+00:00, run_duration=20.721801, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:14:18.178185+00:00, data_interval_end=2024-07-06 03:19:18.178185+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:19:40.089+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:19:18.178185+00:00, run_after=2024-07-06T03:24:18.178185+00:00[0m
[[34m2024-07-06T03:22:11.709+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:24:19.232+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:24:18.178185+00:00, run_after=2024-07-06T03:29:18.178185+00:00[0m
[[34m2024-07-06T03:24:19.284+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:19:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:24:19.284+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:24:19.285+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:19:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:24:19.287+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:24:19.287+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:19:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:24:19.288+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:19:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:24:19.291+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:19:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:24:20.843+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:19:18.178185+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:24:25.195+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:19:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:24:26.986+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:19:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:24:26.999+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:19:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:24:25.316535+00:00, run_end_date=2024-07-06 03:24:25.745482+00:00, run_duration=0.428947, state=success, executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:24:19.285699+00:00, queued_by_job_id=33, pid=10569[0m
[[34m2024-07-06T03:24:31.108+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:19:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:24:31.108+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:24:31.109+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:19:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:24:31.110+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:24:31.111+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:19:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:24:31.111+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:19:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:24:31.115+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:19:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:24:32.632+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:19:18.178185+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:24:36.817+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:19:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:24:39.702+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:19:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:24:39.717+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:19:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:24:36.906901+00:00, run_end_date=2024-07-06 03:24:38.328587+00:00, run_duration=1.421686, state=success, executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:24:31.109677+00:00, queued_by_job_id=33, pid=10575[0m
[[34m2024-07-06T03:24:44.007+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:19:18.178185+00:00: scheduled__2024-07-06T03:19:18.178185+00:00, state:running, queued_at: 2024-07-06 03:24:19.227401+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:24:44.008+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:19:18.178185+00:00, run_id=scheduled__2024-07-06T03:19:18.178185+00:00, run_start_date=2024-07-06 03:24:19.248310+00:00, run_end_date=2024-07-06 03:24:44.007939+00:00, run_duration=24.759629, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:19:18.178185+00:00, data_interval_end=2024-07-06 03:24:18.178185+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:24:44.011+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:24:18.178185+00:00, run_after=2024-07-06T03:29:18.178185+00:00[0m
[[34m2024-07-06T03:27:11.716+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:29:19.417+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:29:18.178185+00:00, run_after=2024-07-06T03:34:18.178185+00:00[0m
[[34m2024-07-06T03:29:19.465+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:24:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:29:19.466+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:29:19.466+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:24:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:29:19.468+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:29:19.469+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:24:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:29:19.469+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:24:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:29:19.472+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:24:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:29:21.031+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:24:18.178185+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:29:25.236+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:24:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:29:27.156+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:24:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:29:27.162+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:24:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:29:25.325974+00:00, run_end_date=2024-07-06 03:29:25.761630+00:00, run_duration=0.435656, state=success, executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:29:19.467121+00:00, queued_by_job_id=33, pid=10631[0m
[[34m2024-07-06T03:29:27.228+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:24:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:29:27.228+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:29:27.228+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:24:18.178185+00:00 [scheduled]>[0m
[[34m2024-07-06T03:29:27.230+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:29:27.231+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:24:18.178185+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:29:27.231+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:24:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:29:27.234+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:24:18.178185+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:29:28.775+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:24:18.178185+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:29:33.086+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:24:18.178185+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:29:36.021+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:24:18.178185+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:29:36.033+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:24:18.178185+00:00, map_index=-1, run_start_date=2024-07-06 03:29:33.176652+00:00, run_end_date=2024-07-06 03:29:34.793775+00:00, run_duration=1.617123, state=success, executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:29:27.229373+00:00, queued_by_job_id=33, pid=10635[0m
[[34m2024-07-06T03:29:36.094+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:24:18.178185+00:00: scheduled__2024-07-06T03:24:18.178185+00:00, state:running, queued_at: 2024-07-06 03:29:19.411278+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:29:36.094+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:24:18.178185+00:00, run_id=scheduled__2024-07-06T03:24:18.178185+00:00, run_start_date=2024-07-06 03:29:19.432540+00:00, run_end_date=2024-07-06 03:29:36.094687+00:00, run_duration=16.662147, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:24:18.178185+00:00, data_interval_end=2024-07-06 03:29:18.178185+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:29:36.097+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:29:18.178185+00:00, run_after=2024-07-06T03:34:18.178185+00:00[0m
[[34m2024-07-06T03:32:11.772+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:34:21.956+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:34:20.787101+00:00, run_after=2024-07-06T03:39:20.787101+00:00[0m
[[34m2024-07-06T03:34:22.005+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:29:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:34:22.005+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:34:22.006+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:29:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:34:22.007+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:34:22.008+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:29:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:34:22.008+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:29:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:34:22.012+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:29:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:34:23.622+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:29:20.787101+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:34:27.871+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:29:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:34:29.621+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:29:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:34:29.634+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:29:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:34:27.984320+00:00, run_end_date=2024-07-06 03:34:28.402456+00:00, run_duration=0.418136, state=success, executor_state=success, try_number=1, max_tries=0, job_id=184, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:34:22.006522+00:00, queued_by_job_id=33, pid=10708[0m
[[34m2024-07-06T03:34:33.736+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:29:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:34:33.736+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:34:33.736+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:29:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:34:33.739+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:34:33.740+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:29:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:34:33.740+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:29:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:34:33.744+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:29:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:34:35.372+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:29:20.787101+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:34:39.974+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:29:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:34:42.977+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:29:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:34:42.991+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:29:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:34:40.066962+00:00, run_end_date=2024-07-06 03:34:41.734187+00:00, run_duration=1.667225, state=success, executor_state=success, try_number=1, max_tries=0, job_id=185, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:34:33.737620+00:00, queued_by_job_id=33, pid=10714[0m
[[34m2024-07-06T03:34:43.063+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:29:20.787101+00:00: scheduled__2024-07-06T03:29:20.787101+00:00, state:running, queued_at: 2024-07-06 03:34:21.949929+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:34:43.064+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:29:20.787101+00:00, run_id=scheduled__2024-07-06T03:29:20.787101+00:00, run_start_date=2024-07-06 03:34:21.969944+00:00, run_end_date=2024-07-06 03:34:43.064254+00:00, run_duration=21.09431, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:29:20.787101+00:00, data_interval_end=2024-07-06 03:34:20.787101+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:34:43.067+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:34:20.787101+00:00, run_after=2024-07-06T03:39:20.787101+00:00[0m
[[34m2024-07-06T03:37:14.215+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:39:21.511+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:39:20.787101+00:00, run_after=2024-07-06T03:44:20.787101+00:00[0m
[[34m2024-07-06T03:39:21.557+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:34:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:39:21.558+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:39:21.558+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:34:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:39:21.560+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:39:21.560+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:34:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:39:21.561+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:34:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:39:21.565+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:34:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:39:23.196+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:34:20.787101+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:39:27.607+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:34:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:39:29.503+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:34:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:39:29.518+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:34:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:39:27.701774+00:00, run_end_date=2024-07-06 03:39:28.128887+00:00, run_duration=0.427113, state=success, executor_state=success, try_number=1, max_tries=0, job_id=186, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:39:21.559038+00:00, queued_by_job_id=33, pid=10780[0m
[[34m2024-07-06T03:39:33.825+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:34:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:39:33.826+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:39:33.826+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:34:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:39:33.828+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:39:33.829+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:34:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:39:33.829+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:34:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:39:33.833+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:34:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:39:35.331+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:34:20.787101+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:39:39.593+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:34:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:39:42.556+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:34:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:39:42.569+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:34:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:39:39.688589+00:00, run_end_date=2024-07-06 03:39:41.282759+00:00, run_duration=1.59417, state=success, executor_state=success, try_number=1, max_tries=0, job_id=187, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:39:33.827377+00:00, queued_by_job_id=33, pid=10786[0m
[[34m2024-07-06T03:39:46.756+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:34:20.787101+00:00: scheduled__2024-07-06T03:34:20.787101+00:00, state:running, queued_at: 2024-07-06 03:39:21.506246+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:39:46.757+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:34:20.787101+00:00, run_id=scheduled__2024-07-06T03:34:20.787101+00:00, run_start_date=2024-07-06 03:39:21.525763+00:00, run_end_date=2024-07-06 03:39:46.757042+00:00, run_duration=25.231279, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:34:20.787101+00:00, data_interval_end=2024-07-06 03:39:20.787101+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:39:46.760+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:39:20.787101+00:00, run_after=2024-07-06T03:44:20.787101+00:00[0m
[[34m2024-07-06T03:42:14.260+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:44:22.088+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:44:20.787101+00:00, run_after=2024-07-06T03:49:20.787101+00:00[0m
[[34m2024-07-06T03:44:22.137+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:39:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:44:22.137+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:44:22.137+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:39:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:44:22.139+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:44:22.140+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:39:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:44:22.140+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:39:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:44:22.144+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:39:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:44:23.735+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:39:20.787101+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:44:28.127+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:39:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:44:29.963+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:39:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:44:29.975+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:39:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:44:28.218553+00:00, run_end_date=2024-07-06 03:44:28.672432+00:00, run_duration=0.453879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=188, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:44:22.138433+00:00, queued_by_job_id=33, pid=10843[0m
[[34m2024-07-06T03:44:30.077+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:39:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:44:30.077+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:44:30.078+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:39:20.787101+00:00 [scheduled]>[0m
[[34m2024-07-06T03:44:30.079+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:44:30.080+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:39:20.787101+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:44:30.080+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:39:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:44:30.084+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:39:20.787101+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:44:31.688+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:39:20.787101+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:44:36.062+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:39:20.787101+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:44:39.081+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:39:20.787101+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:44:39.094+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:39:20.787101+00:00, map_index=-1, run_start_date=2024-07-06 03:44:36.152974+00:00, run_end_date=2024-07-06 03:44:37.807904+00:00, run_duration=1.65493, state=success, executor_state=success, try_number=1, max_tries=0, job_id=189, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:44:30.078571+00:00, queued_by_job_id=33, pid=10848[0m
[[34m2024-07-06T03:44:39.153+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:39:20.787101+00:00: scheduled__2024-07-06T03:39:20.787101+00:00, state:running, queued_at: 2024-07-06 03:44:22.082373+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:44:39.153+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:39:20.787101+00:00, run_id=scheduled__2024-07-06T03:39:20.787101+00:00, run_start_date=2024-07-06 03:44:22.102632+00:00, run_end_date=2024-07-06 03:44:39.153730+00:00, run_duration=17.051098, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:39:20.787101+00:00, data_interval_end=2024-07-06 03:44:20.787101+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:44:39.157+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:44:20.787101+00:00, run_after=2024-07-06T03:49:20.787101+00:00[0m
[[34m2024-07-06T03:47:15.624+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:49:24.418+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:49:23.253226+00:00, run_after=2024-07-06T03:54:23.253226+00:00[0m
[[34m2024-07-06T03:49:24.465+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:44:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:49:24.465+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:49:24.469+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:44:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:49:24.471+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:49:24.472+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:44:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:49:24.472+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:44:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:49:24.479+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:44:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:49:26.121+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:44:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:49:30.570+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:44:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:49:32.355+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:44:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:49:32.368+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:44:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:49:30.675190+00:00, run_end_date=2024-07-06 03:49:31.089513+00:00, run_duration=0.414323, state=success, executor_state=success, try_number=1, max_tries=0, job_id=190, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:49:24.470117+00:00, queued_by_job_id=33, pid=10913[0m
[[34m2024-07-06T03:49:36.672+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:44:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:49:36.673+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:49:36.673+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:44:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:49:36.675+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:49:36.676+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:44:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:49:36.676+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:44:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:49:36.680+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:44:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:49:38.266+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:44:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:49:42.513+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:44:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:49:45.615+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:44:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:49:45.626+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:44:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:49:42.603453+00:00, run_end_date=2024-07-06 03:49:44.287980+00:00, run_duration=1.684527, state=success, executor_state=success, try_number=1, max_tries=0, job_id=191, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:49:36.674408+00:00, queued_by_job_id=33, pid=10919[0m
[[34m2024-07-06T03:49:45.696+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:44:23.253226+00:00: scheduled__2024-07-06T03:44:23.253226+00:00, state:running, queued_at: 2024-07-06 03:49:24.413459+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:49:45.697+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:44:23.253226+00:00, run_id=scheduled__2024-07-06T03:44:23.253226+00:00, run_start_date=2024-07-06 03:49:24.431448+00:00, run_end_date=2024-07-06 03:49:45.697242+00:00, run_duration=21.265794, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:44:23.253226+00:00, data_interval_end=2024-07-06 03:49:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:49:45.700+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:49:23.253226+00:00, run_after=2024-07-06T03:54:23.253226+00:00[0m
[[34m2024-07-06T03:52:17.689+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:54:24.989+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:54:23.253226+00:00, run_after=2024-07-06T03:59:23.253226+00:00[0m
[[34m2024-07-06T03:54:25.036+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:49:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:54:25.036+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:54:25.037+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:49:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:54:25.039+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:54:25.039+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:49:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:54:25.040+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:49:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:54:25.048+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:49:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:54:26.671+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:49:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:54:31.076+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:49:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:54:33.010+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:49:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:54:33.022+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:49:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:54:31.171578+00:00, run_end_date=2024-07-06 03:54:31.608023+00:00, run_duration=0.436445, state=success, executor_state=success, try_number=1, max_tries=0, job_id=192, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:54:25.037813+00:00, queued_by_job_id=33, pid=10978[0m
[[34m2024-07-06T03:54:37.368+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:49:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:54:37.369+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:54:37.369+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:49:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:54:37.372+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:54:37.372+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:49:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:54:37.373+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:49:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:54:37.376+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:49:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:54:38.972+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:49:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:54:43.282+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:49:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:54:46.504+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:49:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:54:46.517+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:49:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:54:43.373725+00:00, run_end_date=2024-07-06 03:54:45.060179+00:00, run_duration=1.686454, state=success, executor_state=success, try_number=1, max_tries=0, job_id=193, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:54:37.370445+00:00, queued_by_job_id=33, pid=10983[0m
[[34m2024-07-06T03:54:46.601+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:49:23.253226+00:00: scheduled__2024-07-06T03:49:23.253226+00:00, state:running, queued_at: 2024-07-06 03:54:24.984553+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:54:46.601+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:49:23.253226+00:00, run_id=scheduled__2024-07-06T03:49:23.253226+00:00, run_start_date=2024-07-06 03:54:25.002851+00:00, run_end_date=2024-07-06 03:54:46.601591+00:00, run_duration=21.59874, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:49:23.253226+00:00, data_interval_end=2024-07-06 03:54:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:54:46.604+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:54:23.253226+00:00, run_after=2024-07-06T03:59:23.253226+00:00[0m
[[34m2024-07-06T03:57:17.736+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T03:59:28.049+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:59:23.253226+00:00, run_after=2024-07-06T04:04:23.253226+00:00[0m
[[34m2024-07-06T03:59:28.103+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:54:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:59:28.104+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:59:28.104+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:54:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:59:28.106+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:59:28.107+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:54:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T03:59:28.107+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:54:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:59:28.111+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:54:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:59:29.701+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:54:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T03:59:34.050+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:54:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:59:35.909+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:54:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:59:35.921+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:54:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:59:34.176322+00:00, run_end_date=2024-07-06 03:59:34.592325+00:00, run_duration=0.416003, state=success, executor_state=success, try_number=1, max_tries=0, job_id=194, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 03:59:28.104939+00:00, queued_by_job_id=33, pid=11045[0m
[[34m2024-07-06T03:59:36.010+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:54:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:59:36.010+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T03:59:36.011+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:54:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T03:59:36.012+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T03:59:36.013+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:54:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T03:59:36.013+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:54:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:59:36.018+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:54:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T03:59:37.577+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:54:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T03:59:41.853+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:54:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T03:59:44.661+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:54:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T03:59:44.674+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:54:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 03:59:41.947447+00:00, run_end_date=2024-07-06 03:59:43.408783+00:00, run_duration=1.461336, state=success, executor_state=success, try_number=1, max_tries=0, job_id=195, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 03:59:36.011733+00:00, queued_by_job_id=33, pid=11048[0m
[[34m2024-07-06T03:59:44.732+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:54:23.253226+00:00: scheduled__2024-07-06T03:54:23.253226+00:00, state:running, queued_at: 2024-07-06 03:59:28.029342+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T03:59:44.733+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:54:23.253226+00:00, run_id=scheduled__2024-07-06T03:54:23.253226+00:00, run_start_date=2024-07-06 03:59:28.067324+00:00, run_end_date=2024-07-06 03:59:44.733194+00:00, run_duration=16.66587, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:54:23.253226+00:00, data_interval_end=2024-07-06 03:59:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T03:59:44.736+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T03:59:23.253226+00:00, run_after=2024-07-06T04:04:23.253226+00:00[0m
[[34m2024-07-06T04:02:21.726+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:04:24.302+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:04:23.253226+00:00, run_after=2024-07-06T04:09:23.253226+00:00[0m
[[34m2024-07-06T04:04:24.374+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:59:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:04:24.374+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:04:24.374+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:59:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:04:24.376+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:04:24.377+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:59:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:04:24.378+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:59:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:04:24.381+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T03:59:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:04:26.029+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:59:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:04:30.561+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T03:59:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:04:32.375+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T03:59:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:04:32.389+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T03:59:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:04:30.650160+00:00, run_end_date=2024-07-06 04:04:31.063561+00:00, run_duration=0.413401, state=success, executor_state=success, try_number=1, max_tries=0, job_id=196, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:04:24.375145+00:00, queued_by_job_id=33, pid=11108[0m
[[34m2024-07-06T04:04:36.693+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:59:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:04:36.694+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:04:36.694+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:59:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:04:36.696+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:04:36.696+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:59:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:04:36.697+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:59:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:04:36.700+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T03:59:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:04:38.286+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T03:59:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:04:42.778+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T03:59:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:04:46.338+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T03:59:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:04:46.354+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T03:59:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:04:42.874550+00:00, run_end_date=2024-07-06 04:04:44.718116+00:00, run_duration=1.843566, state=success, executor_state=success, try_number=1, max_tries=0, job_id=197, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:04:36.694863+00:00, queued_by_job_id=33, pid=11113[0m
[[34m2024-07-06T04:04:50.752+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 03:59:23.253226+00:00: scheduled__2024-07-06T03:59:23.253226+00:00, state:running, queued_at: 2024-07-06 04:04:24.295880+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:04:50.752+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 03:59:23.253226+00:00, run_id=scheduled__2024-07-06T03:59:23.253226+00:00, run_start_date=2024-07-06 04:04:24.320905+00:00, run_end_date=2024-07-06 04:04:50.752759+00:00, run_duration=26.431854, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 03:59:23.253226+00:00, data_interval_end=2024-07-06 04:04:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:04:50.756+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:04:23.253226+00:00, run_after=2024-07-06T04:09:23.253226+00:00[0m
[[34m2024-07-06T04:07:21.775+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:09:27.135+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:09:23.253226+00:00, run_after=2024-07-06T04:14:23.253226+00:00[0m
[[34m2024-07-06T04:09:27.190+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:04:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:09:27.190+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:09:27.191+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:04:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:09:27.193+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:09:27.194+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:04:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:09:27.194+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:04:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:09:27.199+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:04:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:09:28.858+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:04:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:09:33.331+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:04:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:09:35.384+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:04:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:09:35.399+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:04:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:09:33.432850+00:00, run_end_date=2024-07-06 04:09:33.889566+00:00, run_duration=0.456716, state=success, executor_state=success, try_number=1, max_tries=0, job_id=198, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:09:27.191842+00:00, queued_by_job_id=33, pid=11176[0m
[[34m2024-07-06T04:09:35.495+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:04:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:09:35.496+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:09:35.496+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:04:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:09:35.498+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:09:35.499+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:04:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:09:35.499+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:04:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:09:35.504+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:04:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:09:37.094+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:04:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:09:41.760+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:04:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:09:44.923+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:04:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:09:44.938+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:04:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:09:41.857370+00:00, run_end_date=2024-07-06 04:09:43.557322+00:00, run_duration=1.699952, state=success, executor_state=success, try_number=1, max_tries=0, job_id=199, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:09:35.497205+00:00, queued_by_job_id=33, pid=11179[0m
[[34m2024-07-06T04:09:45.006+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:04:23.253226+00:00: scheduled__2024-07-06T04:04:23.253226+00:00, state:running, queued_at: 2024-07-06 04:09:27.128264+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:09:45.006+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:04:23.253226+00:00, run_id=scheduled__2024-07-06T04:04:23.253226+00:00, run_start_date=2024-07-06 04:09:27.151219+00:00, run_end_date=2024-07-06 04:09:45.006604+00:00, run_duration=17.855385, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:04:23.253226+00:00, data_interval_end=2024-07-06 04:09:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:09:45.010+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:09:23.253226+00:00, run_after=2024-07-06T04:14:23.253226+00:00[0m
[[34m2024-07-06T04:12:21.830+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:14:24.472+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:14:23.253226+00:00, run_after=2024-07-06T04:19:23.253226+00:00[0m
[[34m2024-07-06T04:14:24.521+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:09:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:14:24.521+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:14:24.522+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:09:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:14:24.523+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:14:24.524+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:09:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:14:24.524+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:09:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:14:24.528+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:09:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:14:26.239+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:09:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:14:30.804+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:09:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:14:32.722+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:09:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:14:32.734+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:09:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:14:30.897639+00:00, run_end_date=2024-07-06 04:14:31.383064+00:00, run_duration=0.485425, state=success, executor_state=success, try_number=1, max_tries=0, job_id=200, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:14:24.522623+00:00, queued_by_job_id=33, pid=11229[0m
[[34m2024-07-06T04:14:37.041+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:09:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:14:37.041+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:14:37.042+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:09:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:14:37.043+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:14:37.044+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:09:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:14:37.044+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:09:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:14:37.048+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:09:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:14:38.637+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:09:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:14:43.120+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:09:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:14:46.211+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:09:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:14:46.225+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:09:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:14:43.213250+00:00, run_end_date=2024-07-06 04:14:44.870617+00:00, run_duration=1.657367, state=success, executor_state=success, try_number=1, max_tries=0, job_id=201, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:14:37.042614+00:00, queued_by_job_id=33, pid=11234[0m
[[34m2024-07-06T04:14:50.528+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:09:23.253226+00:00: scheduled__2024-07-06T04:09:23.253226+00:00, state:running, queued_at: 2024-07-06 04:14:24.465948+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:14:50.529+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:09:23.253226+00:00, run_id=scheduled__2024-07-06T04:09:23.253226+00:00, run_start_date=2024-07-06 04:14:24.487350+00:00, run_end_date=2024-07-06 04:14:50.529756+00:00, run_duration=26.042406, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:09:23.253226+00:00, data_interval_end=2024-07-06 04:14:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:14:50.533+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:14:23.253226+00:00, run_after=2024-07-06T04:19:23.253226+00:00[0m
[[34m2024-07-06T04:17:21.881+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:19:26.207+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:19:23.253226+00:00, run_after=2024-07-06T04:24:23.253226+00:00[0m
[[34m2024-07-06T04:19:26.261+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:14:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:19:26.261+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:19:26.262+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:14:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:19:26.263+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:19:26.264+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:14:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:19:26.264+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:14:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:19:26.268+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:14:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:19:27.820+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:14:23.253226+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:19:32.135+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:14:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:19:33.938+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:14:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:19:33.944+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:14:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:19:32.226422+00:00, run_end_date=2024-07-06 04:19:32.677260+00:00, run_duration=0.450838, state=success, executor_state=success, try_number=1, max_tries=0, job_id=202, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:19:26.262745+00:00, queued_by_job_id=33, pid=11303[0m
[[34m2024-07-06T04:19:34.034+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:14:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:19:34.035+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:19:34.035+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:14:23.253226+00:00 [scheduled]>[0m
[[34m2024-07-06T04:19:34.037+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:19:34.037+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:14:23.253226+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:19:34.037+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:14:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:19:34.041+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:14:23.253226+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:19:35.659+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:14:23.253226+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:19:40.106+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:14:23.253226+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:19:43.122+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:14:23.253226+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:19:43.135+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:14:23.253226+00:00, map_index=-1, run_start_date=2024-07-06 04:19:40.198764+00:00, run_end_date=2024-07-06 04:19:41.716627+00:00, run_duration=1.517863, state=success, executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:19:34.035814+00:00, queued_by_job_id=33, pid=11308[0m
[[34m2024-07-06T04:19:43.194+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:14:23.253226+00:00: scheduled__2024-07-06T04:14:23.253226+00:00, state:running, queued_at: 2024-07-06 04:19:26.199722+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:19:43.194+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:14:23.253226+00:00, run_id=scheduled__2024-07-06T04:14:23.253226+00:00, run_start_date=2024-07-06 04:19:26.224820+00:00, run_end_date=2024-07-06 04:19:43.194800+00:00, run_duration=16.96998, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:14:23.253226+00:00, data_interval_end=2024-07-06 04:19:23.253226+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:19:43.198+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:19:23.253226+00:00, run_after=2024-07-06T04:24:23.253226+00:00[0m
[[34m2024-07-06T04:22:21.941+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:24:30.044+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:24:28.879938+00:00, run_after=2024-07-06T04:29:28.879938+00:00[0m
[[34m2024-07-06T04:24:30.090+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:19:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:24:30.090+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:24:30.090+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:19:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:24:30.092+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:24:30.093+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:19:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:24:30.093+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:19:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:24:30.097+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:19:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:24:31.747+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:19:28.879938+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:24:36.265+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:19:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:24:38.007+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:19:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:24:38.020+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:19:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:24:36.354635+00:00, run_end_date=2024-07-06 04:24:36.770304+00:00, run_duration=0.415669, state=success, executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:24:30.091442+00:00, queued_by_job_id=33, pid=11395[0m
[[34m2024-07-06T04:24:42.071+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:19:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:24:42.071+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:24:42.072+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:19:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:24:42.078+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:24:42.079+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:19:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:24:42.079+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:19:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:24:42.082+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:19:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:24:43.651+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:19:28.879938+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:24:47.823+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:19:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:24:51.087+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:19:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:24:51.111+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:19:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:24:47.913402+00:00, run_end_date=2024-07-06 04:24:49.570936+00:00, run_duration=1.657534, state=success, executor_state=success, try_number=1, max_tries=0, job_id=205, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:24:42.073053+00:00, queued_by_job_id=33, pid=11400[0m
[[34m2024-07-06T04:24:51.195+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:19:28.879938+00:00: scheduled__2024-07-06T04:19:28.879938+00:00, state:running, queued_at: 2024-07-06 04:24:30.039644+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:24:51.196+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:19:28.879938+00:00, run_id=scheduled__2024-07-06T04:19:28.879938+00:00, run_start_date=2024-07-06 04:24:30.057479+00:00, run_end_date=2024-07-06 04:24:51.196093+00:00, run_duration=21.138614, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:19:28.879938+00:00, data_interval_end=2024-07-06 04:24:28.879938+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:24:51.203+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:24:28.879938+00:00, run_after=2024-07-06T04:29:28.879938+00:00[0m
[[34m2024-07-06T04:27:21.991+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:29:29.524+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:29:28.879938+00:00, run_after=2024-07-06T04:34:28.879938+00:00[0m
[[34m2024-07-06T04:29:29.572+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:24:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:29:29.573+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:29:29.573+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:24:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:29:29.575+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:29:29.576+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:24:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:29:29.576+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:24:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:29:29.579+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:24:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:29:31.189+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:24:28.879938+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:29:35.572+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:24:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:29:37.322+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:24:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:29:37.343+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:24:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:29:35.661860+00:00, run_end_date=2024-07-06 04:29:36.067324+00:00, run_duration=0.405464, state=success, executor_state=success, try_number=1, max_tries=0, job_id=206, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:29:29.574100+00:00, queued_by_job_id=33, pid=11480[0m
[[34m2024-07-06T04:29:41.555+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:24:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:29:41.555+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:29:41.555+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:24:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:29:41.557+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:29:41.558+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:24:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:29:41.558+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:24:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:29:41.561+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:24:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:29:43.132+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:24:28.879938+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:29:47.371+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:24:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:29:50.294+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:24:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:29:50.307+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:24:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:29:47.461966+00:00, run_end_date=2024-07-06 04:29:49.066531+00:00, run_duration=1.604565, state=success, executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:29:41.556452+00:00, queued_by_job_id=33, pid=11487[0m
[[34m2024-07-06T04:29:54.311+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:24:28.879938+00:00: scheduled__2024-07-06T04:24:28.879938+00:00, state:running, queued_at: 2024-07-06 04:29:29.517945+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:29:54.311+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:24:28.879938+00:00, run_id=scheduled__2024-07-06T04:24:28.879938+00:00, run_start_date=2024-07-06 04:29:29.538410+00:00, run_end_date=2024-07-06 04:29:54.311557+00:00, run_duration=24.773147, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:24:28.879938+00:00, data_interval_end=2024-07-06 04:29:28.879938+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:29:54.314+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:29:28.879938+00:00, run_after=2024-07-06T04:34:28.879938+00:00[0m
[[34m2024-07-06T04:32:22.050+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:34:29.644+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:34:28.879938+00:00, run_after=2024-07-06T04:39:28.879938+00:00[0m
[[34m2024-07-06T04:34:29.691+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:29:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:34:29.691+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:34:29.691+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:29:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:34:29.693+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:34:29.694+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:29:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:34:29.694+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:29:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:34:29.697+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:29:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:34:31.312+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:29:28.879938+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:34:36.347+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:29:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:34:38.333+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:29:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:34:38.349+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:29:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:34:36.452841+00:00, run_end_date=2024-07-06 04:34:36.910720+00:00, run_duration=0.457879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=208, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:34:29.692334+00:00, queued_by_job_id=33, pid=11550[0m
[[34m2024-07-06T04:34:38.447+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:29:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:34:38.448+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:34:38.448+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:29:28.879938+00:00 [scheduled]>[0m
[[34m2024-07-06T04:34:38.450+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:34:38.451+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:29:28.879938+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:34:38.451+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:29:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:34:38.454+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:29:28.879938+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:34:40.158+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:29:28.879938+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:34:44.280+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:29:28.879938+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:34:47.218+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:29:28.879938+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:34:47.225+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:29:28.879938+00:00, map_index=-1, run_start_date=2024-07-06 04:34:44.368814+00:00, run_end_date=2024-07-06 04:34:45.963920+00:00, run_duration=1.595106, state=success, executor_state=success, try_number=1, max_tries=0, job_id=209, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:34:38.449210+00:00, queued_by_job_id=33, pid=11553[0m
[[34m2024-07-06T04:34:47.282+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:29:28.879938+00:00: scheduled__2024-07-06T04:29:28.879938+00:00, state:running, queued_at: 2024-07-06 04:34:29.638311+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:34:47.283+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:29:28.879938+00:00, run_id=scheduled__2024-07-06T04:29:28.879938+00:00, run_start_date=2024-07-06 04:34:29.657692+00:00, run_end_date=2024-07-06 04:34:47.283282+00:00, run_duration=17.62559, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:29:28.879938+00:00, data_interval_end=2024-07-06 04:34:28.879938+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:34:47.286+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:34:28.879938+00:00, run_after=2024-07-06T04:39:28.879938+00:00[0m
[[34m2024-07-06T04:37:22.097+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:39:33.360+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:39:32.173438+00:00, run_after=2024-07-06T04:44:32.173438+00:00[0m
[[34m2024-07-06T04:39:33.410+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:34:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:39:33.410+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:39:33.411+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:34:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:39:33.412+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:39:33.413+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:34:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:39:33.413+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:34:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:39:33.417+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:34:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:39:35.064+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:34:32.173438+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:39:39.451+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:34:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:39:41.257+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:34:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:39:41.270+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:34:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:39:39.543330+00:00, run_end_date=2024-07-06 04:39:39.991017+00:00, run_duration=0.447687, state=success, executor_state=success, try_number=1, max_tries=0, job_id=210, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:39:33.411674+00:00, queued_by_job_id=33, pid=11621[0m
[[34m2024-07-06T04:39:45.475+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:34:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:39:45.475+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:39:45.476+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:34:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:39:45.478+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:39:45.478+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:34:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:39:45.479+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:34:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:39:45.482+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:34:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:39:47.121+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:34:32.173438+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:39:51.363+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:34:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:39:54.308+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:34:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:39:54.315+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:34:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:39:51.454562+00:00, run_end_date=2024-07-06 04:39:52.931407+00:00, run_duration=1.476845, state=success, executor_state=success, try_number=1, max_tries=0, job_id=211, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:39:45.476756+00:00, queued_by_job_id=33, pid=11626[0m
[[34m2024-07-06T04:39:54.385+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:34:32.173438+00:00: scheduled__2024-07-06T04:34:32.173438+00:00, state:running, queued_at: 2024-07-06 04:39:33.354770+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:39:54.385+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:34:32.173438+00:00, run_id=scheduled__2024-07-06T04:34:32.173438+00:00, run_start_date=2024-07-06 04:39:33.375990+00:00, run_end_date=2024-07-06 04:39:54.385605+00:00, run_duration=21.009615, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:34:32.173438+00:00, data_interval_end=2024-07-06 04:39:32.173438+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:39:54.389+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:39:32.173438+00:00, run_after=2024-07-06T04:44:32.173438+00:00[0m
[[34m2024-07-06T04:42:26.217+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:44:33.532+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:44:32.173438+00:00, run_after=2024-07-06T04:49:32.173438+00:00[0m
[[34m2024-07-06T04:44:33.602+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:39:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:44:33.603+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:44:33.603+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:39:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:44:33.605+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:44:33.605+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:39:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:44:33.606+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:39:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:44:33.609+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:39:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:44:35.213+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:39:32.173438+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:44:39.590+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:39:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:44:41.387+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:39:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:44:41.401+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:39:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:44:39.705866+00:00, run_end_date=2024-07-06 04:44:40.127580+00:00, run_duration=0.421714, state=success, executor_state=success, try_number=1, max_tries=0, job_id=212, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:44:33.604114+00:00, queued_by_job_id=33, pid=11688[0m
[[34m2024-07-06T04:44:45.511+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:39:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:44:45.511+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:44:45.512+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:39:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:44:45.513+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:44:45.514+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:39:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:44:45.514+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:39:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:44:45.519+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:39:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:44:47.082+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:39:32.173438+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:44:51.486+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:39:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:44:54.586+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:39:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:44:54.598+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:39:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:44:51.575380+00:00, run_end_date=2024-07-06 04:44:53.236205+00:00, run_duration=1.660825, state=success, executor_state=success, try_number=1, max_tries=0, job_id=213, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:44:45.512693+00:00, queued_by_job_id=33, pid=11695[0m
[[34m2024-07-06T04:44:58.679+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:39:32.173438+00:00: scheduled__2024-07-06T04:39:32.173438+00:00, state:running, queued_at: 2024-07-06 04:44:33.527323+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:44:58.680+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:39:32.173438+00:00, run_id=scheduled__2024-07-06T04:39:32.173438+00:00, run_start_date=2024-07-06 04:44:33.554730+00:00, run_end_date=2024-07-06 04:44:58.680276+00:00, run_duration=25.125546, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:39:32.173438+00:00, data_interval_end=2024-07-06 04:44:32.173438+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:44:58.684+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:44:32.173438+00:00, run_after=2024-07-06T04:49:32.173438+00:00[0m
[[34m2024-07-06T04:47:26.267+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:49:34.381+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:49:32.173438+00:00, run_after=2024-07-06T04:54:32.173438+00:00[0m
[[34m2024-07-06T04:49:34.429+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:44:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:49:34.429+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:49:34.429+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:44:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:49:34.431+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:49:34.432+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:44:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:49:34.432+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:44:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:49:34.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:44:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:49:36.014+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:44:32.173438+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:49:40.434+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:44:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:49:42.199+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:44:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:49:42.214+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:44:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:49:40.526285+00:00, run_end_date=2024-07-06 04:49:40.954823+00:00, run_duration=0.428538, state=success, executor_state=success, try_number=1, max_tries=0, job_id=214, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:49:34.430508+00:00, queued_by_job_id=33, pid=11762[0m
[[34m2024-07-06T04:49:42.312+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:44:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:49:42.313+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:49:42.313+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:44:32.173438+00:00 [scheduled]>[0m
[[34m2024-07-06T04:49:42.315+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:49:42.316+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:44:32.173438+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:49:42.316+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:44:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:49:42.320+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:44:32.173438+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:49:43.889+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:44:32.173438+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:49:48.088+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:44:32.173438+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:49:51.126+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:44:32.173438+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:49:51.140+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:44:32.173438+00:00, map_index=-1, run_start_date=2024-07-06 04:49:48.184754+00:00, run_end_date=2024-07-06 04:49:49.813438+00:00, run_duration=1.628684, state=success, executor_state=success, try_number=1, max_tries=0, job_id=215, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:49:42.314174+00:00, queued_by_job_id=33, pid=11765[0m
[[34m2024-07-06T04:49:51.199+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:44:32.173438+00:00: scheduled__2024-07-06T04:44:32.173438+00:00, state:running, queued_at: 2024-07-06 04:49:34.375204+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:49:51.200+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:44:32.173438+00:00, run_id=scheduled__2024-07-06T04:44:32.173438+00:00, run_start_date=2024-07-06 04:49:34.395039+00:00, run_end_date=2024-07-06 04:49:51.199877+00:00, run_duration=16.804838, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:44:32.173438+00:00, data_interval_end=2024-07-06 04:49:32.173438+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:49:51.203+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:49:32.173438+00:00, run_after=2024-07-06T04:54:32.173438+00:00[0m
[[34m2024-07-06T04:52:28.971+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:54:37.050+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:54:35.876110+00:00, run_after=2024-07-06T04:59:35.876110+00:00[0m
[[34m2024-07-06T04:54:37.103+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:49:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:54:37.104+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:54:37.104+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:49:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:54:37.106+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:54:37.107+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:49:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:54:37.107+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:49:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:54:37.111+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:49:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:54:38.717+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:49:35.876110+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:54:43.094+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:49:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:54:44.910+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:49:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:54:44.923+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:49:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 04:54:43.191414+00:00, run_end_date=2024-07-06 04:54:43.626988+00:00, run_duration=0.435574, state=success, executor_state=success, try_number=1, max_tries=0, job_id=216, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:54:37.105108+00:00, queued_by_job_id=33, pid=11834[0m
[[34m2024-07-06T04:54:49.126+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:49:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:54:49.126+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:54:49.127+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:49:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:54:49.129+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:54:49.129+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:49:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:54:49.130+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:49:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:54:49.133+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:49:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:54:50.740+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:49:35.876110+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:54:55.054+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:49:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:54:58.010+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:49:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:54:58.024+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:49:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 04:54:55.144310+00:00, run_end_date=2024-07-06 04:54:56.780738+00:00, run_duration=1.636428, state=success, executor_state=success, try_number=1, max_tries=0, job_id=217, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:54:49.127726+00:00, queued_by_job_id=33, pid=11839[0m
[[34m2024-07-06T04:54:58.106+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:49:35.876110+00:00: scheduled__2024-07-06T04:49:35.876110+00:00, state:running, queued_at: 2024-07-06 04:54:37.045754+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:54:58.106+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:49:35.876110+00:00, run_id=scheduled__2024-07-06T04:49:35.876110+00:00, run_start_date=2024-07-06 04:54:37.063166+00:00, run_end_date=2024-07-06 04:54:58.106710+00:00, run_duration=21.043544, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:49:35.876110+00:00, data_interval_end=2024-07-06 04:54:35.876110+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:54:58.110+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:54:35.876110+00:00, run_after=2024-07-06T04:59:35.876110+00:00[0m
[[34m2024-07-06T04:57:30.108+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T04:59:36.219+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:59:35.876110+00:00, run_after=2024-07-06T05:04:35.876110+00:00[0m
[[34m2024-07-06T04:59:36.284+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:54:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:59:36.289+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:59:36.289+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:54:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:59:36.291+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:59:36.292+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:54:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T04:59:36.292+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:54:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:59:36.298+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:54:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:59:37.894+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:54:35.876110+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T04:59:42.288+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:54:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:59:44.110+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:54:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:59:44.122+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:54:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 04:59:42.379902+00:00, run_end_date=2024-07-06 04:59:42.832224+00:00, run_duration=0.452322, state=success, executor_state=success, try_number=1, max_tries=0, job_id=218, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 04:59:36.290504+00:00, queued_by_job_id=33, pid=11895[0m
[[34m2024-07-06T04:59:44.199+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:54:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:59:44.200+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T04:59:44.200+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:54:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T04:59:44.202+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T04:59:44.202+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:54:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T04:59:44.202+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:54:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:59:44.206+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:54:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T04:59:45.750+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:54:35.876110+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T04:59:49.954+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:54:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T04:59:52.826+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:54:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T04:59:52.832+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:54:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 04:59:50.045928+00:00, run_end_date=2024-07-06 04:59:51.456145+00:00, run_duration=1.410217, state=success, executor_state=success, try_number=1, max_tries=0, job_id=219, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 04:59:44.200808+00:00, queued_by_job_id=33, pid=11899[0m
[[34m2024-07-06T04:59:57.228+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:54:35.876110+00:00: scheduled__2024-07-06T04:54:35.876110+00:00, state:running, queued_at: 2024-07-06 04:59:36.214569+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T04:59:57.229+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:54:35.876110+00:00, run_id=scheduled__2024-07-06T04:54:35.876110+00:00, run_start_date=2024-07-06 04:59:36.233474+00:00, run_end_date=2024-07-06 04:59:57.229713+00:00, run_duration=20.996239, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:54:35.876110+00:00, data_interval_end=2024-07-06 04:59:35.876110+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T04:59:57.233+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T04:59:35.876110+00:00, run_after=2024-07-06T05:04:35.876110+00:00[0m
[[34m2024-07-06T05:02:30.152+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:04:39.319+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:04:35.876110+00:00, run_after=2024-07-06T05:09:35.876110+00:00[0m
[[34m2024-07-06T05:04:39.369+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:59:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:04:39.369+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:04:39.369+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:59:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:04:39.371+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:04:39.372+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:59:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:04:39.372+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:59:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:04:39.375+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T04:59:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:04:40.997+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:59:35.876110+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:04:45.354+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T04:59:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:04:47.133+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T04:59:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:04:47.139+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T04:59:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:04:45.448395+00:00, run_end_date=2024-07-06 05:04:45.890830+00:00, run_duration=0.442435, state=success, executor_state=success, try_number=1, max_tries=0, job_id=220, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:04:39.370203+00:00, queued_by_job_id=33, pid=11965[0m
[[34m2024-07-06T05:04:47.219+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:59:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:04:47.219+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:04:47.220+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:59:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:04:47.222+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:04:47.222+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:59:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:04:47.222+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:59:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:04:47.226+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T04:59:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:04:48.832+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T04:59:35.876110+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:04:53.153+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T04:59:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:04:56.179+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T04:59:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:04:56.193+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T04:59:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:04:53.248650+00:00, run_end_date=2024-07-06 05:04:54.910996+00:00, run_duration=1.662346, state=success, executor_state=success, try_number=1, max_tries=0, job_id=221, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:04:47.220844+00:00, queued_by_job_id=33, pid=11968[0m
[[34m2024-07-06T05:04:56.256+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 04:59:35.876110+00:00: scheduled__2024-07-06T04:59:35.876110+00:00, state:running, queued_at: 2024-07-06 05:04:39.312278+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:04:56.256+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 04:59:35.876110+00:00, run_id=scheduled__2024-07-06T04:59:35.876110+00:00, run_start_date=2024-07-06 05:04:39.333789+00:00, run_end_date=2024-07-06 05:04:56.256788+00:00, run_duration=16.922999, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 04:59:35.876110+00:00, data_interval_end=2024-07-06 05:04:35.876110+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:04:56.260+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:04:35.876110+00:00, run_after=2024-07-06T05:09:35.876110+00:00[0m
[[34m2024-07-06T05:07:33.425+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:09:36.300+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:09:35.876110+00:00, run_after=2024-07-06T05:14:35.876110+00:00[0m
[[34m2024-07-06T05:09:36.352+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:04:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:09:36.352+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:09:36.352+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:04:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:09:36.354+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:09:36.355+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:04:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:09:36.355+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:04:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:09:36.359+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:04:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:09:37.998+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:04:35.876110+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:09:42.283+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:04:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:09:44.045+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:04:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:09:44.058+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:04:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:09:42.372095+00:00, run_end_date=2024-07-06 05:09:42.799300+00:00, run_duration=0.427205, state=success, executor_state=success, try_number=1, max_tries=0, job_id=222, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:09:36.353275+00:00, queued_by_job_id=33, pid=12031[0m
[[34m2024-07-06T05:09:48.385+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:04:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:09:48.386+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:09:48.386+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:04:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:09:48.388+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:09:48.389+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:04:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:09:48.390+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:04:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:09:48.394+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:04:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:09:50.117+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:04:35.876110+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:09:54.648+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:04:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:09:57.872+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:04:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:09:57.885+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:04:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:09:54.741633+00:00, run_end_date=2024-07-06 05:09:56.628687+00:00, run_duration=1.887054, state=success, executor_state=success, try_number=1, max_tries=0, job_id=223, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:09:48.387001+00:00, queued_by_job_id=33, pid=12037[0m
[[34m2024-07-06T05:10:02.077+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:04:35.876110+00:00: scheduled__2024-07-06T05:04:35.876110+00:00, state:running, queued_at: 2024-07-06 05:09:36.294063+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:10:02.078+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:04:35.876110+00:00, run_id=scheduled__2024-07-06T05:04:35.876110+00:00, run_start_date=2024-07-06 05:09:36.316485+00:00, run_end_date=2024-07-06 05:10:02.077851+00:00, run_duration=25.761366, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:04:35.876110+00:00, data_interval_end=2024-07-06 05:09:35.876110+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:10:02.081+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:09:35.876110+00:00, run_after=2024-07-06T05:14:35.876110+00:00[0m
[[34m2024-07-06T05:12:33.470+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:14:37.157+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:14:35.876110+00:00, run_after=2024-07-06T05:19:35.876110+00:00[0m
[[34m2024-07-06T05:14:37.207+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:09:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:14:37.208+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:14:37.208+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:09:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:14:37.210+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:14:37.211+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:09:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:14:37.211+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:09:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:14:37.214+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:09:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:14:38.926+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:09:35.876110+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:14:43.245+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:09:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:14:45.165+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:09:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:14:45.174+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:09:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:14:43.334402+00:00, run_end_date=2024-07-06 05:14:43.796897+00:00, run_duration=0.462495, state=success, executor_state=success, try_number=1, max_tries=0, job_id=224, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:14:37.209199+00:00, queued_by_job_id=33, pid=12093[0m
[[34m2024-07-06T05:14:45.255+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:09:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:14:45.255+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:14:45.256+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:09:35.876110+00:00 [scheduled]>[0m
[[34m2024-07-06T05:14:45.257+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:14:45.258+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:09:35.876110+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:14:45.258+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:09:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:14:45.261+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:09:35.876110+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:14:46.798+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:09:35.876110+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:14:51.149+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:09:35.876110+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:14:54.215+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:09:35.876110+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:14:54.227+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:09:35.876110+00:00, map_index=-1, run_start_date=2024-07-06 05:14:51.242929+00:00, run_end_date=2024-07-06 05:14:52.923716+00:00, run_duration=1.680787, state=success, executor_state=success, try_number=1, max_tries=0, job_id=225, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:14:45.256549+00:00, queued_by_job_id=33, pid=12096[0m
[[34m2024-07-06T05:14:54.286+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:09:35.876110+00:00: scheduled__2024-07-06T05:09:35.876110+00:00, state:running, queued_at: 2024-07-06 05:14:37.151515+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:14:54.287+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:09:35.876110+00:00, run_id=scheduled__2024-07-06T05:09:35.876110+00:00, run_start_date=2024-07-06 05:14:37.173614+00:00, run_end_date=2024-07-06 05:14:54.286948+00:00, run_duration=17.113334, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:09:35.876110+00:00, data_interval_end=2024-07-06 05:14:35.876110+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:14:54.290+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:14:35.876110+00:00, run_after=2024-07-06T05:19:35.876110+00:00[0m
[[34m2024-07-06T05:17:33.519+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:19:39.912+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:19:38.757490+00:00, run_after=2024-07-06T05:24:38.757490+00:00[0m
[[34m2024-07-06T05:19:39.959+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:14:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:19:39.959+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:19:39.959+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:14:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:19:39.961+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:19:39.961+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:14:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:19:39.962+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:14:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:19:39.965+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:14:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:19:41.511+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:14:38.757490+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:19:45.706+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:14:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:19:47.437+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:14:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:19:47.442+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:14:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:19:45.817418+00:00, run_end_date=2024-07-06 05:19:46.225025+00:00, run_duration=0.407607, state=success, executor_state=success, try_number=1, max_tries=0, job_id=226, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:19:39.960245+00:00, queued_by_job_id=33, pid=12183[0m
[[34m2024-07-06T05:19:51.419+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:14:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:19:51.420+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:19:51.420+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:14:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:19:51.422+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:19:51.422+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:14:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:19:51.422+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:14:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:19:51.425+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:14:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:19:52.972+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:14:38.757490+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:19:57.170+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:14:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:20:00.182+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:14:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:20:00.194+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:14:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:19:57.258036+00:00, run_end_date=2024-07-06 05:19:58.926903+00:00, run_duration=1.668867, state=success, executor_state=success, try_number=1, max_tries=0, job_id=227, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:19:51.420916+00:00, queued_by_job_id=33, pid=12188[0m
[[34m2024-07-06T05:20:00.265+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:14:38.757490+00:00: scheduled__2024-07-06T05:14:38.757490+00:00, state:running, queued_at: 2024-07-06 05:19:39.907124+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:20:00.266+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:14:38.757490+00:00, run_id=scheduled__2024-07-06T05:14:38.757490+00:00, run_start_date=2024-07-06 05:19:39.925766+00:00, run_end_date=2024-07-06 05:20:00.266377+00:00, run_duration=20.340611, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:14:38.757490+00:00, data_interval_end=2024-07-06 05:19:38.757490+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:20:00.269+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:19:38.757490+00:00, run_after=2024-07-06T05:24:38.757490+00:00[0m
[[34m2024-07-06T05:22:33.582+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:24:39.591+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:24:38.757490+00:00, run_after=2024-07-06T05:29:38.757490+00:00[0m
[[34m2024-07-06T05:24:39.637+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:19:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:24:39.638+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:24:39.638+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:19:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:24:39.640+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:24:39.640+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:19:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:24:39.641+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:19:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:24:39.644+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:19:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:24:41.280+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:19:38.757490+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:24:45.574+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:19:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:24:47.442+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:19:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:24:47.449+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:19:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:24:45.667725+00:00, run_end_date=2024-07-06 05:24:46.120857+00:00, run_duration=0.453132, state=success, executor_state=success, try_number=1, max_tries=0, job_id=228, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:24:39.638971+00:00, queued_by_job_id=33, pid=12260[0m
[[34m2024-07-06T05:24:51.862+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:19:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:24:51.863+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:24:51.863+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:19:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:24:51.865+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:24:51.866+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:19:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:24:51.867+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:19:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:24:51.871+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:19:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:24:53.505+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:19:38.757490+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:24:57.677+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:19:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:25:00.924+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:19:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:25:00.938+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:19:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:24:57.767069+00:00, run_end_date=2024-07-06 05:24:59.475025+00:00, run_duration=1.707956, state=success, executor_state=success, try_number=1, max_tries=0, job_id=229, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:24:51.864247+00:00, queued_by_job_id=33, pid=12266[0m
[[34m2024-07-06T05:25:05.228+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:19:38.757490+00:00: scheduled__2024-07-06T05:19:38.757490+00:00, state:running, queued_at: 2024-07-06 05:24:39.586301+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:25:05.229+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:19:38.757490+00:00, run_id=scheduled__2024-07-06T05:19:38.757490+00:00, run_start_date=2024-07-06 05:24:39.605365+00:00, run_end_date=2024-07-06 05:25:05.229001+00:00, run_duration=25.623636, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:19:38.757490+00:00, data_interval_end=2024-07-06 05:24:38.757490+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:25:05.232+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:24:38.757490+00:00, run_after=2024-07-06T05:29:38.757490+00:00[0m
[[34m2024-07-06T05:27:33.632+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:29:42.031+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:29:38.757490+00:00, run_after=2024-07-06T05:34:38.757490+00:00[0m
[[34m2024-07-06T05:29:42.089+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:24:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:29:42.089+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:29:42.089+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:24:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:29:42.091+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:29:42.092+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:24:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:29:42.092+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:24:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:29:42.095+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:24:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:29:43.676+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:24:38.757490+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:29:48.098+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:24:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:29:49.896+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:24:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:29:49.909+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:24:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:29:48.191349+00:00, run_end_date=2024-07-06 05:29:48.635488+00:00, run_duration=0.444139, state=success, executor_state=success, try_number=1, max_tries=0, job_id=230, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:29:42.090205+00:00, queued_by_job_id=33, pid=12328[0m
[[34m2024-07-06T05:29:49.997+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:24:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:29:49.998+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:29:49.998+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:24:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:29:50.000+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:29:50.001+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:24:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:29:50.001+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:24:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:29:50.004+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:24:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:29:51.511+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:24:38.757490+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:29:55.640+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:24:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:29:58.593+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:24:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:29:58.606+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:24:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:29:55.736172+00:00, run_end_date=2024-07-06 05:29:57.363023+00:00, run_duration=1.626851, state=success, executor_state=success, try_number=1, max_tries=0, job_id=231, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:29:49.999138+00:00, queued_by_job_id=33, pid=12331[0m
[[34m2024-07-06T05:29:58.683+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:24:38.757490+00:00: scheduled__2024-07-06T05:24:38.757490+00:00, state:running, queued_at: 2024-07-06 05:29:42.025327+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:29:58.683+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:24:38.757490+00:00, run_id=scheduled__2024-07-06T05:24:38.757490+00:00, run_start_date=2024-07-06 05:29:42.046077+00:00, run_end_date=2024-07-06 05:29:58.683486+00:00, run_duration=16.637409, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:24:38.757490+00:00, data_interval_end=2024-07-06 05:29:38.757490+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:29:58.687+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:29:38.757490+00:00, run_after=2024-07-06T05:34:38.757490+00:00[0m
[[34m2024-07-06T05:32:35.746+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:34:39.078+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:34:38.757490+00:00, run_after=2024-07-06T05:39:38.757490+00:00[0m
[[34m2024-07-06T05:34:39.121+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:29:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:34:39.122+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:34:39.122+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:29:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:34:39.124+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:34:39.124+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:29:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:34:39.125+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:29:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:34:39.128+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:29:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:34:40.708+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:29:38.757490+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:34:45.112+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:29:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:34:46.869+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:29:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:34:46.881+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:29:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:34:45.218686+00:00, run_end_date=2024-07-06 05:34:45.638109+00:00, run_duration=0.419423, state=success, executor_state=success, try_number=1, max_tries=0, job_id=232, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:34:39.123027+00:00, queued_by_job_id=33, pid=12440[0m
[[34m2024-07-06T05:34:51.089+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:29:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:34:51.090+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:34:51.090+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:29:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:34:51.092+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:34:51.092+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:29:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:34:51.092+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:29:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:34:51.096+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:29:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:34:52.602+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:29:38.757490+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:34:56.878+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:29:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:34:59.822+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:29:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:34:59.844+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:29:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:34:56.978378+00:00, run_end_date=2024-07-06 05:34:58.582963+00:00, run_duration=1.604585, state=success, executor_state=success, try_number=1, max_tries=0, job_id=233, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:34:51.090780+00:00, queued_by_job_id=33, pid=12445[0m
[[34m2024-07-06T05:35:04.051+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:29:38.757490+00:00: scheduled__2024-07-06T05:29:38.757490+00:00, state:running, queued_at: 2024-07-06 05:34:39.073005+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:35:04.052+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:29:38.757490+00:00, run_id=scheduled__2024-07-06T05:29:38.757490+00:00, run_start_date=2024-07-06 05:34:39.090956+00:00, run_end_date=2024-07-06 05:35:04.051875+00:00, run_duration=24.960919, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:29:38.757490+00:00, data_interval_end=2024-07-06 05:34:38.757490+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:35:04.055+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:34:38.757490+00:00, run_after=2024-07-06T05:39:38.757490+00:00[0m
[[34m2024-07-06T05:37:35.793+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:39:40.021+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:39:38.757490+00:00, run_after=2024-07-06T05:44:38.757490+00:00[0m
[[34m2024-07-06T05:39:40.072+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:34:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:39:40.072+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:39:40.073+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:34:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:39:40.074+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:39:40.075+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:34:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:39:40.076+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:34:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:39:40.079+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:34:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:39:41.651+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:34:38.757490+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:39:46.129+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:34:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:39:48.012+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:34:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:39:48.025+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:34:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:39:46.226078+00:00, run_end_date=2024-07-06 05:39:46.646664+00:00, run_duration=0.420586, state=success, executor_state=success, try_number=1, max_tries=0, job_id=234, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:39:40.073765+00:00, queued_by_job_id=33, pid=12505[0m
[[34m2024-07-06T05:39:48.119+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:34:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:39:48.119+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:39:48.120+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:34:38.757490+00:00 [scheduled]>[0m
[[34m2024-07-06T05:39:48.122+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:39:48.123+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:34:38.757490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:39:48.123+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:34:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:39:48.127+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:34:38.757490+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:39:49.662+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:34:38.757490+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:39:54.039+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:34:38.757490+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:39:57.105+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:34:38.757490+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:39:57.121+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:34:38.757490+00:00, map_index=-1, run_start_date=2024-07-06 05:39:54.147439+00:00, run_end_date=2024-07-06 05:39:55.790040+00:00, run_duration=1.642601, state=success, executor_state=success, try_number=1, max_tries=0, job_id=235, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:39:48.120777+00:00, queued_by_job_id=33, pid=12509[0m
[[34m2024-07-06T05:39:57.191+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:34:38.757490+00:00: scheduled__2024-07-06T05:34:38.757490+00:00, state:running, queued_at: 2024-07-06 05:39:40.015331+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:39:57.191+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:34:38.757490+00:00, run_id=scheduled__2024-07-06T05:34:38.757490+00:00, run_start_date=2024-07-06 05:39:40.039068+00:00, run_end_date=2024-07-06 05:39:57.191754+00:00, run_duration=17.152686, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:34:38.757490+00:00, data_interval_end=2024-07-06 05:39:38.757490+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:39:57.196+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:39:38.757490+00:00, run_after=2024-07-06T05:44:38.757490+00:00[0m
[[34m2024-07-06T05:42:35.842+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:44:41.528+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:44:40.370163+00:00, run_after=2024-07-06T05:49:40.370163+00:00[0m
[[34m2024-07-06T05:44:41.582+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:39:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:44:41.582+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:44:41.582+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:39:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:44:41.584+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:44:41.585+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:39:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:44:41.585+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:39:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:44:41.589+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:39:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:44:43.189+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:39:40.370163+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:44:47.528+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:39:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:44:49.500+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:39:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:44:49.515+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:39:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:44:47.634931+00:00, run_end_date=2024-07-06 05:44:48.111425+00:00, run_duration=0.476494, state=success, executor_state=success, try_number=1, max_tries=0, job_id=236, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:44:41.583324+00:00, queued_by_job_id=33, pid=12569[0m
[[34m2024-07-06T05:44:53.746+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:39:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:44:53.746+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:44:53.747+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:39:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:44:53.749+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:44:53.749+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:39:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:44:53.750+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:39:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:44:53.754+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:39:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:44:55.368+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:39:40.370163+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:44:59.569+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:39:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:45:02.761+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:39:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:45:02.769+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:39:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:44:59.657086+00:00, run_end_date=2024-07-06 05:45:01.294490+00:00, run_duration=1.637404, state=success, executor_state=success, try_number=1, max_tries=0, job_id=237, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:44:53.747703+00:00, queued_by_job_id=33, pid=12576[0m
[[34m2024-07-06T05:45:02.839+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:39:40.370163+00:00: scheduled__2024-07-06T05:39:40.370163+00:00, state:running, queued_at: 2024-07-06 05:44:41.523473+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:45:02.840+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:39:40.370163+00:00, run_id=scheduled__2024-07-06T05:39:40.370163+00:00, run_start_date=2024-07-06 05:44:41.549002+00:00, run_end_date=2024-07-06 05:45:02.840496+00:00, run_duration=21.291494, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:39:40.370163+00:00, data_interval_end=2024-07-06 05:44:40.370163+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:45:02.844+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:44:40.370163+00:00, run_after=2024-07-06T05:49:40.370163+00:00[0m
[[34m2024-07-06T05:47:35.891+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:49:41.369+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:49:40.370163+00:00, run_after=2024-07-06T05:54:40.370163+00:00[0m
[[34m2024-07-06T05:49:41.422+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:44:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:49:41.423+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:49:41.423+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:44:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:49:41.425+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:49:41.425+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:44:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:49:41.426+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:44:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:49:41.429+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:44:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:49:43.099+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:44:40.370163+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:49:47.606+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:44:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:49:49.397+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:44:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:49:49.410+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:44:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:49:47.695778+00:00, run_end_date=2024-07-06 05:49:48.130052+00:00, run_duration=0.434274, state=success, executor_state=success, try_number=1, max_tries=0, job_id=238, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:49:41.423894+00:00, queued_by_job_id=33, pid=12648[0m
[[34m2024-07-06T05:49:53.823+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:44:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:49:53.823+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:49:53.823+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:44:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:49:53.825+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:49:53.826+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:44:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:49:53.826+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:44:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:49:53.830+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:44:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:49:55.397+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:44:40.370163+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:49:59.896+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:44:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:50:03.103+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:44:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:50:03.115+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:44:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:49:59.991307+00:00, run_end_date=2024-07-06 05:50:01.741626+00:00, run_duration=1.750319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=239, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:49:53.824260+00:00, queued_by_job_id=33, pid=12655[0m
[[34m2024-07-06T05:50:07.431+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:44:40.370163+00:00: scheduled__2024-07-06T05:44:40.370163+00:00, state:running, queued_at: 2024-07-06 05:49:41.359016+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:50:07.432+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:44:40.370163+00:00, run_id=scheduled__2024-07-06T05:44:40.370163+00:00, run_start_date=2024-07-06 05:49:41.385405+00:00, run_end_date=2024-07-06 05:50:07.432497+00:00, run_duration=26.047092, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:44:40.370163+00:00, data_interval_end=2024-07-06 05:49:40.370163+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:50:07.440+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:49:40.370163+00:00, run_after=2024-07-06T05:54:40.370163+00:00[0m
[[34m2024-07-06T05:52:35.950+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:54:43.357+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:54:40.370163+00:00, run_after=2024-07-06T05:59:40.370163+00:00[0m
[[34m2024-07-06T05:54:43.414+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:49:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:54:43.414+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:54:43.415+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:49:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:54:43.417+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:54:43.418+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:49:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:54:43.418+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:49:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:54:43.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:49:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:54:44.988+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:49:40.370163+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:54:49.206+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:49:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:54:51.164+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:49:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:54:51.171+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:49:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:54:49.301951+00:00, run_end_date=2024-07-06 05:54:49.752556+00:00, run_duration=0.450605, state=success, executor_state=success, try_number=1, max_tries=0, job_id=240, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:54:43.415902+00:00, queued_by_job_id=33, pid=12753[0m
[[34m2024-07-06T05:54:51.248+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:49:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:54:51.249+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:54:51.249+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:49:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:54:51.251+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:54:51.251+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:49:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:54:51.252+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:49:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:54:51.256+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:49:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:54:52.891+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:49:40.370163+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:54:57.766+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:49:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:55:01.209+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:49:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:55:01.225+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:49:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:54:57.863886+00:00, run_end_date=2024-07-06 05:54:59.712408+00:00, run_duration=1.848522, state=success, executor_state=success, try_number=1, max_tries=0, job_id=241, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:54:51.249975+00:00, queued_by_job_id=33, pid=12757[0m
[[34m2024-07-06T05:55:01.294+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:49:40.370163+00:00: scheduled__2024-07-06T05:49:40.370163+00:00, state:running, queued_at: 2024-07-06 05:54:43.352371+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T05:55:01.294+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:49:40.370163+00:00, run_id=scheduled__2024-07-06T05:49:40.370163+00:00, run_start_date=2024-07-06 05:54:43.374238+00:00, run_end_date=2024-07-06 05:55:01.294606+00:00, run_duration=17.920368, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:49:40.370163+00:00, data_interval_end=2024-07-06 05:54:40.370163+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T05:55:01.298+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:54:40.370163+00:00, run_after=2024-07-06T05:59:40.370163+00:00[0m
[[34m2024-07-06T05:57:37.688+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T05:59:41.369+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:59:40.370163+00:00, run_after=2024-07-06T06:04:40.370163+00:00[0m
[[34m2024-07-06T05:59:41.419+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:54:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:59:41.419+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:59:41.419+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:54:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:59:41.421+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:59:41.422+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:54:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T05:59:41.422+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:54:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:59:41.426+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:54:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:59:43.061+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:54:40.370163+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T05:59:47.536+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:54:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T05:59:49.427+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:54:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T05:59:49.448+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:54:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:59:47.639513+00:00, run_end_date=2024-07-06 05:59:48.103890+00:00, run_duration=0.464377, state=success, executor_state=success, try_number=1, max_tries=0, job_id=242, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 05:59:41.420408+00:00, queued_by_job_id=33, pid=12936[0m
[[34m2024-07-06T05:59:53.587+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:54:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:59:53.588+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T05:59:53.588+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:54:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T05:59:53.590+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T05:59:53.591+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:54:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T05:59:53.592+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:54:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:59:53.596+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:54:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T05:59:55.122+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:54:40.370163+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T05:59:59.327+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:54:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:00:02.548+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:54:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:00:02.563+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:54:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 05:59:59.415821+00:00, run_end_date=2024-07-06 06:00:01.076089+00:00, run_duration=1.660268, state=success, executor_state=success, try_number=1, max_tries=0, job_id=243, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 05:59:53.589536+00:00, queued_by_job_id=33, pid=12942[0m
[[34m2024-07-06T06:00:06.771+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:54:40.370163+00:00: scheduled__2024-07-06T05:54:40.370163+00:00, state:running, queued_at: 2024-07-06 05:59:41.364231+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:00:06.772+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:54:40.370163+00:00, run_id=scheduled__2024-07-06T05:54:40.370163+00:00, run_start_date=2024-07-06 05:59:41.384238+00:00, run_end_date=2024-07-06 06:00:06.772252+00:00, run_duration=25.388014, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:54:40.370163+00:00, data_interval_end=2024-07-06 05:59:40.370163+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:00:06.776+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T05:59:40.370163+00:00, run_after=2024-07-06T06:04:40.370163+00:00[0m
[[34m2024-07-06T06:02:37.744+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:04:41.788+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:04:40.370163+00:00, run_after=2024-07-06T06:09:40.370163+00:00[0m
[[34m2024-07-06T06:04:41.836+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:59:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T06:04:41.836+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:04:41.836+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:59:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T06:04:41.838+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:04:41.839+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:59:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:04:41.839+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:59:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:04:41.842+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T05:59:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:04:43.451+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:59:40.370163+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:04:47.759+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T05:59:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:04:49.583+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T05:59:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:04:49.589+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T05:59:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 06:04:47.862005+00:00, run_end_date=2024-07-06 06:04:48.291578+00:00, run_duration=0.429573, state=success, executor_state=success, try_number=1, max_tries=0, job_id=244, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:04:41.837531+00:00, queued_by_job_id=33, pid=13078[0m
[[34m2024-07-06T06:04:49.670+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:59:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T06:04:49.670+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:04:49.671+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:59:40.370163+00:00 [scheduled]>[0m
[[34m2024-07-06T06:04:49.672+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:04:49.673+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:59:40.370163+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:04:49.673+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:59:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:04:49.678+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T05:59:40.370163+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:04:51.284+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T05:59:40.370163+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:04:55.751+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T05:59:40.370163+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:04:58.819+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T05:59:40.370163+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:04:58.833+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T05:59:40.370163+00:00, map_index=-1, run_start_date=2024-07-06 06:04:55.857491+00:00, run_end_date=2024-07-06 06:04:57.540388+00:00, run_duration=1.682897, state=success, executor_state=success, try_number=1, max_tries=0, job_id=245, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:04:49.671577+00:00, queued_by_job_id=33, pid=13085[0m
[[34m2024-07-06T06:04:58.904+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 05:59:40.370163+00:00: scheduled__2024-07-06T05:59:40.370163+00:00, state:running, queued_at: 2024-07-06 06:04:41.781995+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:04:58.905+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 05:59:40.370163+00:00, run_id=scheduled__2024-07-06T05:59:40.370163+00:00, run_start_date=2024-07-06 06:04:41.803614+00:00, run_end_date=2024-07-06 06:04:58.904877+00:00, run_duration=17.101263, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 05:59:40.370163+00:00, data_interval_end=2024-07-06 06:04:40.370163+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:04:58.910+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:04:40.370163+00:00, run_after=2024-07-06T06:09:40.370163+00:00[0m
[[34m2024-07-06T06:07:37.791+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:09:46.746+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:09:45.508019+00:00, run_after=2024-07-06T06:14:45.508019+00:00[0m
[[34m2024-07-06T06:09:46.791+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:04:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:09:46.791+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:09:46.791+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:04:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:09:46.793+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:09:46.794+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:04:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:09:46.794+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:04:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:09:46.798+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:04:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:09:50.548+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:04:45.508019+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:10:01.077+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:04:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:10:05.478+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:04:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:10:05.508+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:04:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:10:01.284056+00:00, run_end_date=2024-07-06 06:10:02.302274+00:00, run_duration=1.018218, state=success, executor_state=success, try_number=1, max_tries=0, job_id=246, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:09:46.792446+00:00, queued_by_job_id=33, pid=13845[0m
[[34m2024-07-06T06:10:11.650+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:04:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:10:11.651+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:10:11.651+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:04:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:10:11.653+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:10:11.654+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:04:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:10:11.654+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:04:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:10:11.658+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:04:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:10:13.497+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:04:45.508019+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:10:17.816+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:04:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:10:20.784+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:04:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:10:20.798+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:04:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:10:17.909470+00:00, run_end_date=2024-07-06 06:10:19.509382+00:00, run_duration=1.599912, state=success, executor_state=success, try_number=1, max_tries=0, job_id=247, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:10:11.651959+00:00, queued_by_job_id=33, pid=13920[0m
[[34m2024-07-06T06:10:24.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:04:45.508019+00:00: scheduled__2024-07-06T06:04:45.508019+00:00, state:running, queued_at: 2024-07-06 06:09:46.738235+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:10:24.994+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:04:45.508019+00:00, run_id=scheduled__2024-07-06T06:04:45.508019+00:00, run_start_date=2024-07-06 06:09:46.759800+00:00, run_end_date=2024-07-06 06:10:24.994283+00:00, run_duration=38.234483, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:04:45.508019+00:00, data_interval_end=2024-07-06 06:09:45.508019+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:10:24.997+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:09:45.508019+00:00, run_after=2024-07-06T06:14:45.508019+00:00[0m
[[34m2024-07-06T06:12:37.810+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:14:50.451+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:14:45.508019+00:00, run_after=2024-07-06T06:19:45.508019+00:00[0m
[[34m2024-07-06T06:14:50.500+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:09:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:14:50.500+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:14:50.500+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:09:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:14:50.502+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:14:50.503+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:09:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:14:50.503+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:09:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:14:50.507+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:09:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:14:52.425+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:09:45.508019+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:14:57.166+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:09:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:14:58.995+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:09:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:14:59.008+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:09:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:14:57.266874+00:00, run_end_date=2024-07-06 06:14:57.727687+00:00, run_duration=0.460813, state=success, executor_state=success, try_number=1, max_tries=0, job_id=248, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:14:50.501419+00:00, queued_by_job_id=33, pid=14044[0m
[[34m2024-07-06T06:15:03.220+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:09:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:15:03.221+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:15:03.221+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:09:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:15:03.223+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:15:03.224+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:09:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:15:03.224+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:09:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:15:03.228+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:09:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:15:04.864+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:09:45.508019+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:15:09.306+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:09:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:15:13.529+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:09:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:15:13.546+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:09:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:15:09.405021+00:00, run_end_date=2024-07-06 06:15:11.065733+00:00, run_duration=1.660712, state=success, executor_state=success, try_number=1, max_tries=0, job_id=249, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:15:03.222088+00:00, queued_by_job_id=33, pid=14054[0m
[[34m2024-07-06T06:15:13.647+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:09:45.508019+00:00: scheduled__2024-07-06T06:09:45.508019+00:00, state:running, queued_at: 2024-07-06 06:14:50.442466+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:15:13.649+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:09:45.508019+00:00, run_id=scheduled__2024-07-06T06:09:45.508019+00:00, run_start_date=2024-07-06 06:14:50.465430+00:00, run_end_date=2024-07-06 06:15:13.648993+00:00, run_duration=23.183563, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:09:45.508019+00:00, data_interval_end=2024-07-06 06:14:45.508019+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:15:13.654+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:14:45.508019+00:00, run_after=2024-07-06T06:19:45.508019+00:00[0m
[[34m2024-07-06T06:17:37.862+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:19:46.394+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:19:45.508019+00:00, run_after=2024-07-06T06:24:45.508019+00:00[0m
[[34m2024-07-06T06:19:46.449+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:14:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:19:46.449+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:19:46.450+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:14:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:19:46.452+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:19:46.452+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:14:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:19:46.453+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:14:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:19:46.457+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:14:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:19:48.083+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:14:45.508019+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:19:52.489+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:14:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:19:54.371+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:14:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:19:54.383+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:14:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:19:52.584592+00:00, run_end_date=2024-07-06 06:19:53.036395+00:00, run_duration=0.451803, state=success, executor_state=success, try_number=1, max_tries=0, job_id=250, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:19:46.450746+00:00, queued_by_job_id=33, pid=14139[0m
[[34m2024-07-06T06:19:54.463+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:14:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:19:54.464+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:19:54.464+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:14:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:19:54.466+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:19:54.467+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:14:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:19:54.467+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:14:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:19:54.471+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:14:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:19:56.034+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:14:45.508019+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:20:00.365+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:14:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:20:03.421+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:14:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:20:03.432+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:14:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:20:00.451113+00:00, run_end_date=2024-07-06 06:20:02.122669+00:00, run_duration=1.671556, state=success, executor_state=success, try_number=1, max_tries=0, job_id=251, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:19:54.464983+00:00, queued_by_job_id=33, pid=14142[0m
[[34m2024-07-06T06:20:07.619+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:14:45.508019+00:00: scheduled__2024-07-06T06:14:45.508019+00:00, state:running, queued_at: 2024-07-06 06:19:46.388045+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:20:07.620+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:14:45.508019+00:00, run_id=scheduled__2024-07-06T06:14:45.508019+00:00, run_start_date=2024-07-06 06:19:46.413875+00:00, run_end_date=2024-07-06 06:20:07.620169+00:00, run_duration=21.206294, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:14:45.508019+00:00, data_interval_end=2024-07-06 06:19:45.508019+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:20:07.624+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:19:45.508019+00:00, run_after=2024-07-06T06:24:45.508019+00:00[0m
[[34m2024-07-06T06:22:37.930+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:24:46.545+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:24:45.508019+00:00, run_after=2024-07-06T06:29:45.508019+00:00[0m
[[34m2024-07-06T06:24:46.599+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:19:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:24:46.599+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:24:46.600+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:19:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:24:46.602+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:24:46.602+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:19:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:24:46.603+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:19:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:24:46.606+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:19:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:24:48.159+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:19:45.508019+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:24:52.639+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:19:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:24:54.551+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:19:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:24:54.565+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:19:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:24:52.731501+00:00, run_end_date=2024-07-06 06:24:53.223935+00:00, run_duration=0.492434, state=success, executor_state=success, try_number=1, max_tries=0, job_id=252, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:24:46.600732+00:00, queued_by_job_id=33, pid=14278[0m
[[34m2024-07-06T06:24:59.399+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:19:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:24:59.400+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:24:59.400+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:19:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:24:59.403+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:24:59.404+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:19:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:24:59.404+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:19:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:24:59.407+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:19:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:25:01.091+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:19:45.508019+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:25:06.287+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:19:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:25:09.544+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:19:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:25:09.571+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:19:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:25:06.392443+00:00, run_end_date=2024-07-06 06:25:08.187583+00:00, run_duration=1.79514, state=success, executor_state=success, try_number=1, max_tries=0, job_id=253, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:24:59.401706+00:00, queued_by_job_id=33, pid=14297[0m
[[34m2024-07-06T06:25:09.654+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:19:45.508019+00:00: scheduled__2024-07-06T06:19:45.508019+00:00, state:running, queued_at: 2024-07-06 06:24:46.537773+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:25:09.654+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:19:45.508019+00:00, run_id=scheduled__2024-07-06T06:19:45.508019+00:00, run_start_date=2024-07-06 06:24:46.561890+00:00, run_end_date=2024-07-06 06:25:09.654736+00:00, run_duration=23.092846, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:19:45.508019+00:00, data_interval_end=2024-07-06 06:24:45.508019+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:25:09.658+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:24:45.508019+00:00, run_after=2024-07-06T06:29:45.508019+00:00[0m
[[34m2024-07-06T06:27:38.004+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:29:46.328+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:29:45.508019+00:00, run_after=2024-07-06T06:34:45.508019+00:00[0m
[[34m2024-07-06T06:29:46.376+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:24:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:29:46.377+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:29:46.377+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:24:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:29:46.379+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:29:46.380+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:24:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:29:46.380+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:24:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:29:46.384+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:24:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:29:47.996+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:24:45.508019+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:29:52.541+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:24:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:29:54.396+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:24:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:29:54.411+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:24:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:29:52.641017+00:00, run_end_date=2024-07-06 06:29:53.086025+00:00, run_duration=0.445008, state=success, executor_state=success, try_number=1, max_tries=0, job_id=254, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:29:46.378103+00:00, queued_by_job_id=33, pid=14363[0m
[[34m2024-07-06T06:29:58.509+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:24:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:29:58.510+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:29:58.510+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:24:45.508019+00:00 [scheduled]>[0m
[[34m2024-07-06T06:29:58.512+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:29:58.513+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:24:45.508019+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:29:58.513+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:24:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:29:58.517+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:24:45.508019+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:30:00.109+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:24:45.508019+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:30:04.456+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:24:45.508019+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:30:07.564+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:24:45.508019+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:30:07.578+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:24:45.508019+00:00, map_index=-1, run_start_date=2024-07-06 06:30:04.549338+00:00, run_end_date=2024-07-06 06:30:06.209884+00:00, run_duration=1.660546, state=success, executor_state=success, try_number=1, max_tries=0, job_id=255, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:29:58.511312+00:00, queued_by_job_id=33, pid=14369[0m
[[34m2024-07-06T06:30:11.778+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:24:45.508019+00:00: scheduled__2024-07-06T06:24:45.508019+00:00, state:running, queued_at: 2024-07-06 06:29:46.321373+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:30:11.779+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:24:45.508019+00:00, run_id=scheduled__2024-07-06T06:24:45.508019+00:00, run_start_date=2024-07-06 06:29:46.341465+00:00, run_end_date=2024-07-06 06:30:11.779429+00:00, run_duration=25.437964, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:24:45.508019+00:00, data_interval_end=2024-07-06 06:29:45.508019+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:30:11.783+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:29:45.508019+00:00, run_after=2024-07-06T06:34:45.508019+00:00[0m
[[34m2024-07-06T06:32:38.060+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:34:48.899+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:34:47.732063+00:00, run_after=2024-07-06T06:39:47.732063+00:00[0m
[[34m2024-07-06T06:34:48.947+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:29:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:34:48.947+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:34:48.947+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:29:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:34:48.949+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:34:48.950+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:29:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:34:48.950+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:29:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:34:48.953+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:29:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:34:50.614+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:29:47.732063+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:34:55.066+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:29:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:34:56.939+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:29:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:34:56.952+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:29:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:34:55.164324+00:00, run_end_date=2024-07-06 06:34:55.630021+00:00, run_duration=0.465697, state=success, executor_state=success, try_number=1, max_tries=0, job_id=256, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:34:48.948390+00:00, queued_by_job_id=33, pid=14430[0m
[[34m2024-07-06T06:34:57.028+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:29:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:34:57.028+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:34:57.029+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:29:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:34:57.030+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:34:57.031+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:29:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:34:57.031+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:29:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:34:57.035+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:29:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:34:58.575+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:29:47.732063+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:35:02.926+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:29:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:35:05.828+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:29:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:35:05.841+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:29:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:35:03.032105+00:00, run_end_date=2024-07-06 06:35:04.514993+00:00, run_duration=1.482888, state=success, executor_state=success, try_number=1, max_tries=0, job_id=257, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:34:57.029680+00:00, queued_by_job_id=33, pid=14434[0m
[[34m2024-07-06T06:35:05.908+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:29:47.732063+00:00: scheduled__2024-07-06T06:29:47.732063+00:00, state:running, queued_at: 2024-07-06 06:34:48.893900+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:35:05.909+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:29:47.732063+00:00, run_id=scheduled__2024-07-06T06:29:47.732063+00:00, run_start_date=2024-07-06 06:34:48.913415+00:00, run_end_date=2024-07-06 06:35:05.909240+00:00, run_duration=16.995825, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:29:47.732063+00:00, data_interval_end=2024-07-06 06:34:47.732063+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:35:05.912+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:34:47.732063+00:00, run_after=2024-07-06T06:39:47.732063+00:00[0m
[[34m2024-07-06T06:37:40.025+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:39:48.559+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:39:47.732063+00:00, run_after=2024-07-06T06:44:47.732063+00:00[0m
[[34m2024-07-06T06:39:48.610+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:34:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:39:48.610+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:39:48.610+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:34:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:39:48.612+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:39:48.613+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:34:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:39:48.613+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:34:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:39:48.618+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:34:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:39:50.305+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:34:47.732063+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:39:54.757+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:34:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:39:56.617+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:34:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:39:56.629+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:34:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:39:54.846358+00:00, run_end_date=2024-07-06 06:39:55.316041+00:00, run_duration=0.469683, state=success, executor_state=success, try_number=1, max_tries=0, job_id=258, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:39:48.611436+00:00, queued_by_job_id=33, pid=14504[0m
[[34m2024-07-06T06:40:00.948+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:34:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:40:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:40:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:34:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:40:00.951+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:40:00.952+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:34:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:40:00.953+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:34:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:40:00.956+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:34:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:40:02.568+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:34:47.732063+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:40:06.936+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:34:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:40:10.076+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:34:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:40:10.083+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:34:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:40:07.030039+00:00, run_end_date=2024-07-06 06:40:08.611940+00:00, run_duration=1.581901, state=success, executor_state=success, try_number=1, max_tries=0, job_id=259, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:40:00.950345+00:00, queued_by_job_id=33, pid=14513[0m
[[34m2024-07-06T06:40:10.148+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:34:47.732063+00:00: scheduled__2024-07-06T06:34:47.732063+00:00, state:running, queued_at: 2024-07-06 06:39:48.553866+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:40:10.148+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:34:47.732063+00:00, run_id=scheduled__2024-07-06T06:34:47.732063+00:00, run_start_date=2024-07-06 06:39:48.576009+00:00, run_end_date=2024-07-06 06:40:10.148494+00:00, run_duration=21.572485, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:34:47.732063+00:00, data_interval_end=2024-07-06 06:39:47.732063+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:40:10.152+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:39:47.732063+00:00, run_after=2024-07-06T06:44:47.732063+00:00[0m
[[34m2024-07-06T06:42:41.113+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:44:48.896+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:44:47.732063+00:00, run_after=2024-07-06T06:49:47.732063+00:00[0m
[[34m2024-07-06T06:44:48.944+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:39:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:44:48.944+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:44:48.944+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:39:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:44:48.946+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:44:48.946+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:39:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:44:48.947+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:39:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:44:48.951+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:39:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:44:50.588+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:39:47.732063+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:44:55.140+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:39:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:44:56.976+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:39:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:44:56.990+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:39:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:44:55.237350+00:00, run_end_date=2024-07-06 06:44:55.674227+00:00, run_duration=0.436877, state=success, executor_state=success, try_number=1, max_tries=0, job_id=260, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:44:48.945169+00:00, queued_by_job_id=33, pid=14581[0m
[[34m2024-07-06T06:45:01.301+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:39:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:45:01.302+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:45:01.302+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:39:47.732063+00:00 [scheduled]>[0m
[[34m2024-07-06T06:45:01.304+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:45:01.305+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:39:47.732063+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:45:01.305+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:39:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:45:01.309+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:39:47.732063+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:45:02.889+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:39:47.732063+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:45:07.338+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:39:47.732063+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:45:10.342+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:39:47.732063+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:45:10.354+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:39:47.732063+00:00, map_index=-1, run_start_date=2024-07-06 06:45:07.434939+00:00, run_end_date=2024-07-06 06:45:09.076207+00:00, run_duration=1.641268, state=success, executor_state=success, try_number=1, max_tries=0, job_id=261, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:45:01.303139+00:00, queued_by_job_id=33, pid=14587[0m
[[34m2024-07-06T06:45:14.552+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:39:47.732063+00:00: scheduled__2024-07-06T06:39:47.732063+00:00, state:running, queued_at: 2024-07-06 06:44:48.890585+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:45:14.552+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:39:47.732063+00:00, run_id=scheduled__2024-07-06T06:39:47.732063+00:00, run_start_date=2024-07-06 06:44:48.909969+00:00, run_end_date=2024-07-06 06:45:14.552758+00:00, run_duration=25.642789, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:39:47.732063+00:00, data_interval_end=2024-07-06 06:44:47.732063+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:45:14.556+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:44:47.732063+00:00, run_after=2024-07-06T06:49:47.732063+00:00[0m
[[34m2024-07-06T06:47:41.170+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:49:51.379+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:49:50.187246+00:00, run_after=2024-07-06T06:54:50.187246+00:00[0m
[[34m2024-07-06T06:49:51.426+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:44:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:49:51.427+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:49:51.427+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:44:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:49:51.429+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:49:51.430+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:44:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:49:51.430+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:44:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:49:51.433+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:44:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:49:53.086+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:44:50.187246+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:49:57.600+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:44:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:49:59.458+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:44:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:49:59.470+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:44:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 06:49:57.692668+00:00, run_end_date=2024-07-06 06:49:58.148915+00:00, run_duration=0.456247, state=success, executor_state=success, try_number=1, max_tries=0, job_id=262, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:49:51.428023+00:00, queued_by_job_id=33, pid=14651[0m
[[34m2024-07-06T06:49:59.549+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:44:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:49:59.549+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:49:59.550+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:44:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:49:59.552+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:49:59.553+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:44:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:49:59.553+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:44:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:49:59.558+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:44:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:50:01.102+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:44:50.187246+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:50:05.518+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:44:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:50:08.644+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:44:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:50:08.658+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:44:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 06:50:05.613351+00:00, run_end_date=2024-07-06 06:50:07.260894+00:00, run_duration=1.647543, state=success, executor_state=success, try_number=1, max_tries=0, job_id=263, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:49:59.550849+00:00, queued_by_job_id=33, pid=14655[0m
[[34m2024-07-06T06:50:12.954+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:44:50.187246+00:00: scheduled__2024-07-06T06:44:50.187246+00:00, state:running, queued_at: 2024-07-06 06:49:51.372832+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:50:12.955+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:44:50.187246+00:00, run_id=scheduled__2024-07-06T06:44:50.187246+00:00, run_start_date=2024-07-06 06:49:51.393023+00:00, run_end_date=2024-07-06 06:50:12.955325+00:00, run_duration=21.562302, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:44:50.187246+00:00, data_interval_end=2024-07-06 06:49:50.187246+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:50:12.959+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:49:50.187246+00:00, run_after=2024-07-06T06:54:50.187246+00:00[0m
[[34m2024-07-06T06:52:43.643+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:54:51.472+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:54:50.187246+00:00, run_after=2024-07-06T06:59:50.187246+00:00[0m
[[34m2024-07-06T06:54:51.527+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:49:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:54:51.527+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:54:51.527+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:49:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:54:51.529+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:54:51.530+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:49:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:54:51.531+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:49:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:54:51.537+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:49:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:54:53.201+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:49:50.187246+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:54:57.746+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:49:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:54:59.691+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:49:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:54:59.705+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:49:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 06:54:57.839140+00:00, run_end_date=2024-07-06 06:54:58.292653+00:00, run_duration=0.453513, state=success, executor_state=success, try_number=1, max_tries=0, job_id=264, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:54:51.528513+00:00, queued_by_job_id=33, pid=14716[0m
[[34m2024-07-06T06:55:04.164+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:49:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:55:04.165+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:55:04.166+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:49:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:55:04.168+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:55:04.169+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:49:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T06:55:04.169+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:49:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:55:04.172+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:49:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:55:05.784+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:49:50.187246+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T06:55:10.195+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:49:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T06:55:13.328+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:49:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T06:55:13.342+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:49:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 06:55:10.294340+00:00, run_end_date=2024-07-06 06:55:11.978659+00:00, run_duration=1.684319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=265, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 06:55:04.166988+00:00, queued_by_job_id=33, pid=14725[0m
[[34m2024-07-06T06:55:13.425+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:49:50.187246+00:00: scheduled__2024-07-06T06:49:50.187246+00:00, state:running, queued_at: 2024-07-06 06:54:51.463052+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T06:55:13.425+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:49:50.187246+00:00, run_id=scheduled__2024-07-06T06:49:50.187246+00:00, run_start_date=2024-07-06 06:54:51.489488+00:00, run_end_date=2024-07-06 06:55:13.425655+00:00, run_duration=21.936167, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:49:50.187246+00:00, data_interval_end=2024-07-06 06:54:50.187246+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T06:55:13.429+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:54:50.187246+00:00, run_after=2024-07-06T06:59:50.187246+00:00[0m
[[34m2024-07-06T06:57:45.797+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T06:59:51.965+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:59:50.187246+00:00, run_after=2024-07-06T07:04:50.187246+00:00[0m
[[34m2024-07-06T06:59:52.014+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:54:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:59:52.015+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T06:59:52.015+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:54:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T06:59:52.018+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T06:59:52.019+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:54:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T06:59:52.019+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:54:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:59:52.022+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:54:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T06:59:53.703+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:54:50.187246+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T06:59:58.261+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:54:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:00:00.289+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:54:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:00:00.303+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:54:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 06:59:58.357197+00:00, run_end_date=2024-07-06 06:59:58.835575+00:00, run_duration=0.478378, state=success, executor_state=success, try_number=1, max_tries=0, job_id=266, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 06:59:52.016531+00:00, queued_by_job_id=33, pid=14795[0m
[[34m2024-07-06T07:00:05.139+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:54:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:00:05.141+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:00:05.141+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:54:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:00:05.144+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:00:05.145+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:54:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:00:05.146+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:54:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:00:05.151+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:54:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:00:07.047+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:54:50.187246+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:00:11.765+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:54:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:00:15.073+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:54:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:00:15.090+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:54:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 07:00:11.904431+00:00, run_end_date=2024-07-06 07:00:13.625475+00:00, run_duration=1.721044, state=success, executor_state=success, try_number=1, max_tries=0, job_id=267, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:00:05.142748+00:00, queued_by_job_id=33, pid=14801[0m
[[34m2024-07-06T07:00:19.326+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:54:50.187246+00:00: scheduled__2024-07-06T06:54:50.187246+00:00, state:running, queued_at: 2024-07-06 06:59:51.960135+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:00:19.327+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:54:50.187246+00:00, run_id=scheduled__2024-07-06T06:54:50.187246+00:00, run_start_date=2024-07-06 06:59:51.981337+00:00, run_end_date=2024-07-06 07:00:19.327085+00:00, run_duration=27.345748, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:54:50.187246+00:00, data_interval_end=2024-07-06 06:59:50.187246+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:00:19.330+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T06:59:50.187246+00:00, run_after=2024-07-06T07:04:50.187246+00:00[0m
[[34m2024-07-06T07:02:45.848+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:04:51.965+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:04:50.187246+00:00, run_after=2024-07-06T07:09:50.187246+00:00[0m
[[34m2024-07-06T07:04:52.016+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:59:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:04:52.016+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:04:52.017+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:59:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:04:52.019+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:04:52.020+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:59:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:04:52.020+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:59:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:04:52.023+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T06:59:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:04:53.754+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:59:50.187246+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:04:58.183+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T06:59:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:05:00.072+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T06:59:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:05:00.085+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T06:59:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 07:04:58.270873+00:00, run_end_date=2024-07-06 07:04:58.704785+00:00, run_duration=0.433912, state=success, executor_state=success, try_number=1, max_tries=0, job_id=268, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:04:52.017878+00:00, queued_by_job_id=33, pid=14847[0m
[[34m2024-07-06T07:05:04.504+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:59:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:05:04.506+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:05:04.506+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:59:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:05:04.508+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:05:04.508+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:59:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:05:04.509+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:59:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:05:04.512+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T06:59:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:05:06.077+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T06:59:50.187246+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:05:10.343+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T06:59:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:05:13.413+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T06:59:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:05:13.418+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T06:59:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 07:05:10.434434+00:00, run_end_date=2024-07-06 07:05:12.129025+00:00, run_duration=1.694591, state=success, executor_state=success, try_number=1, max_tries=0, job_id=269, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:05:04.506841+00:00, queued_by_job_id=33, pid=14855[0m
[[34m2024-07-06T07:05:13.479+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 06:59:50.187246+00:00: scheduled__2024-07-06T06:59:50.187246+00:00, state:running, queued_at: 2024-07-06 07:04:51.959639+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:05:13.480+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 06:59:50.187246+00:00, run_id=scheduled__2024-07-06T06:59:50.187246+00:00, run_start_date=2024-07-06 07:04:51.980359+00:00, run_end_date=2024-07-06 07:05:13.479843+00:00, run_duration=21.499484, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 06:59:50.187246+00:00, data_interval_end=2024-07-06 07:04:50.187246+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:05:13.483+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:04:50.187246+00:00, run_after=2024-07-06T07:09:50.187246+00:00[0m
[[34m2024-07-06T07:07:45.902+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:09:51.573+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:09:50.187246+00:00, run_after=2024-07-06T07:14:50.187246+00:00[0m
[[34m2024-07-06T07:09:51.621+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:04:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:09:51.621+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:09:51.622+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:04:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:09:51.623+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:09:51.624+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:04:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:09:51.624+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:04:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:09:51.628+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:04:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:09:53.235+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:04:50.187246+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:09:57.645+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:04:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:09:59.471+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:04:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:09:59.483+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:04:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 07:09:57.732748+00:00, run_end_date=2024-07-06 07:09:58.175339+00:00, run_duration=0.442591, state=success, executor_state=success, try_number=1, max_tries=0, job_id=270, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:09:51.622630+00:00, queued_by_job_id=33, pid=14913[0m
[[34m2024-07-06T07:10:03.584+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:04:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:10:03.584+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:10:03.584+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:04:50.187246+00:00 [scheduled]>[0m
[[34m2024-07-06T07:10:03.586+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:10:03.587+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:04:50.187246+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:10:03.587+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:04:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:10:03.591+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:04:50.187246+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:10:05.174+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:04:50.187246+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:10:09.603+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:04:50.187246+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:10:12.636+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:04:50.187246+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:10:12.650+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:04:50.187246+00:00, map_index=-1, run_start_date=2024-07-06 07:10:09.696274+00:00, run_end_date=2024-07-06 07:10:11.323327+00:00, run_duration=1.627053, state=success, executor_state=success, try_number=1, max_tries=0, job_id=271, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:10:03.585334+00:00, queued_by_job_id=33, pid=14919[0m
[[34m2024-07-06T07:10:16.846+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:04:50.187246+00:00: scheduled__2024-07-06T07:04:50.187246+00:00, state:running, queued_at: 2024-07-06 07:09:51.568307+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:10:16.847+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:04:50.187246+00:00, run_id=scheduled__2024-07-06T07:04:50.187246+00:00, run_start_date=2024-07-06 07:09:51.587660+00:00, run_end_date=2024-07-06 07:10:16.847324+00:00, run_duration=25.259664, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:04:50.187246+00:00, data_interval_end=2024-07-06 07:09:50.187246+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:10:16.850+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:09:50.187246+00:00, run_after=2024-07-06T07:14:50.187246+00:00[0m
[[34m2024-07-06T07:12:45.949+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:14:53.734+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:14:52.512503+00:00, run_after=2024-07-06T07:19:52.512503+00:00[0m
[[34m2024-07-06T07:14:53.782+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:09:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:14:53.783+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:14:53.783+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:09:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:14:53.785+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:14:53.786+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:09:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:14:53.786+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:09:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:14:53.789+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:09:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:14:55.435+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:09:52.512503+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:14:59.800+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:09:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:15:01.630+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:09:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:15:01.637+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:09:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:14:59.899818+00:00, run_end_date=2024-07-06 07:15:00.357567+00:00, run_duration=0.457749, state=success, executor_state=success, try_number=1, max_tries=0, job_id=272, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:14:53.784009+00:00, queued_by_job_id=33, pid=14976[0m
[[34m2024-07-06T07:15:01.699+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:09:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:15:01.700+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:15:01.700+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:09:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:15:01.702+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:15:01.702+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:09:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:15:01.702+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:09:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:15:01.705+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:09:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:15:03.227+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:09:52.512503+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:15:07.664+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:09:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:15:10.630+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:09:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:15:10.644+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:09:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:15:07.756690+00:00, run_end_date=2024-07-06 07:15:09.214429+00:00, run_duration=1.457739, state=success, executor_state=success, try_number=1, max_tries=0, job_id=273, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:15:01.701024+00:00, queued_by_job_id=33, pid=14980[0m
[[34m2024-07-06T07:15:10.716+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:09:52.512503+00:00: scheduled__2024-07-06T07:09:52.512503+00:00, state:running, queued_at: 2024-07-06 07:14:53.728496+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:15:10.717+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:09:52.512503+00:00, run_id=scheduled__2024-07-06T07:09:52.512503+00:00, run_start_date=2024-07-06 07:14:53.748307+00:00, run_end_date=2024-07-06 07:15:10.717350+00:00, run_duration=16.969043, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:09:52.512503+00:00, data_interval_end=2024-07-06 07:14:52.512503+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:15:10.721+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:14:52.512503+00:00, run_after=2024-07-06T07:19:52.512503+00:00[0m
[[34m2024-07-06T07:17:47.123+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:19:54.612+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:19:52.512503+00:00, run_after=2024-07-06T07:24:52.512503+00:00[0m
[[34m2024-07-06T07:19:54.665+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:14:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:19:54.665+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:19:54.666+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:14:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:19:54.667+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:19:54.668+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:14:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:19:54.669+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:14:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:19:54.672+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:14:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:19:56.273+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:14:52.512503+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:20:00.643+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:14:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:20:02.427+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:14:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:20:02.433+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:14:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:20:00.732723+00:00, run_end_date=2024-07-06 07:20:01.169847+00:00, run_duration=0.437124, state=success, executor_state=success, try_number=1, max_tries=0, job_id=274, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:19:54.666595+00:00, queued_by_job_id=33, pid=15039[0m
[[34m2024-07-06T07:20:06.757+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:14:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:20:06.760+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:20:06.761+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:14:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:20:06.763+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:20:06.764+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:14:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:20:06.764+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:14:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:20:06.768+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:14:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:20:08.333+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:14:52.512503+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:20:12.671+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:14:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:20:15.727+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:14:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:20:15.733+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:14:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:20:12.771117+00:00, run_end_date=2024-07-06 07:20:14.398144+00:00, run_duration=1.627027, state=success, executor_state=success, try_number=1, max_tries=0, job_id=275, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:20:06.761993+00:00, queued_by_job_id=33, pid=15045[0m
[[34m2024-07-06T07:20:15.800+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:14:52.512503+00:00: scheduled__2024-07-06T07:14:52.512503+00:00, state:running, queued_at: 2024-07-06 07:19:54.604699+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:20:15.801+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:14:52.512503+00:00, run_id=scheduled__2024-07-06T07:14:52.512503+00:00, run_start_date=2024-07-06 07:19:54.629250+00:00, run_end_date=2024-07-06 07:20:15.800878+00:00, run_duration=21.171628, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:14:52.512503+00:00, data_interval_end=2024-07-06 07:19:52.512503+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:20:15.804+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:19:52.512503+00:00, run_after=2024-07-06T07:24:52.512503+00:00[0m
[[34m2024-07-06T07:22:47.177+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:24:53.607+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:24:52.512503+00:00, run_after=2024-07-06T07:29:52.512503+00:00[0m
[[34m2024-07-06T07:24:53.656+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:19:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:24:53.656+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:24:53.656+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:19:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:24:53.658+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:24:53.659+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:19:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:24:53.660+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:19:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:24:53.665+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:19:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:24:55.231+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:19:52.512503+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:24:59.671+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:19:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:25:01.676+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:19:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:25:01.689+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:19:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:24:59.766639+00:00, run_end_date=2024-07-06 07:25:00.281887+00:00, run_duration=0.515248, state=success, executor_state=success, try_number=1, max_tries=0, job_id=276, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:24:53.657534+00:00, queued_by_job_id=33, pid=15108[0m
[[34m2024-07-06T07:25:05.899+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:19:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:25:05.899+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:25:05.900+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:19:52.512503+00:00 [scheduled]>[0m
[[34m2024-07-06T07:25:05.901+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:25:05.902+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:19:52.512503+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:25:05.902+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:19:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:25:05.906+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:19:52.512503+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:25:07.597+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:19:52.512503+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:25:12.096+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:19:52.512503+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:25:15.092+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:19:52.512503+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:25:15.104+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:19:52.512503+00:00, map_index=-1, run_start_date=2024-07-06 07:25:12.201053+00:00, run_end_date=2024-07-06 07:25:13.777774+00:00, run_duration=1.576721, state=success, executor_state=success, try_number=1, max_tries=0, job_id=277, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:25:05.900566+00:00, queued_by_job_id=33, pid=15114[0m
[[34m2024-07-06T07:25:19.200+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:19:52.512503+00:00: scheduled__2024-07-06T07:19:52.512503+00:00, state:running, queued_at: 2024-07-06 07:24:53.601452+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:25:19.201+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:19:52.512503+00:00, run_id=scheduled__2024-07-06T07:19:52.512503+00:00, run_start_date=2024-07-06 07:24:53.623437+00:00, run_end_date=2024-07-06 07:25:19.201466+00:00, run_duration=25.578029, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:19:52.512503+00:00, data_interval_end=2024-07-06 07:24:52.512503+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:25:19.204+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:24:52.512503+00:00, run_after=2024-07-06T07:29:52.512503+00:00[0m
[[34m2024-07-06T07:27:47.227+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:29:55.943+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:29:54.730687+00:00, run_after=2024-07-06T07:34:54.730687+00:00[0m
[[34m2024-07-06T07:29:55.997+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:24:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:29:55.997+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:29:55.998+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:24:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:29:56.000+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:29:56.001+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:24:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:29:56.001+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:24:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:29:56.006+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:24:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:29:57.836+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:24:54.730687+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:30:02.449+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:24:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:30:04.352+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:24:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:30:04.364+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:24:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:30:02.559327+00:00, run_end_date=2024-07-06 07:30:02.997890+00:00, run_duration=0.438563, state=success, executor_state=success, try_number=1, max_tries=0, job_id=278, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:29:55.998899+00:00, queued_by_job_id=33, pid=15181[0m
[[34m2024-07-06T07:30:04.456+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:24:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:30:04.457+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:30:04.457+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:24:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:30:04.459+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:30:04.459+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:24:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:30:04.460+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:24:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:30:04.463+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:24:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:30:06.111+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:24:54.730687+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:30:10.776+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:24:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:30:13.910+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:24:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:30:13.922+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:24:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:30:10.872768+00:00, run_end_date=2024-07-06 07:30:12.497978+00:00, run_duration=1.62521, state=success, executor_state=success, try_number=1, max_tries=0, job_id=279, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:30:04.457895+00:00, queued_by_job_id=33, pid=15185[0m
[[34m2024-07-06T07:30:18.006+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:24:54.730687+00:00: scheduled__2024-07-06T07:24:54.730687+00:00, state:running, queued_at: 2024-07-06 07:29:55.936650+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:30:18.007+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:24:54.730687+00:00, run_id=scheduled__2024-07-06T07:24:54.730687+00:00, run_start_date=2024-07-06 07:29:55.960374+00:00, run_end_date=2024-07-06 07:30:18.006966+00:00, run_duration=22.046592, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:24:54.730687+00:00, data_interval_end=2024-07-06 07:29:54.730687+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:30:18.010+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:29:54.730687+00:00, run_after=2024-07-06T07:34:54.730687+00:00[0m
[[34m2024-07-06T07:32:47.310+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:34:55.662+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:34:54.730687+00:00, run_after=2024-07-06T07:39:54.730687+00:00[0m
[[34m2024-07-06T07:34:55.708+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:29:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:34:55.708+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:34:55.709+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:29:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:34:55.710+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:34:55.717+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:29:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:34:55.717+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:29:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:34:55.721+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:29:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:34:57.300+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:29:54.730687+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:35:02.026+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:29:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:35:04.056+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:29:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:35:04.070+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:29:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:35:02.126735+00:00, run_end_date=2024-07-06 07:35:02.536868+00:00, run_duration=0.410133, state=success, executor_state=success, try_number=1, max_tries=0, job_id=280, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:34:55.709644+00:00, queued_by_job_id=33, pid=15260[0m
[[34m2024-07-06T07:35:08.310+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:29:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:35:08.310+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:35:08.310+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:29:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:35:08.312+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:35:08.313+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:29:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:35:08.313+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:29:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:35:08.318+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:29:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:35:09.870+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:29:54.730687+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:35:14.322+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:29:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:35:17.721+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:29:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:35:17.734+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:29:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:35:14.413095+00:00, run_end_date=2024-07-06 07:35:16.339867+00:00, run_duration=1.926772, state=success, executor_state=success, try_number=1, max_tries=0, job_id=281, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:35:08.311409+00:00, queued_by_job_id=33, pid=15268[0m
[[34m2024-07-06T07:35:17.809+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:29:54.730687+00:00: scheduled__2024-07-06T07:29:54.730687+00:00, state:running, queued_at: 2024-07-06 07:34:55.655779+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:35:17.809+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:29:54.730687+00:00, run_id=scheduled__2024-07-06T07:29:54.730687+00:00, run_start_date=2024-07-06 07:34:55.676465+00:00, run_end_date=2024-07-06 07:35:17.809610+00:00, run_duration=22.133145, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:29:54.730687+00:00, data_interval_end=2024-07-06 07:34:54.730687+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:35:17.812+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:34:54.730687+00:00, run_after=2024-07-06T07:39:54.730687+00:00[0m
[[34m2024-07-06T07:37:48.603+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:39:56.043+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:39:54.730687+00:00, run_after=2024-07-06T07:44:54.730687+00:00[0m
[[34m2024-07-06T07:39:56.106+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:34:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:39:56.106+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:39:56.107+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:34:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:39:56.108+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:39:56.109+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:34:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:39:56.109+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:34:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:39:56.113+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:34:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:39:57.764+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:34:54.730687+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:40:02.303+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:34:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:40:04.310+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:34:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:40:04.323+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:34:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:40:02.398765+00:00, run_end_date=2024-07-06 07:40:02.855732+00:00, run_duration=0.456967, state=success, executor_state=success, try_number=1, max_tries=0, job_id=282, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:39:56.107511+00:00, queued_by_job_id=33, pid=15370[0m
[[34m2024-07-06T07:40:09.049+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:34:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:40:09.051+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:40:09.051+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:34:54.730687+00:00 [scheduled]>[0m
[[34m2024-07-06T07:40:09.053+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:40:09.054+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:34:54.730687+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:40:09.054+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:34:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:40:09.059+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:34:54.730687+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:40:10.643+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:34:54.730687+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:40:15.031+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:34:54.730687+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:40:17.900+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:34:54.730687+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:40:17.911+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:34:54.730687+00:00, map_index=-1, run_start_date=2024-07-06 07:40:15.123803+00:00, run_end_date=2024-07-06 07:40:16.593338+00:00, run_duration=1.469535, state=success, executor_state=success, try_number=1, max_tries=0, job_id=283, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:40:09.051973+00:00, queued_by_job_id=33, pid=15377[0m
[[34m2024-07-06T07:40:22.629+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:34:54.730687+00:00: scheduled__2024-07-06T07:34:54.730687+00:00, state:running, queued_at: 2024-07-06 07:39:56.037599+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:40:22.630+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:34:54.730687+00:00, run_id=scheduled__2024-07-06T07:34:54.730687+00:00, run_start_date=2024-07-06 07:39:56.058380+00:00, run_end_date=2024-07-06 07:40:22.630607+00:00, run_duration=26.572227, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:34:54.730687+00:00, data_interval_end=2024-07-06 07:39:54.730687+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:40:22.634+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:39:54.730687+00:00, run_after=2024-07-06T07:44:54.730687+00:00[0m
[[34m2024-07-06T07:42:48.650+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:44:59.620+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:44:58.430441+00:00, run_after=2024-07-06T07:49:58.430441+00:00[0m
[[34m2024-07-06T07:44:59.677+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:39:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:44:59.678+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:44:59.678+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:39:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:44:59.680+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:44:59.680+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:39:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:44:59.680+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:39:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:44:59.684+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:39:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:45:01.292+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:39:58.430441+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:45:05.605+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:39:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:45:07.453+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:39:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:45:07.466+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:39:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:45:05.698508+00:00, run_end_date=2024-07-06 07:45:06.117778+00:00, run_duration=0.41927, state=success, executor_state=success, try_number=1, max_tries=0, job_id=284, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:44:59.678849+00:00, queued_by_job_id=33, pid=15479[0m
[[34m2024-07-06T07:45:07.545+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:39:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:45:07.545+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:45:07.545+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:39:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:45:07.547+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:45:07.548+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:39:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:45:07.548+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:39:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:45:07.551+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:39:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:45:09.142+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:39:58.430441+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:45:13.533+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:39:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:45:16.370+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:39:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:45:16.384+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:39:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:45:13.624392+00:00, run_end_date=2024-07-06 07:45:15.091124+00:00, run_duration=1.466732, state=success, executor_state=success, try_number=1, max_tries=0, job_id=285, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:45:07.546435+00:00, queued_by_job_id=33, pid=15490[0m
[[34m2024-07-06T07:45:16.454+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:39:58.430441+00:00: scheduled__2024-07-06T07:39:58.430441+00:00, state:running, queued_at: 2024-07-06 07:44:59.615047+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:45:16.455+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:39:58.430441+00:00, run_id=scheduled__2024-07-06T07:39:58.430441+00:00, run_start_date=2024-07-06 07:44:59.633648+00:00, run_end_date=2024-07-06 07:45:16.455400+00:00, run_duration=16.821752, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:39:58.430441+00:00, data_interval_end=2024-07-06 07:44:58.430441+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:45:16.459+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:44:58.430441+00:00, run_after=2024-07-06T07:49:58.430441+00:00[0m
[[34m2024-07-06T07:47:51.001+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:50:00.052+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:49:58.430441+00:00, run_after=2024-07-06T07:54:58.430441+00:00[0m
[[34m2024-07-06T07:50:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:44:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:50:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:50:00.116+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:44:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:50:00.118+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:50:00.119+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:44:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:50:00.119+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:44:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:50:00.122+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:44:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:50:01.667+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:44:58.430441+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:50:05.972+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:44:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:50:08.079+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:44:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:50:08.095+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:44:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:50:06.076159+00:00, run_end_date=2024-07-06 07:50:06.514622+00:00, run_duration=0.438463, state=success, executor_state=success, try_number=1, max_tries=0, job_id=286, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:50:00.117246+00:00, queued_by_job_id=33, pid=15584[0m
[[34m2024-07-06T07:50:12.604+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:44:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:50:12.605+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:50:12.605+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:44:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:50:12.607+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:50:12.608+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:44:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:50:12.608+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:44:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:50:12.612+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:44:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:50:14.272+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:44:58.430441+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:50:18.695+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:44:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:50:21.824+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:44:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:50:21.836+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:44:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:50:18.792395+00:00, run_end_date=2024-07-06 07:50:20.479979+00:00, run_duration=1.687584, state=success, executor_state=success, try_number=1, max_tries=0, job_id=287, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:50:12.605984+00:00, queued_by_job_id=33, pid=15591[0m
[[34m2024-07-06T07:50:21.906+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:44:58.430441+00:00: scheduled__2024-07-06T07:44:58.430441+00:00, state:running, queued_at: 2024-07-06 07:50:00.045983+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:50:21.906+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:44:58.430441+00:00, run_id=scheduled__2024-07-06T07:44:58.430441+00:00, run_start_date=2024-07-06 07:50:00.080448+00:00, run_end_date=2024-07-06 07:50:21.906493+00:00, run_duration=21.826045, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:44:58.430441+00:00, data_interval_end=2024-07-06 07:49:58.430441+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:50:21.909+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:49:58.430441+00:00, run_after=2024-07-06T07:54:58.430441+00:00[0m
[[34m2024-07-06T07:52:51.031+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T07:54:59.214+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:54:58.430441+00:00, run_after=2024-07-06T07:59:58.430441+00:00[0m
[[34m2024-07-06T07:54:59.268+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:49:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:54:59.268+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:54:59.268+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:49:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:54:59.270+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:54:59.271+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:49:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T07:54:59.271+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:49:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:54:59.274+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:49:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:55:00.975+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:49:58.430441+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T07:55:05.421+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:49:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:55:07.257+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:49:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:55:07.270+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:49:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:55:05.514780+00:00, run_end_date=2024-07-06 07:55:05.931007+00:00, run_duration=0.416227, state=success, executor_state=success, try_number=1, max_tries=0, job_id=288, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 07:54:59.269518+00:00, queued_by_job_id=33, pid=15643[0m
[[34m2024-07-06T07:55:11.168+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:49:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:55:11.169+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T07:55:11.169+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:49:58.430441+00:00 [scheduled]>[0m
[[34m2024-07-06T07:55:11.171+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T07:55:11.172+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:49:58.430441+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T07:55:11.172+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:49:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:55:11.176+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:49:58.430441+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T07:55:12.748+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:49:58.430441+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T07:55:17.003+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:49:58.430441+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T07:55:19.995+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:49:58.430441+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T07:55:20.008+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:49:58.430441+00:00, map_index=-1, run_start_date=2024-07-06 07:55:17.094267+00:00, run_end_date=2024-07-06 07:55:18.691472+00:00, run_duration=1.597205, state=success, executor_state=success, try_number=1, max_tries=0, job_id=289, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 07:55:11.170117+00:00, queued_by_job_id=33, pid=15648[0m
[[34m2024-07-06T07:55:23.993+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:49:58.430441+00:00: scheduled__2024-07-06T07:49:58.430441+00:00, state:running, queued_at: 2024-07-06 07:54:59.209971+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T07:55:23.994+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:49:58.430441+00:00, run_id=scheduled__2024-07-06T07:49:58.430441+00:00, run_start_date=2024-07-06 07:54:59.234886+00:00, run_end_date=2024-07-06 07:55:23.994054+00:00, run_duration=24.759168, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:49:58.430441+00:00, data_interval_end=2024-07-06 07:54:58.430441+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T07:55:23.997+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:54:58.430441+00:00, run_after=2024-07-06T07:59:58.430441+00:00[0m
[[34m2024-07-06T07:57:51.083+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:00:00.307+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:59:59.065050+00:00, run_after=2024-07-06T08:04:59.065050+00:00[0m
[[34m2024-07-06T08:00:00.359+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:54:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:00:00.359+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:00:00.360+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:54:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:00:00.362+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:00:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:54:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:00:00.363+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:54:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:00:00.367+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:54:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:00:02.078+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:54:59.065050+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:00:06.584+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:54:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:00:08.594+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:54:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:00:08.601+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:54:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:00:06.684155+00:00, run_end_date=2024-07-06 08:00:07.139793+00:00, run_duration=0.455638, state=success, executor_state=success, try_number=1, max_tries=0, job_id=290, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:00:00.360695+00:00, queued_by_job_id=33, pid=15695[0m
[[34m2024-07-06T08:00:08.671+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:54:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:00:08.671+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:00:08.672+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:54:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:00:08.674+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:00:08.675+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:54:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:00:08.675+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:54:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:00:08.682+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:54:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:00:10.339+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:54:59.065050+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:00:14.742+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:54:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:00:18.100+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:54:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:00:18.114+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:54:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:00:14.845203+00:00, run_end_date=2024-07-06 08:00:16.527903+00:00, run_duration=1.6827, state=success, executor_state=success, try_number=1, max_tries=0, job_id=291, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:00:08.672870+00:00, queued_by_job_id=33, pid=15698[0m
[[34m2024-07-06T08:00:22.441+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:54:59.065050+00:00: scheduled__2024-07-06T07:54:59.065050+00:00, state:running, queued_at: 2024-07-06 08:00:00.301444+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:00:22.442+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:54:59.065050+00:00, run_id=scheduled__2024-07-06T07:54:59.065050+00:00, run_start_date=2024-07-06 08:00:00.322676+00:00, run_end_date=2024-07-06 08:00:22.442056+00:00, run_duration=22.11938, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:54:59.065050+00:00, data_interval_end=2024-07-06 07:59:59.065050+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:00:22.446+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T07:59:59.065050+00:00, run_after=2024-07-06T08:04:59.065050+00:00[0m
[[34m2024-07-06T08:02:51.550+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:05:00.235+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:04:59.065050+00:00, run_after=2024-07-06T08:09:59.065050+00:00[0m
[[34m2024-07-06T08:05:00.285+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:59:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:05:00.285+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:05:00.286+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:59:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:05:00.287+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:05:00.288+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:59:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:05:00.288+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:59:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:05:00.292+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T07:59:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:05:01.959+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:59:59.065050+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:05:06.523+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T07:59:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:05:08.314+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T07:59:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:05:08.325+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T07:59:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:05:06.620040+00:00, run_end_date=2024-07-06 08:05:07.047828+00:00, run_duration=0.427788, state=success, executor_state=success, try_number=1, max_tries=0, job_id=292, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:05:00.286674+00:00, queued_by_job_id=33, pid=15804[0m
[[34m2024-07-06T08:05:12.533+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:59:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:05:12.533+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:05:12.533+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:59:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:05:12.535+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:05:12.535+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:59:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:05:12.536+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:59:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:05:12.540+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T07:59:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:05:14.105+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T07:59:59.065050+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:05:18.447+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T07:59:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:05:21.265+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T07:59:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:05:21.278+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T07:59:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:05:18.540105+00:00, run_end_date=2024-07-06 08:05:19.967092+00:00, run_duration=1.426987, state=success, executor_state=success, try_number=1, max_tries=0, job_id=293, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:05:12.534196+00:00, queued_by_job_id=33, pid=15809[0m
[[34m2024-07-06T08:05:21.346+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 07:59:59.065050+00:00: scheduled__2024-07-06T07:59:59.065050+00:00, state:running, queued_at: 2024-07-06 08:05:00.230532+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:05:21.347+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 07:59:59.065050+00:00, run_id=scheduled__2024-07-06T07:59:59.065050+00:00, run_start_date=2024-07-06 08:05:00.252026+00:00, run_end_date=2024-07-06 08:05:21.347201+00:00, run_duration=21.095175, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 07:59:59.065050+00:00, data_interval_end=2024-07-06 08:04:59.065050+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:05:21.350+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:04:59.065050+00:00, run_after=2024-07-06T08:09:59.065050+00:00[0m
[[34m2024-07-06T08:07:52.065+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:10:00.297+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:09:59.065050+00:00, run_after=2024-07-06T08:14:59.065050+00:00[0m
[[34m2024-07-06T08:10:00.344+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:04:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:10:00.344+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:10:00.345+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:04:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:10:00.346+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:10:00.347+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:04:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:10:00.347+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:04:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:10:00.350+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:04:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:10:01.887+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:04:59.065050+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:10:06.171+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:04:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:10:08.231+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:04:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:10:08.246+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:04:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:10:06.274532+00:00, run_end_date=2024-07-06 08:10:06.686305+00:00, run_duration=0.411773, state=success, executor_state=success, try_number=1, max_tries=0, job_id=294, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:10:00.345646+00:00, queued_by_job_id=33, pid=15862[0m
[[34m2024-07-06T08:10:12.579+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:04:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:10:12.580+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:10:12.580+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:04:59.065050+00:00 [scheduled]>[0m
[[34m2024-07-06T08:10:12.582+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:10:12.583+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:04:59.065050+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:10:12.583+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:04:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:10:12.587+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:04:59.065050+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:10:14.148+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:04:59.065050+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:10:18.519+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:04:59.065050+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:10:21.732+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:04:59.065050+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:10:21.746+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:04:59.065050+00:00, map_index=-1, run_start_date=2024-07-06 08:10:18.612098+00:00, run_end_date=2024-07-06 08:10:20.243705+00:00, run_duration=1.631607, state=success, executor_state=success, try_number=1, max_tries=0, job_id=295, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:10:12.581058+00:00, queued_by_job_id=33, pid=15867[0m
[[34m2024-07-06T08:10:26.059+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:04:59.065050+00:00: scheduled__2024-07-06T08:04:59.065050+00:00, state:running, queued_at: 2024-07-06 08:10:00.290921+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:10:26.060+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:04:59.065050+00:00, run_id=scheduled__2024-07-06T08:04:59.065050+00:00, run_start_date=2024-07-06 08:10:00.312601+00:00, run_end_date=2024-07-06 08:10:26.060408+00:00, run_duration=25.747807, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:04:59.065050+00:00, data_interval_end=2024-07-06 08:09:59.065050+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:10:26.065+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:09:59.065050+00:00, run_after=2024-07-06T08:14:59.065050+00:00[0m
[[34m2024-07-06T08:12:52.110+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:15:02.196+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:15:01.024633+00:00, run_after=2024-07-06T08:20:01.024633+00:00[0m
[[34m2024-07-06T08:15:02.246+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:10:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:15:02.246+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:15:02.246+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:10:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:15:02.248+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:15:02.248+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:10:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:15:02.249+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:10:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:15:02.252+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:10:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:15:03.914+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:10:01.024633+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:15:08.402+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:10:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:15:10.445+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:10:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:15:10.457+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:10:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:15:08.505998+00:00, run_end_date=2024-07-06 08:15:08.993641+00:00, run_duration=0.487643, state=success, executor_state=success, try_number=1, max_tries=0, job_id=296, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:15:02.247089+00:00, queued_by_job_id=33, pid=15982[0m
[[34m2024-07-06T08:15:10.535+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:10:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:15:10.535+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:15:10.535+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:10:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:15:10.537+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:15:10.538+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:10:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:15:10.538+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:10:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:15:10.542+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:10:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:15:12.157+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:10:01.024633+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:15:16.407+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:10:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:15:19.489+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:10:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:15:19.501+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:10:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:15:16.495651+00:00, run_end_date=2024-07-06 08:15:18.139496+00:00, run_duration=1.643845, state=success, executor_state=success, try_number=1, max_tries=0, job_id=297, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:15:10.536573+00:00, queued_by_job_id=33, pid=15985[0m
[[34m2024-07-06T08:15:23.624+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:10:01.024633+00:00: scheduled__2024-07-06T08:10:01.024633+00:00, state:running, queued_at: 2024-07-06 08:15:02.189760+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:15:23.626+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:10:01.024633+00:00, run_id=scheduled__2024-07-06T08:10:01.024633+00:00, run_start_date=2024-07-06 08:15:02.209738+00:00, run_end_date=2024-07-06 08:15:23.625808+00:00, run_duration=21.41607, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:10:01.024633+00:00, data_interval_end=2024-07-06 08:15:01.024633+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:15:23.629+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:15:01.024633+00:00, run_after=2024-07-06T08:20:01.024633+00:00[0m
[[34m2024-07-06T08:17:53.864+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:20:02.798+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:20:01.024633+00:00, run_after=2024-07-06T08:25:01.024633+00:00[0m
[[34m2024-07-06T08:20:02.853+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:15:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:20:02.854+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:20:02.854+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:15:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:20:02.856+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:20:02.857+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:15:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:20:02.857+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:15:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:20:02.861+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:15:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:20:04.558+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:15:01.024633+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:20:09.317+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:15:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:20:11.352+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:15:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:20:11.365+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:15:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:20:09.416766+00:00, run_end_date=2024-07-06 08:20:09.875839+00:00, run_duration=0.459073, state=success, executor_state=success, try_number=1, max_tries=0, job_id=298, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:20:02.854963+00:00, queued_by_job_id=33, pid=16104[0m
[[34m2024-07-06T08:20:15.680+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:15:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:20:15.681+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:20:15.681+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:15:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:20:15.683+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:20:15.684+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:15:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:20:15.684+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:15:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:20:15.689+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:15:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:20:17.344+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:15:01.024633+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:20:22.072+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:15:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:20:25.320+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:15:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:20:25.328+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:15:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:20:22.178835+00:00, run_end_date=2024-07-06 08:20:23.801107+00:00, run_duration=1.622272, state=success, executor_state=success, try_number=1, max_tries=0, job_id=299, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:20:15.682308+00:00, queued_by_job_id=33, pid=16114[0m
[[34m2024-07-06T08:20:25.398+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:15:01.024633+00:00: scheduled__2024-07-06T08:15:01.024633+00:00, state:running, queued_at: 2024-07-06 08:20:02.785176+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:20:25.399+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:15:01.024633+00:00, run_id=scheduled__2024-07-06T08:15:01.024633+00:00, run_start_date=2024-07-06 08:20:02.819495+00:00, run_end_date=2024-07-06 08:20:25.399310+00:00, run_duration=22.579815, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:15:01.024633+00:00, data_interval_end=2024-07-06 08:20:01.024633+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:20:25.402+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:20:01.024633+00:00, run_after=2024-07-06T08:25:01.024633+00:00[0m
[[34m2024-07-06T08:22:54.736+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:25:02.960+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:25:01.024633+00:00, run_after=2024-07-06T08:30:01.024633+00:00[0m
[[34m2024-07-06T08:25:03.013+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:20:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:25:03.013+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:25:03.013+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:20:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:25:03.015+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:25:03.016+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:20:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:25:03.016+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:20:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:25:03.020+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:20:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:25:04.774+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:20:01.024633+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:25:09.498+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:20:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:25:11.484+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:20:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:25:11.498+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:20:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:25:09.600665+00:00, run_end_date=2024-07-06 08:25:10.030814+00:00, run_duration=0.430149, state=success, executor_state=success, try_number=1, max_tries=0, job_id=300, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:25:03.014575+00:00, queued_by_job_id=33, pid=16167[0m
[[34m2024-07-06T08:25:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:20:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:25:15.709+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:25:15.710+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:20:01.024633+00:00 [scheduled]>[0m
[[34m2024-07-06T08:25:15.712+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:25:15.712+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:20:01.024633+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:25:15.713+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:20:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:25:15.716+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:20:01.024633+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:25:17.351+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:20:01.024633+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:25:21.837+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:20:01.024633+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:25:25.001+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:20:01.024633+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:25:25.014+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:20:01.024633+00:00, map_index=-1, run_start_date=2024-07-06 08:25:21.928198+00:00, run_end_date=2024-07-06 08:25:23.610423+00:00, run_duration=1.682225, state=success, executor_state=success, try_number=1, max_tries=0, job_id=301, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:25:15.710736+00:00, queued_by_job_id=33, pid=16172[0m
[[34m2024-07-06T08:25:29.207+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:20:01.024633+00:00: scheduled__2024-07-06T08:20:01.024633+00:00, state:running, queued_at: 2024-07-06 08:25:02.951216+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:25:29.207+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:20:01.024633+00:00, run_id=scheduled__2024-07-06T08:20:01.024633+00:00, run_start_date=2024-07-06 08:25:02.976578+00:00, run_end_date=2024-07-06 08:25:29.207836+00:00, run_duration=26.231258, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:20:01.024633+00:00, data_interval_end=2024-07-06 08:25:01.024633+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:25:29.211+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:25:01.024633+00:00, run_after=2024-07-06T08:30:01.024633+00:00[0m
[[34m2024-07-06T08:27:54.796+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:30:07.388+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:30:06.198139+00:00, run_after=2024-07-06T08:35:06.198139+00:00[0m
[[34m2024-07-06T08:30:07.439+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:25:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:30:07.439+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:30:07.439+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:25:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:30:07.441+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:30:07.442+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:25:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:30:07.442+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:25:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:30:07.445+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:25:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:30:09.140+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:25:06.198139+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:30:13.743+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:25:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:30:15.643+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:25:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:30:15.657+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:25:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:30:13.837006+00:00, run_end_date=2024-07-06 08:30:14.280183+00:00, run_duration=0.443177, state=success, executor_state=success, try_number=1, max_tries=0, job_id=302, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:30:07.440207+00:00, queued_by_job_id=33, pid=16230[0m
[[34m2024-07-06T08:30:15.740+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:25:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:30:15.740+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:30:15.740+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:25:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:30:15.743+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:30:15.743+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:25:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:30:15.744+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:25:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:30:15.748+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:25:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:30:17.408+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:25:06.198139+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:30:21.799+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:25:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:30:24.908+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:25:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:30:24.920+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:25:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:30:21.916726+00:00, run_end_date=2024-07-06 08:30:23.636647+00:00, run_duration=1.719921, state=success, executor_state=success, try_number=1, max_tries=0, job_id=303, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:30:15.741645+00:00, queued_by_job_id=33, pid=16235[0m
[[34m2024-07-06T08:30:24.992+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:25:06.198139+00:00: scheduled__2024-07-06T08:25:06.198139+00:00, state:running, queued_at: 2024-07-06 08:30:07.382997+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:30:24.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:25:06.198139+00:00, run_id=scheduled__2024-07-06T08:25:06.198139+00:00, run_start_date=2024-07-06 08:30:07.402735+00:00, run_end_date=2024-07-06 08:30:24.993044+00:00, run_duration=17.590309, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:25:06.198139+00:00, data_interval_end=2024-07-06 08:30:06.198139+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:30:24.997+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:30:06.198139+00:00, run_after=2024-07-06T08:35:06.198139+00:00[0m
[[34m2024-07-06T08:32:59.102+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:35:08.036+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:35:06.198139+00:00, run_after=2024-07-06T08:40:06.198139+00:00[0m
[[34m2024-07-06T08:35:08.085+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:30:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:35:08.085+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:35:08.085+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:30:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:35:08.087+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:35:08.088+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:30:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:35:08.088+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:30:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:35:08.092+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:30:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:35:09.791+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:30:06.198139+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:35:14.400+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:30:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:35:16.541+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:30:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:35:16.554+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:30:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:35:14.506132+00:00, run_end_date=2024-07-06 08:35:14.931574+00:00, run_duration=0.425442, state=success, executor_state=success, try_number=1, max_tries=0, job_id=304, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:35:08.086241+00:00, queued_by_job_id=33, pid=16289[0m
[[34m2024-07-06T08:35:20.854+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:30:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:35:20.854+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:35:20.855+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:30:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:35:20.856+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:35:20.857+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:30:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:35:20.857+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:30:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:35:20.860+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:30:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:35:22.418+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:30:06.198139+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:35:26.763+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:30:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:35:29.846+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:30:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:35:29.860+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:30:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:35:26.862352+00:00, run_end_date=2024-07-06 08:35:28.499234+00:00, run_duration=1.636882, state=success, executor_state=success, try_number=1, max_tries=0, job_id=305, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:35:20.855682+00:00, queued_by_job_id=33, pid=16295[0m
[[34m2024-07-06T08:35:29.933+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:30:06.198139+00:00: scheduled__2024-07-06T08:30:06.198139+00:00, state:running, queued_at: 2024-07-06 08:35:08.028672+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:35:29.933+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:30:06.198139+00:00, run_id=scheduled__2024-07-06T08:30:06.198139+00:00, run_start_date=2024-07-06 08:35:08.051331+00:00, run_end_date=2024-07-06 08:35:29.933715+00:00, run_duration=21.882384, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:30:06.198139+00:00, data_interval_end=2024-07-06 08:35:06.198139+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:35:29.937+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:35:06.198139+00:00, run_after=2024-07-06T08:40:06.198139+00:00[0m
[[34m2024-07-06T08:37:59.955+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:40:07.168+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:40:06.198139+00:00, run_after=2024-07-06T08:45:06.198139+00:00[0m
[[34m2024-07-06T08:40:07.222+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:35:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:40:07.223+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:40:07.223+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:35:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:40:07.225+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:40:07.226+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:35:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:40:07.226+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:35:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:40:07.230+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:35:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:40:08.883+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:35:06.198139+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:40:13.398+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:35:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:40:15.326+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:35:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:40:15.339+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:35:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:40:13.500171+00:00, run_end_date=2024-07-06 08:40:13.975916+00:00, run_duration=0.475745, state=success, executor_state=success, try_number=1, max_tries=0, job_id=306, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:40:07.224034+00:00, queued_by_job_id=33, pid=16339[0m
[[34m2024-07-06T08:40:19.677+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:35:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:40:19.678+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:40:19.678+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:35:06.198139+00:00 [scheduled]>[0m
[[34m2024-07-06T08:40:19.680+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:40:19.680+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:35:06.198139+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:40:19.681+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:35:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:40:19.685+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:35:06.198139+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:40:21.245+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:35:06.198139+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:40:25.647+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:35:06.198139+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:40:28.872+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:35:06.198139+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:40:28.879+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:35:06.198139+00:00, map_index=-1, run_start_date=2024-07-06 08:40:25.737543+00:00, run_end_date=2024-07-06 08:40:27.442289+00:00, run_duration=1.704746, state=success, executor_state=success, try_number=1, max_tries=0, job_id=307, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:40:19.678949+00:00, queued_by_job_id=33, pid=16347[0m
[[34m2024-07-06T08:40:33.029+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:35:06.198139+00:00: scheduled__2024-07-06T08:35:06.198139+00:00, state:running, queued_at: 2024-07-06 08:40:07.155073+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:40:33.030+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:35:06.198139+00:00, run_id=scheduled__2024-07-06T08:35:06.198139+00:00, run_start_date=2024-07-06 08:40:07.185868+00:00, run_end_date=2024-07-06 08:40:33.030618+00:00, run_duration=25.84475, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:35:06.198139+00:00, data_interval_end=2024-07-06 08:40:06.198139+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:40:33.034+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:40:06.198139+00:00, run_after=2024-07-06T08:45:06.198139+00:00[0m
[[34m2024-07-06T08:43:00.016+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:45:10.025+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:45:08.835588+00:00, run_after=2024-07-06T08:50:08.835588+00:00[0m
[[34m2024-07-06T08:45:10.082+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:40:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:45:10.083+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:45:10.083+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:40:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:45:10.085+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:45:10.086+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:40:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:45:10.086+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:40:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:45:10.091+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:40:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:45:11.699+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:40:08.835588+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:45:16.084+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:40:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:45:17.999+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:40:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:45:18.006+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:40:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:45:16.189639+00:00, run_end_date=2024-07-06 08:45:16.644825+00:00, run_duration=0.455186, state=success, executor_state=success, try_number=1, max_tries=0, job_id=308, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:45:10.084187+00:00, queued_by_job_id=33, pid=16423[0m
[[34m2024-07-06T08:45:18.076+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:40:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:45:18.077+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:45:18.077+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:40:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:45:18.079+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:45:18.080+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:40:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:45:18.080+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:40:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:45:18.084+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:40:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:45:19.675+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:40:08.835588+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:45:24.073+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:40:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:45:27.236+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:40:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:45:27.249+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:40:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:45:24.179750+00:00, run_end_date=2024-07-06 08:45:25.850566+00:00, run_duration=1.670816, state=success, executor_state=success, try_number=1, max_tries=0, job_id=309, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:45:18.078242+00:00, queued_by_job_id=33, pid=16427[0m
[[34m2024-07-06T08:45:31.341+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:40:08.835588+00:00: scheduled__2024-07-06T08:40:08.835588+00:00, state:running, queued_at: 2024-07-06 08:45:10.018997+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:45:31.342+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:40:08.835588+00:00, run_id=scheduled__2024-07-06T08:40:08.835588+00:00, run_start_date=2024-07-06 08:45:10.042330+00:00, run_end_date=2024-07-06 08:45:31.342490+00:00, run_duration=21.30016, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:40:08.835588+00:00, data_interval_end=2024-07-06 08:45:08.835588+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:45:31.345+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:45:08.835588+00:00, run_after=2024-07-06T08:50:08.835588+00:00[0m
[[34m2024-07-06T08:48:00.958+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:50:09.202+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:50:08.835588+00:00, run_after=2024-07-06T08:55:08.835588+00:00[0m
[[34m2024-07-06T08:50:09.252+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:45:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:50:09.252+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:50:09.253+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:45:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:50:09.255+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:50:09.255+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:45:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:50:09.256+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:45:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:50:09.260+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:45:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:50:10.878+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:45:08.835588+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:50:15.338+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:45:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:50:17.247+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:45:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:50:17.259+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:45:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:50:15.432601+00:00, run_end_date=2024-07-06 08:50:15.849826+00:00, run_duration=0.417225, state=success, executor_state=success, try_number=1, max_tries=0, job_id=310, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:50:09.253774+00:00, queued_by_job_id=33, pid=16478[0m
[[34m2024-07-06T08:50:21.488+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:45:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:50:21.488+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:50:21.489+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:45:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:50:21.491+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:50:21.491+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:45:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:50:21.492+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:45:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:50:21.496+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:45:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:50:23.097+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:45:08.835588+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:50:27.335+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:45:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:50:30.316+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:45:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:50:30.323+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:45:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:50:27.420687+00:00, run_end_date=2024-07-06 08:50:28.937030+00:00, run_duration=1.516343, state=success, executor_state=success, try_number=1, max_tries=0, job_id=311, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:50:21.489902+00:00, queued_by_job_id=33, pid=16483[0m
[[34m2024-07-06T08:50:30.388+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:45:08.835588+00:00: scheduled__2024-07-06T08:45:08.835588+00:00, state:running, queued_at: 2024-07-06 08:50:09.194270+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:50:30.388+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:45:08.835588+00:00, run_id=scheduled__2024-07-06T08:45:08.835588+00:00, run_start_date=2024-07-06 08:50:09.217791+00:00, run_end_date=2024-07-06 08:50:30.388511+00:00, run_duration=21.17072, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:45:08.835588+00:00, data_interval_end=2024-07-06 08:50:08.835588+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:50:30.391+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:50:08.835588+00:00, run_after=2024-07-06T08:55:08.835588+00:00[0m
[[34m2024-07-06T08:53:03.961+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T08:55:09.142+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:55:08.835588+00:00, run_after=2024-07-06T09:00:08.835588+00:00[0m
[[34m2024-07-06T08:55:09.195+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:50:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:55:09.195+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:55:09.195+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:50:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:55:09.202+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:55:09.203+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:50:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T08:55:09.204+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:50:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:55:09.208+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:50:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:55:10.860+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:50:08.835588+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T08:55:15.418+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:50:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:55:17.332+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:50:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:55:17.344+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:50:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:55:15.517875+00:00, run_end_date=2024-07-06 08:55:15.931031+00:00, run_duration=0.413156, state=success, executor_state=success, try_number=1, max_tries=0, job_id=312, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 08:55:09.199895+00:00, queued_by_job_id=33, pid=16544[0m
[[34m2024-07-06T08:55:17.426+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:50:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:55:17.426+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T08:55:17.426+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:50:08.835588+00:00 [scheduled]>[0m
[[34m2024-07-06T08:55:17.428+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T08:55:17.429+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:50:08.835588+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T08:55:17.429+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:50:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:55:17.432+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:50:08.835588+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T08:55:19.024+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:50:08.835588+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T08:55:23.458+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:50:08.835588+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T08:55:26.611+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:50:08.835588+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T08:55:26.624+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:50:08.835588+00:00, map_index=-1, run_start_date=2024-07-06 08:55:23.554498+00:00, run_end_date=2024-07-06 08:55:25.217748+00:00, run_duration=1.66325, state=success, executor_state=success, try_number=1, max_tries=0, job_id=313, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 08:55:17.427461+00:00, queued_by_job_id=33, pid=16547[0m
[[34m2024-07-06T08:55:30.917+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:50:08.835588+00:00: scheduled__2024-07-06T08:50:08.835588+00:00, state:running, queued_at: 2024-07-06 08:55:09.137075+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T08:55:30.918+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:50:08.835588+00:00, run_id=scheduled__2024-07-06T08:50:08.835588+00:00, run_start_date=2024-07-06 08:55:09.159250+00:00, run_end_date=2024-07-06 08:55:30.918095+00:00, run_duration=21.758845, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:50:08.835588+00:00, data_interval_end=2024-07-06 08:55:08.835588+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T08:55:30.922+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T08:55:08.835588+00:00, run_after=2024-07-06T09:00:08.835588+00:00[0m
[[34m2024-07-06T08:58:04.008+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:00:13.582+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:00:12.411210+00:00, run_after=2024-07-06T09:05:12.411210+00:00[0m
[[34m2024-07-06T09:00:13.634+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:55:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:00:13.634+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:00:13.634+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:55:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:00:13.636+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:00:13.637+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:55:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:00:13.637+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:55:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:00:13.643+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T08:55:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:00:15.319+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:55:12.411210+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:00:19.846+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T08:55:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:00:21.865+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T08:55:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:00:21.879+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T08:55:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:00:19.940696+00:00, run_end_date=2024-07-06 09:00:20.392371+00:00, run_duration=0.451675, state=success, executor_state=success, try_number=1, max_tries=0, job_id=314, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:00:13.635361+00:00, queued_by_job_id=33, pid=16596[0m
[[34m2024-07-06T09:00:21.964+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:55:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:00:21.964+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:00:21.964+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:55:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:00:21.966+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:00:21.967+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:55:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:00:21.967+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:55:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:00:21.971+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T08:55:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:00:23.590+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T08:55:12.411210+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:00:27.898+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T08:55:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:00:30.918+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T08:55:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:00:30.929+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T08:55:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:00:27.988889+00:00, run_end_date=2024-07-06 09:00:29.637466+00:00, run_duration=1.648577, state=success, executor_state=success, try_number=1, max_tries=0, job_id=315, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:00:21.965279+00:00, queued_by_job_id=33, pid=16599[0m
[[34m2024-07-06T09:00:30.992+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 08:55:12.411210+00:00: scheduled__2024-07-06T08:55:12.411210+00:00, state:running, queued_at: 2024-07-06 09:00:13.577352+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:00:30.993+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 08:55:12.411210+00:00, run_id=scheduled__2024-07-06T08:55:12.411210+00:00, run_start_date=2024-07-06 09:00:13.595925+00:00, run_end_date=2024-07-06 09:00:30.993353+00:00, run_duration=17.397428, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 08:55:12.411210+00:00, data_interval_end=2024-07-06 09:00:12.411210+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:00:30.996+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:00:12.411210+00:00, run_after=2024-07-06T09:05:12.411210+00:00[0m
[[34m2024-07-06T09:03:04.066+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:05:13.523+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:05:12.411210+00:00, run_after=2024-07-06T09:10:12.411210+00:00[0m
[[34m2024-07-06T09:05:13.574+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:00:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:05:13.574+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:05:13.575+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:00:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:05:13.577+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:05:13.578+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:00:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:05:13.578+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:00:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:05:13.581+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:00:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:05:15.845+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:00:12.411210+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:05:20.799+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:00:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:05:23.042+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:00:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:05:23.059+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:00:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:05:20.939903+00:00, run_end_date=2024-07-06 09:05:21.509839+00:00, run_duration=0.569936, state=success, executor_state=success, try_number=1, max_tries=0, job_id=316, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:05:13.575938+00:00, queued_by_job_id=33, pid=17427[0m
[[34m2024-07-06T09:05:28.133+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:00:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:05:28.134+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:05:28.134+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:00:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:05:28.137+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:05:28.137+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:00:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:05:28.138+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:00:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:05:28.142+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:00:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:05:30.137+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:00:12.411210+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:05:35.233+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:00:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:05:38.637+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:00:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:05:38.654+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:00:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:05:35.341963+00:00, run_end_date=2024-07-06 09:05:37.204492+00:00, run_duration=1.862529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=317, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:05:28.135272+00:00, queued_by_job_id=33, pid=17434[0m
[[34m2024-07-06T09:05:43.065+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:00:12.411210+00:00: scheduled__2024-07-06T09:00:12.411210+00:00, state:running, queued_at: 2024-07-06 09:05:13.513699+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:05:43.066+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:00:12.411210+00:00, run_id=scheduled__2024-07-06T09:00:12.411210+00:00, run_start_date=2024-07-06 09:05:13.541021+00:00, run_end_date=2024-07-06 09:05:43.066320+00:00, run_duration=29.525299, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:00:12.411210+00:00, data_interval_end=2024-07-06 09:05:12.411210+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:05:43.071+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:05:12.411210+00:00, run_after=2024-07-06T09:10:12.411210+00:00[0m
[[34m2024-07-06T09:08:04.117+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:10:13.818+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:10:12.411210+00:00, run_after=2024-07-06T09:15:12.411210+00:00[0m
[[34m2024-07-06T09:10:13.867+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:05:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:10:13.867+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:10:13.867+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:05:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:10:13.869+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:10:13.870+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:05:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:10:13.870+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:05:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:10:13.877+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:05:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:10:15.639+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:05:12.411210+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:10:20.293+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:05:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:10:22.172+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:05:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:10:22.187+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:05:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:10:20.392286+00:00, run_end_date=2024-07-06 09:10:20.835981+00:00, run_duration=0.443695, state=success, executor_state=success, try_number=1, max_tries=0, job_id=318, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:10:13.868485+00:00, queued_by_job_id=33, pid=17511[0m
[[34m2024-07-06T09:10:26.507+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:05:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:10:26.508+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:10:26.508+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:05:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:10:26.510+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:10:26.510+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:05:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:10:26.511+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:05:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:10:26.514+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:05:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:10:28.096+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:05:12.411210+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:10:32.465+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:05:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:10:35.678+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:05:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:10:35.692+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:05:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:10:32.570268+00:00, run_end_date=2024-07-06 09:10:34.290354+00:00, run_duration=1.720086, state=success, executor_state=success, try_number=1, max_tries=0, job_id=319, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:10:26.508848+00:00, queued_by_job_id=33, pid=17518[0m
[[34m2024-07-06T09:10:35.766+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:05:12.411210+00:00: scheduled__2024-07-06T09:05:12.411210+00:00, state:running, queued_at: 2024-07-06 09:10:13.810081+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:10:35.767+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:05:12.411210+00:00, run_id=scheduled__2024-07-06T09:05:12.411210+00:00, run_start_date=2024-07-06 09:10:13.831968+00:00, run_end_date=2024-07-06 09:10:35.767330+00:00, run_duration=21.935362, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:05:12.411210+00:00, data_interval_end=2024-07-06 09:10:12.411210+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:10:35.771+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:10:12.411210+00:00, run_after=2024-07-06T09:15:12.411210+00:00[0m
[[34m2024-07-06T09:13:04.163+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:15:15.483+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:15:12.411210+00:00, run_after=2024-07-06T09:20:12.411210+00:00[0m
[[34m2024-07-06T09:15:15.531+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:10:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:15:15.532+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:15:15.532+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:10:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:15:15.534+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:15:15.535+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:10:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:15:15.535+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:10:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:15:15.539+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:10:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:15:17.220+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:10:12.411210+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:15:21.737+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:10:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:15:23.684+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:10:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:15:23.699+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:10:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:15:21.831525+00:00, run_end_date=2024-07-06 09:15:22.291276+00:00, run_duration=0.459751, state=success, executor_state=success, try_number=1, max_tries=0, job_id=320, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:15:15.532987+00:00, queued_by_job_id=33, pid=17586[0m
[[34m2024-07-06T09:15:23.795+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:10:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:15:23.796+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:15:23.796+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:10:12.411210+00:00 [scheduled]>[0m
[[34m2024-07-06T09:15:23.798+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:15:23.799+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:10:12.411210+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:15:23.799+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:10:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:15:23.803+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:10:12.411210+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:15:25.391+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:10:12.411210+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:15:29.843+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:10:12.411210+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:15:33.010+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:10:12.411210+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:15:33.027+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:10:12.411210+00:00, map_index=-1, run_start_date=2024-07-06 09:15:29.940518+00:00, run_end_date=2024-07-06 09:15:31.512099+00:00, run_duration=1.571581, state=success, executor_state=success, try_number=1, max_tries=0, job_id=321, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:15:23.797209+00:00, queued_by_job_id=33, pid=17590[0m
[[34m2024-07-06T09:15:33.098+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:10:12.411210+00:00: scheduled__2024-07-06T09:10:12.411210+00:00, state:running, queued_at: 2024-07-06 09:15:15.476821+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:15:33.098+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:10:12.411210+00:00, run_id=scheduled__2024-07-06T09:10:12.411210+00:00, run_start_date=2024-07-06 09:15:15.497391+00:00, run_end_date=2024-07-06 09:15:33.098787+00:00, run_duration=17.601396, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:10:12.411210+00:00, data_interval_end=2024-07-06 09:15:12.411210+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:15:33.102+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:15:12.411210+00:00, run_after=2024-07-06T09:20:12.411210+00:00[0m
[[34m2024-07-06T09:18:04.212+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:20:18.997+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:20:17.806037+00:00, run_after=2024-07-06T09:25:17.806037+00:00[0m
[[34m2024-07-06T09:20:19.047+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:15:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:20:19.047+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:20:19.047+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:15:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:20:19.049+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:20:19.050+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:15:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:20:19.050+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:15:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:20:19.053+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:15:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:20:20.730+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:15:17.806037+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:20:25.222+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:15:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:20:27.053+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:15:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:20:27.066+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:15:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:20:25.311357+00:00, run_end_date=2024-07-06 09:20:25.751312+00:00, run_duration=0.439955, state=success, executor_state=success, try_number=1, max_tries=0, job_id=322, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:20:19.048311+00:00, queued_by_job_id=33, pid=17673[0m
[[34m2024-07-06T09:20:31.271+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:15:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:20:31.272+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:20:31.272+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:15:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:20:31.274+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:20:31.275+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:15:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:20:31.275+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:15:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:20:31.279+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:15:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:20:32.851+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:15:17.806037+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:20:37.242+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:15:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:20:40.380+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:15:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:20:40.394+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:15:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:20:37.337104+00:00, run_end_date=2024-07-06 09:20:38.969669+00:00, run_duration=1.632565, state=success, executor_state=success, try_number=1, max_tries=0, job_id=323, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:20:31.273188+00:00, queued_by_job_id=33, pid=17678[0m
[[34m2024-07-06T09:20:40.472+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:15:17.806037+00:00: scheduled__2024-07-06T09:15:17.806037+00:00, state:running, queued_at: 2024-07-06 09:20:18.990569+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:20:40.472+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:15:17.806037+00:00, run_id=scheduled__2024-07-06T09:15:17.806037+00:00, run_start_date=2024-07-06 09:20:19.014350+00:00, run_end_date=2024-07-06 09:20:40.472481+00:00, run_duration=21.458131, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:15:17.806037+00:00, data_interval_end=2024-07-06 09:20:17.806037+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:20:40.475+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:20:17.806037+00:00, run_after=2024-07-06T09:25:17.806037+00:00[0m
[[34m2024-07-06T09:23:04.258+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:25:18.602+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:25:17.806037+00:00, run_after=2024-07-06T09:30:17.806037+00:00[0m
[[34m2024-07-06T09:25:18.655+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:20:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:25:18.655+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:25:18.655+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:20:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:25:18.658+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:25:18.659+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:20:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:25:18.659+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:20:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:25:18.665+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:20:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:25:20.314+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:20:17.806037+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:25:24.887+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:20:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:25:26.860+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:20:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:25:26.873+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:20:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:25:24.983351+00:00, run_end_date=2024-07-06 09:25:25.449192+00:00, run_duration=0.465841, state=success, executor_state=success, try_number=1, max_tries=0, job_id=324, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:25:18.656761+00:00, queued_by_job_id=33, pid=17750[0m
[[34m2024-07-06T09:25:31.319+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:20:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:25:31.320+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:25:31.321+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:20:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:25:31.323+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:25:31.324+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:20:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:25:31.324+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:20:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:25:31.327+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:20:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:25:32.881+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:20:17.806037+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:25:37.119+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:20:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:25:40.381+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:20:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:25:40.388+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:20:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:25:37.215468+00:00, run_end_date=2024-07-06 09:25:38.860521+00:00, run_duration=1.645053, state=success, executor_state=success, try_number=1, max_tries=0, job_id=325, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:25:31.321700+00:00, queued_by_job_id=33, pid=17757[0m
[[34m2024-07-06T09:25:45.189+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:20:17.806037+00:00: scheduled__2024-07-06T09:20:17.806037+00:00, state:running, queued_at: 2024-07-06 09:25:18.596024+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:25:45.190+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:20:17.806037+00:00, run_id=scheduled__2024-07-06T09:20:17.806037+00:00, run_start_date=2024-07-06 09:25:18.616099+00:00, run_end_date=2024-07-06 09:25:45.190129+00:00, run_duration=26.57403, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:20:17.806037+00:00, data_interval_end=2024-07-06 09:25:17.806037+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:25:45.193+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:25:17.806037+00:00, run_after=2024-07-06T09:30:17.806037+00:00[0m
[[34m2024-07-06T09:28:04.305+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:30:19.909+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:30:17.806037+00:00, run_after=2024-07-06T09:35:17.806037+00:00[0m
[[34m2024-07-06T09:30:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:25:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:30:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:30:19.962+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:25:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:30:19.965+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:30:19.966+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:25:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:30:19.966+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:25:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:30:19.971+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:25:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:30:21.524+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:25:17.806037+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:30:25.787+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:25:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:30:27.799+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:25:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:30:27.813+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:25:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:30:25.892555+00:00, run_end_date=2024-07-06 09:30:26.337704+00:00, run_duration=0.445149, state=success, executor_state=success, try_number=1, max_tries=0, job_id=326, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:30:19.963608+00:00, queued_by_job_id=33, pid=17820[0m
[[34m2024-07-06T09:30:27.924+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:25:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:30:27.925+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:30:27.925+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:25:17.806037+00:00 [scheduled]>[0m
[[34m2024-07-06T09:30:27.927+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:30:27.928+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:25:17.806037+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:30:27.928+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:25:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:30:27.931+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:25:17.806037+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:30:29.517+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:25:17.806037+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:30:33.916+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:25:17.806037+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:30:37.063+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:25:17.806037+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:30:37.077+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:25:17.806037+00:00, map_index=-1, run_start_date=2024-07-06 09:30:34.018161+00:00, run_end_date=2024-07-06 09:30:35.717080+00:00, run_duration=1.698919, state=success, executor_state=success, try_number=1, max_tries=0, job_id=327, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:30:27.925841+00:00, queued_by_job_id=33, pid=17823[0m
[[34m2024-07-06T09:30:37.137+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:25:17.806037+00:00: scheduled__2024-07-06T09:25:17.806037+00:00, state:running, queued_at: 2024-07-06 09:30:19.903409+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:30:37.138+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:25:17.806037+00:00, run_id=scheduled__2024-07-06T09:25:17.806037+00:00, run_start_date=2024-07-06 09:30:19.923911+00:00, run_end_date=2024-07-06 09:30:37.138371+00:00, run_duration=17.21446, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:25:17.806037+00:00, data_interval_end=2024-07-06 09:30:17.806037+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:30:37.142+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:30:17.806037+00:00, run_after=2024-07-06T09:35:17.806037+00:00[0m
[[34m2024-07-06T09:33:04.350+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:35:19.710+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:35:18.573262+00:00, run_after=2024-07-06T09:40:18.573262+00:00[0m
[[34m2024-07-06T09:35:19.757+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:35:19.757+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:35:19.757+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:35:19.759+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:35:19.760+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:30:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:35:19.761+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:35:19.764+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:35:21.539+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:30:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:35:26.366+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:30:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:35:28.399+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:30:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:35:28.414+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:30:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:35:26.463887+00:00, run_end_date=2024-07-06 09:35:26.951418+00:00, run_duration=0.487531, state=success, executor_state=success, try_number=1, max_tries=0, job_id=328, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:35:19.758529+00:00, queued_by_job_id=33, pid=17895[0m
[[34m2024-07-06T09:35:32.720+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:35:32.721+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:35:32.721+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:35:32.723+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:35:32.724+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:30:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:35:32.724+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:35:32.727+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:35:34.308+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:30:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:35:38.751+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:30:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:35:41.702+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:30:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:35:41.717+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:30:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:35:38.844943+00:00, run_end_date=2024-07-06 09:35:40.322854+00:00, run_duration=1.477911, state=success, executor_state=success, try_number=1, max_tries=0, job_id=329, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:35:32.721877+00:00, queued_by_job_id=33, pid=17900[0m
[[34m2024-07-06T09:35:41.801+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:30:18.573262+00:00: scheduled__2024-07-06T09:30:18.573262+00:00, state:running, queued_at: 2024-07-06 09:35:19.705825+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:35:41.802+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:30:18.573262+00:00, run_id=scheduled__2024-07-06T09:30:18.573262+00:00, run_start_date=2024-07-06 09:35:19.724963+00:00, run_end_date=2024-07-06 09:35:41.801937+00:00, run_duration=22.076974, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:30:18.573262+00:00, data_interval_end=2024-07-06 09:35:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:35:41.805+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:35:18.573262+00:00, run_after=2024-07-06T09:40:18.573262+00:00[0m
[[34m2024-07-06T09:38:04.411+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:40:19.085+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:40:18.573262+00:00, run_after=2024-07-06T09:45:18.573262+00:00[0m
[[34m2024-07-06T09:40:19.132+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:40:19.132+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:40:19.133+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:40:19.135+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:40:19.135+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:35:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:40:19.136+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:40:19.143+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:40:21.174+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:35:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:40:25.936+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:35:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:40:27.798+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:35:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:40:27.811+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:35:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:40:26.030553+00:00, run_end_date=2024-07-06 09:40:26.479183+00:00, run_duration=0.44863, state=success, executor_state=success, try_number=1, max_tries=0, job_id=330, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:40:19.133746+00:00, queued_by_job_id=33, pid=18289[0m
[[34m2024-07-06T09:40:32.352+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:40:32.353+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:40:32.353+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:40:32.355+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:40:32.356+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:35:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:40:32.356+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:40:32.359+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:40:33.920+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:35:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:40:38.223+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:35:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:40:41.483+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:35:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:40:41.503+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:35:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:40:38.320310+00:00, run_end_date=2024-07-06 09:40:39.874979+00:00, run_duration=1.554669, state=success, executor_state=success, try_number=1, max_tries=0, job_id=331, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:40:32.354075+00:00, queued_by_job_id=33, pid=18320[0m
[[34m2024-07-06T09:40:45.714+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:35:18.573262+00:00: scheduled__2024-07-06T09:35:18.573262+00:00, state:running, queued_at: 2024-07-06 09:40:19.079048+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:40:45.714+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:35:18.573262+00:00, run_id=scheduled__2024-07-06T09:35:18.573262+00:00, run_start_date=2024-07-06 09:40:19.099739+00:00, run_end_date=2024-07-06 09:40:45.714737+00:00, run_duration=26.614998, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:35:18.573262+00:00, data_interval_end=2024-07-06 09:40:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:40:45.718+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:40:18.573262+00:00, run_after=2024-07-06T09:45:18.573262+00:00[0m
[[34m2024-07-06T09:43:07.761+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:45:19.591+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:45:18.573262+00:00, run_after=2024-07-06T09:50:18.573262+00:00[0m
[[34m2024-07-06T09:45:19.647+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:45:19.648+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:45:19.648+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:45:19.650+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:45:19.651+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:40:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:45:19.651+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:45:19.654+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:45:21.656+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:40:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:45:26.272+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:40:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:45:28.403+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:40:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:45:28.422+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:40:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:45:26.384295+00:00, run_end_date=2024-07-06 09:45:26.952242+00:00, run_duration=0.567947, state=success, executor_state=success, try_number=1, max_tries=0, job_id=332, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:45:19.649349+00:00, queued_by_job_id=33, pid=20095[0m
[[34m2024-07-06T09:45:32.540+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:45:32.541+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:45:32.541+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:45:32.543+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:45:32.544+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:40:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:45:32.544+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:45:32.548+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:45:34.185+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:40:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:45:38.582+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:40:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:45:41.700+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:40:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:45:41.714+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:40:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:45:38.672325+00:00, run_end_date=2024-07-06 09:45:40.365518+00:00, run_duration=1.693193, state=success, executor_state=success, try_number=1, max_tries=0, job_id=333, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:45:32.542011+00:00, queued_by_job_id=33, pid=20142[0m
[[34m2024-07-06T09:45:41.792+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:40:18.573262+00:00: scheduled__2024-07-06T09:40:18.573262+00:00, state:running, queued_at: 2024-07-06 09:45:19.574713+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:45:41.792+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:40:18.573262+00:00, run_id=scheduled__2024-07-06T09:40:18.573262+00:00, run_start_date=2024-07-06 09:45:19.606443+00:00, run_end_date=2024-07-06 09:45:41.792675+00:00, run_duration=22.186232, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:40:18.573262+00:00, data_interval_end=2024-07-06 09:45:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:45:41.796+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:45:18.573262+00:00, run_after=2024-07-06T09:50:18.573262+00:00[0m
[[34m2024-07-06T09:48:08.260+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:50:19.431+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:50:18.573262+00:00, run_after=2024-07-06T09:55:18.573262+00:00[0m
[[34m2024-07-06T09:50:19.477+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:50:19.477+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:50:19.477+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:50:19.480+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:50:19.481+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:45:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:50:19.481+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:50:19.486+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:50:21.304+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:45:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:50:26.001+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:45:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:50:28.158+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:45:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:50:28.174+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:45:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:50:26.101326+00:00, run_end_date=2024-07-06 09:50:26.594167+00:00, run_duration=0.492841, state=success, executor_state=success, try_number=1, max_tries=0, job_id=334, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:50:19.478686+00:00, queued_by_job_id=33, pid=21249[0m
[[34m2024-07-06T09:50:32.599+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:50:32.600+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:50:32.600+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:50:32.602+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:50:32.603+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:45:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:50:32.603+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:50:32.606+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:50:34.241+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:45:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:50:38.833+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:45:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:50:41.895+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:45:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:50:41.910+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:45:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:50:38.933384+00:00, run_end_date=2024-07-06 09:50:40.455774+00:00, run_duration=1.52239, state=success, executor_state=success, try_number=1, max_tries=0, job_id=335, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:50:32.600989+00:00, queued_by_job_id=33, pid=21300[0m
[[34m2024-07-06T09:50:41.980+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:45:18.573262+00:00: scheduled__2024-07-06T09:45:18.573262+00:00, state:running, queued_at: 2024-07-06 09:50:19.425986+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:50:41.980+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:45:18.573262+00:00, run_id=scheduled__2024-07-06T09:45:18.573262+00:00, run_start_date=2024-07-06 09:50:19.444880+00:00, run_end_date=2024-07-06 09:50:41.980603+00:00, run_duration=22.535723, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:45:18.573262+00:00, data_interval_end=2024-07-06 09:50:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:50:41.984+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:50:18.573262+00:00, run_after=2024-07-06T09:55:18.573262+00:00[0m
[[34m2024-07-06T09:53:08.317+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T09:55:19.014+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:55:18.573262+00:00, run_after=2024-07-06T10:00:18.573262+00:00[0m
[[34m2024-07-06T09:55:19.073+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:55:19.073+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:55:19.073+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:55:19.075+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:55:19.076+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:50:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T09:55:19.076+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:55:19.080+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:55:20.855+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:50:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T09:55:25.515+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:50:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:55:27.491+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:50:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:55:27.506+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:50:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:55:25.612630+00:00, run_end_date=2024-07-06 09:55:26.075290+00:00, run_duration=0.46266, state=success, executor_state=success, try_number=1, max_tries=0, job_id=336, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 09:55:19.074450+00:00, queued_by_job_id=33, pid=22625[0m
[[34m2024-07-06T09:55:31.824+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:55:31.825+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T09:55:31.825+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T09:55:31.827+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T09:55:31.828+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:50:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T09:55:31.828+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:55:31.832+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T09:55:33.502+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:50:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T09:55:38.000+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:50:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T09:55:41.173+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:50:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T09:55:41.180+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:50:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 09:55:38.102332+00:00, run_end_date=2024-07-06 09:55:39.700349+00:00, run_duration=1.598017, state=success, executor_state=success, try_number=1, max_tries=0, job_id=337, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 09:55:31.826301+00:00, queued_by_job_id=33, pid=22685[0m
[[34m2024-07-06T09:55:41.244+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:50:18.573262+00:00: scheduled__2024-07-06T09:50:18.573262+00:00, state:running, queued_at: 2024-07-06 09:55:19.007911+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T09:55:41.245+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:50:18.573262+00:00, run_id=scheduled__2024-07-06T09:50:18.573262+00:00, run_start_date=2024-07-06 09:55:19.034303+00:00, run_end_date=2024-07-06 09:55:41.245522+00:00, run_duration=22.211219, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:50:18.573262+00:00, data_interval_end=2024-07-06 09:55:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T09:55:41.249+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T09:55:18.573262+00:00, run_after=2024-07-06T10:00:18.573262+00:00[0m
[[34m2024-07-06T09:58:09.781+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:00:19.910+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:00:18.573262+00:00, run_after=2024-07-06T10:05:18.573262+00:00[0m
[[34m2024-07-06T10:00:19.958+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:00:19.958+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:00:19.958+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:00:19.960+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:00:19.961+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:55:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:00:19.961+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:00:19.966+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T09:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:00:21.798+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:55:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:00:26.571+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T09:55:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:00:28.631+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T09:55:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:00:28.644+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T09:55:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:00:26.672465+00:00, run_end_date=2024-07-06 10:00:27.199442+00:00, run_duration=0.526977, state=success, executor_state=success, try_number=1, max_tries=0, job_id=338, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:00:19.959328+00:00, queued_by_job_id=33, pid=23991[0m
[[34m2024-07-06T10:00:32.967+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:00:32.967+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:00:32.968+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:00:32.969+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:00:32.970+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:55:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:00:32.970+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:00:32.979+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T09:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:00:34.620+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T09:55:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:00:39.252+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T09:55:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:00:42.498+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T09:55:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:00:42.513+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T09:55:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:00:39.348606+00:00, run_end_date=2024-07-06 10:00:41.072332+00:00, run_duration=1.723726, state=success, executor_state=success, try_number=1, max_tries=0, job_id=339, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:00:32.968704+00:00, queued_by_job_id=33, pid=24053[0m
[[34m2024-07-06T10:00:46.715+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 09:55:18.573262+00:00: scheduled__2024-07-06T09:55:18.573262+00:00, state:running, queued_at: 2024-07-06 10:00:19.904930+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:00:46.716+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 09:55:18.573262+00:00, run_id=scheduled__2024-07-06T09:55:18.573262+00:00, run_start_date=2024-07-06 10:00:19.924597+00:00, run_end_date=2024-07-06 10:00:46.716143+00:00, run_duration=26.791546, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 09:55:18.573262+00:00, data_interval_end=2024-07-06 10:00:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:00:46.719+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:00:18.573262+00:00, run_after=2024-07-06T10:05:18.573262+00:00[0m
[[34m2024-07-06T10:03:09.829+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:05:22.422+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:05:18.573262+00:00, run_after=2024-07-06T10:10:18.573262+00:00[0m
[[34m2024-07-06T10:05:22.472+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:05:22.473+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:05:22.473+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:05:22.475+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:05:22.475+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:00:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:05:22.476+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:05:22.484+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:05:24.114+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:00:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:05:28.775+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:00:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:05:30.695+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:00:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:05:30.708+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:00:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:05:28.871920+00:00, run_end_date=2024-07-06 10:05:29.354236+00:00, run_duration=0.482316, state=success, executor_state=success, try_number=1, max_tries=0, job_id=340, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:05:22.473855+00:00, queued_by_job_id=33, pid=25381[0m
[[34m2024-07-06T10:05:30.799+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:05:30.799+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:05:30.800+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:05:30.801+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:05:30.802+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:00:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:05:30.802+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:05:30.806+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:05:32.480+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:00:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:05:37.255+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:00:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:05:40.399+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:00:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:05:40.413+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:00:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:05:37.356609+00:00, run_end_date=2024-07-06 10:05:39.103593+00:00, run_duration=1.746984, state=success, executor_state=success, try_number=1, max_tries=0, job_id=341, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:05:30.800749+00:00, queued_by_job_id=33, pid=25422[0m
[[34m2024-07-06T10:05:40.473+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:00:18.573262+00:00: scheduled__2024-07-06T10:00:18.573262+00:00, state:running, queued_at: 2024-07-06 10:05:22.413973+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:05:40.473+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:00:18.573262+00:00, run_id=scheduled__2024-07-06T10:00:18.573262+00:00, run_start_date=2024-07-06 10:05:22.437835+00:00, run_end_date=2024-07-06 10:05:40.473833+00:00, run_duration=18.035998, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:00:18.573262+00:00, data_interval_end=2024-07-06 10:05:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:05:40.477+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:05:18.573262+00:00, run_after=2024-07-06T10:10:18.573262+00:00[0m
[[34m2024-07-06T10:08:09.881+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:10:19.573+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:10:18.573262+00:00, run_after=2024-07-06T10:15:18.573262+00:00[0m
[[34m2024-07-06T10:10:19.622+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:10:19.622+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:10:19.623+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:10:19.624+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:10:19.625+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:05:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:10:19.625+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:10:19.629+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:10:21.336+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:05:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:10:25.952+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:05:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:10:27.799+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:05:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:10:27.816+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:05:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:10:26.050839+00:00, run_end_date=2024-07-06 10:10:26.527270+00:00, run_duration=0.476431, state=success, executor_state=success, try_number=1, max_tries=0, job_id=342, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:10:19.623507+00:00, queued_by_job_id=33, pid=26752[0m
[[34m2024-07-06T10:10:31.950+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:10:31.950+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:10:31.951+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:10:31.952+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:10:31.953+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:05:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:10:31.953+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:10:31.957+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:10:33.668+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:05:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:10:38.401+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:05:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:10:41.656+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:05:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:10:41.669+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:05:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:10:38.511778+00:00, run_end_date=2024-07-06 10:10:40.258695+00:00, run_duration=1.746917, state=success, executor_state=success, try_number=1, max_tries=0, job_id=343, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:10:31.951501+00:00, queued_by_job_id=33, pid=26812[0m
[[34m2024-07-06T10:10:45.887+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:05:18.573262+00:00: scheduled__2024-07-06T10:05:18.573262+00:00, state:running, queued_at: 2024-07-06 10:10:19.568612+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:10:45.888+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:05:18.573262+00:00, run_id=scheduled__2024-07-06T10:05:18.573262+00:00, run_start_date=2024-07-06 10:10:19.590786+00:00, run_end_date=2024-07-06 10:10:45.887961+00:00, run_duration=26.297175, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:05:18.573262+00:00, data_interval_end=2024-07-06 10:10:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:10:45.891+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:10:18.573262+00:00, run_after=2024-07-06T10:15:18.573262+00:00[0m
[[34m2024-07-06T10:13:09.932+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:15:22.178+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:15:18.573262+00:00, run_after=2024-07-06T10:20:18.573262+00:00[0m
[[34m2024-07-06T10:15:22.224+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:15:22.225+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:15:22.225+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:15:22.227+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:15:22.227+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:10:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:15:22.227+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:15:22.231+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:15:23.866+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:10:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:15:28.526+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:10:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:15:30.426+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:10:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:15:30.441+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:10:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:15:28.620145+00:00, run_end_date=2024-07-06 10:15:29.060550+00:00, run_duration=0.440405, state=success, executor_state=success, try_number=1, max_tries=0, job_id=344, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:15:22.225835+00:00, queued_by_job_id=33, pid=28127[0m
[[34m2024-07-06T10:15:30.536+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:15:30.536+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:15:30.536+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:15:30.538+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:15:30.539+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:10:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:15:30.539+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:15:30.542+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:15:32.232+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:10:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:15:36.910+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:10:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:15:40.252+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:10:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:15:40.264+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:10:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:15:37.014333+00:00, run_end_date=2024-07-06 10:15:38.781280+00:00, run_duration=1.766947, state=success, executor_state=success, try_number=1, max_tries=0, job_id=345, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:15:30.537438+00:00, queued_by_job_id=33, pid=28168[0m
[[34m2024-07-06T10:15:40.323+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:10:18.573262+00:00: scheduled__2024-07-06T10:10:18.573262+00:00, state:running, queued_at: 2024-07-06 10:15:22.172704+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:15:40.323+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:10:18.573262+00:00, run_id=scheduled__2024-07-06T10:10:18.573262+00:00, run_start_date=2024-07-06 10:15:22.192714+00:00, run_end_date=2024-07-06 10:15:40.323538+00:00, run_duration=18.130824, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:10:18.573262+00:00, data_interval_end=2024-07-06 10:15:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:15:40.327+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:15:18.573262+00:00, run_after=2024-07-06T10:20:18.573262+00:00[0m
[[34m2024-07-06T10:18:09.994+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:20:19.391+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:20:18.573262+00:00, run_after=2024-07-06T10:25:18.573262+00:00[0m
[[34m2024-07-06T10:20:19.440+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:15:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:20:19.441+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:20:19.441+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:15:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:20:19.443+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:20:19.443+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:15:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:20:19.444+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:15:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:20:19.446+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:15:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:20:21.135+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:15:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:20:25.679+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:15:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:20:27.745+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:15:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:20:27.758+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:15:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:20:25.784108+00:00, run_end_date=2024-07-06 10:20:26.276835+00:00, run_duration=0.492727, state=success, executor_state=success, try_number=1, max_tries=0, job_id=346, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:20:19.442043+00:00, queued_by_job_id=33, pid=29506[0m
[[34m2024-07-06T10:20:32.088+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:15:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:20:32.089+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:20:32.089+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:15:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:20:32.091+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:20:32.091+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:15:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:20:32.091+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:15:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:20:32.095+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:15:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:20:33.705+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:15:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:20:38.154+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:15:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:20:41.294+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:15:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:20:41.307+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:15:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:20:38.260975+00:00, run_end_date=2024-07-06 10:20:39.945296+00:00, run_duration=1.684321, state=success, executor_state=success, try_number=1, max_tries=0, job_id=347, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:20:32.090054+00:00, queued_by_job_id=33, pid=29566[0m
[[34m2024-07-06T10:20:45.520+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:15:18.573262+00:00: scheduled__2024-07-06T10:15:18.573262+00:00, state:running, queued_at: 2024-07-06 10:20:19.384458+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:20:45.521+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:15:18.573262+00:00, run_id=scheduled__2024-07-06T10:15:18.573262+00:00, run_start_date=2024-07-06 10:20:19.405517+00:00, run_end_date=2024-07-06 10:20:45.521549+00:00, run_duration=26.116032, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:15:18.573262+00:00, data_interval_end=2024-07-06 10:20:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:20:45.525+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:20:18.573262+00:00, run_after=2024-07-06T10:25:18.573262+00:00[0m
[[34m2024-07-06T10:23:10.051+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:25:19.082+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:25:18.573262+00:00, run_after=2024-07-06T10:30:18.573262+00:00[0m
[[34m2024-07-06T10:25:19.166+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:20:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:25:19.167+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:25:19.167+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:20:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:25:19.170+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:25:19.171+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:20:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:25:19.171+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:20:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:25:19.180+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:20:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:25:21.138+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:20:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:25:26.051+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:20:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:25:28.223+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:20:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:25:28.240+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:20:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:25:26.161705+00:00, run_end_date=2024-07-06 10:25:26.658198+00:00, run_duration=0.496493, state=success, executor_state=success, try_number=1, max_tries=0, job_id=348, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:25:19.168441+00:00, queued_by_job_id=33, pid=30890[0m
[[34m2024-07-06T10:25:32.589+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:20:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:25:32.590+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:25:32.590+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:20:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:25:32.592+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:25:32.593+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:20:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:25:32.593+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:20:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:25:32.597+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:20:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:25:34.285+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:20:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:25:38.951+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:20:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:25:42.446+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:20:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:25:42.462+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:20:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:25:39.048116+00:00, run_end_date=2024-07-06 10:25:40.804176+00:00, run_duration=1.75606, state=success, executor_state=success, try_number=1, max_tries=0, job_id=349, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:25:32.591105+00:00, queued_by_job_id=33, pid=30952[0m
[[34m2024-07-06T10:25:42.535+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:20:18.573262+00:00: scheduled__2024-07-06T10:20:18.573262+00:00, state:running, queued_at: 2024-07-06 10:25:19.076661+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:25:42.536+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:20:18.573262+00:00, run_id=scheduled__2024-07-06T10:20:18.573262+00:00, run_start_date=2024-07-06 10:25:19.102739+00:00, run_end_date=2024-07-06 10:25:42.536047+00:00, run_duration=23.433308, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:20:18.573262+00:00, data_interval_end=2024-07-06 10:25:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:25:42.540+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:25:18.573262+00:00, run_after=2024-07-06T10:30:18.573262+00:00[0m
[[34m2024-07-06T10:28:11.395+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:30:19.427+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:30:18.573262+00:00, run_after=2024-07-06T10:35:18.573262+00:00[0m
[[34m2024-07-06T10:30:19.478+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:25:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:30:19.479+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:30:19.479+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:25:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:30:19.481+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:30:19.482+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:25:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:30:19.482+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:25:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:30:19.486+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:25:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:30:21.302+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:25:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:30:26.166+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:25:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:30:28.703+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:25:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:30:28.717+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:25:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:30:26.270897+00:00, run_end_date=2024-07-06 10:30:26.793660+00:00, run_duration=0.522763, state=success, executor_state=success, try_number=1, max_tries=0, job_id=350, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:30:19.480213+00:00, queued_by_job_id=33, pid=32489[0m
[[34m2024-07-06T10:30:33.254+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:25:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:30:33.255+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:30:33.256+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:25:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:30:33.258+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:30:33.259+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:25:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:30:33.259+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:25:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:30:33.263+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:25:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:30:34.928+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:25:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:30:39.474+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:25:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:30:42.767+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:25:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:30:42.781+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:25:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:30:39.578115+00:00, run_end_date=2024-07-06 10:30:41.320330+00:00, run_duration=1.742215, state=success, executor_state=success, try_number=1, max_tries=0, job_id=351, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:30:33.256680+00:00, queued_by_job_id=33, pid=32564[0m
[[34m2024-07-06T10:30:42.886+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:25:18.573262+00:00: scheduled__2024-07-06T10:25:18.573262+00:00, state:running, queued_at: 2024-07-06 10:30:19.421864+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:30:42.887+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:25:18.573262+00:00, run_id=scheduled__2024-07-06T10:25:18.573262+00:00, run_start_date=2024-07-06 10:30:19.443147+00:00, run_end_date=2024-07-06 10:30:42.886911+00:00, run_duration=23.443764, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:25:18.573262+00:00, data_interval_end=2024-07-06 10:30:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:30:42.890+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:30:18.573262+00:00, run_after=2024-07-06T10:35:18.573262+00:00[0m
[[34m2024-07-06T10:33:11.452+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:35:19.705+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:35:18.573262+00:00, run_after=2024-07-06T10:40:18.573262+00:00[0m
[[34m2024-07-06T10:35:19.764+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:35:19.764+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:35:19.764+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:35:19.766+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:35:19.767+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:30:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:35:19.767+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:35:19.771+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:35:21.558+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:30:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:35:26.332+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:30:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:35:28.334+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:30:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:35:28.352+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:30:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:35:26.437523+00:00, run_end_date=2024-07-06 10:35:26.904700+00:00, run_duration=0.467177, state=success, executor_state=success, try_number=1, max_tries=0, job_id=352, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:35:19.765462+00:00, queued_by_job_id=33, pid=34112[0m
[[34m2024-07-06T10:35:32.967+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:35:32.968+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:35:32.969+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:30:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:35:32.971+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:35:32.972+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:30:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:35:32.972+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:35:32.976+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:30:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:35:34.668+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:30:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:35:39.188+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:30:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:35:42.577+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:30:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:35:42.584+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:30:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:35:39.294689+00:00, run_end_date=2024-07-06 10:35:41.085720+00:00, run_duration=1.791031, state=success, executor_state=success, try_number=1, max_tries=0, job_id=353, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:35:32.969766+00:00, queued_by_job_id=33, pid=34185[0m
[[34m2024-07-06T10:35:42.660+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:30:18.573262+00:00: scheduled__2024-07-06T10:30:18.573262+00:00, state:running, queued_at: 2024-07-06 10:35:19.699597+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:35:42.661+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:30:18.573262+00:00, run_id=scheduled__2024-07-06T10:30:18.573262+00:00, run_start_date=2024-07-06 10:35:19.719685+00:00, run_end_date=2024-07-06 10:35:42.660831+00:00, run_duration=22.941146, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:30:18.573262+00:00, data_interval_end=2024-07-06 10:35:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:35:42.664+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:35:18.573262+00:00, run_after=2024-07-06T10:40:18.573262+00:00[0m
[[34m2024-07-06T10:38:11.510+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:40:19.144+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:40:18.573262+00:00, run_after=2024-07-06T10:45:18.573262+00:00[0m
[[34m2024-07-06T10:40:19.197+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:40:19.197+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:40:19.197+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:40:19.199+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:40:19.199+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:35:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:40:19.200+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:40:19.203+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:40:21.025+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:35:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:40:25.810+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:35:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:40:27.984+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:35:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:40:27.999+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:35:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:40:25.905067+00:00, run_end_date=2024-07-06 10:40:26.401010+00:00, run_duration=0.495943, state=success, executor_state=success, try_number=1, max_tries=0, job_id=354, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:40:19.198129+00:00, queued_by_job_id=33, pid=35743[0m
[[34m2024-07-06T10:40:33.033+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:40:33.034+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:40:33.034+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:35:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:40:33.036+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:40:33.037+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:35:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:40:33.037+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:40:33.040+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:35:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:40:34.680+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:35:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:40:39.337+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:35:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:40:42.621+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:35:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:40:42.633+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:35:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:40:39.439250+00:00, run_end_date=2024-07-06 10:40:41.202307+00:00, run_duration=1.763057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=355, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:40:33.035259+00:00, queued_by_job_id=33, pid=35819[0m
[[34m2024-07-06T10:40:46.958+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:35:18.573262+00:00: scheduled__2024-07-06T10:35:18.573262+00:00, state:running, queued_at: 2024-07-06 10:40:19.139339+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:40:46.959+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:35:18.573262+00:00, run_id=scheduled__2024-07-06T10:35:18.573262+00:00, run_start_date=2024-07-06 10:40:19.161686+00:00, run_end_date=2024-07-06 10:40:46.959302+00:00, run_duration=27.797616, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:35:18.573262+00:00, data_interval_end=2024-07-06 10:40:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:40:46.963+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:40:18.573262+00:00, run_after=2024-07-06T10:45:18.573262+00:00[0m
[[34m2024-07-06T10:43:11.560+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:45:19.248+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:45:18.573262+00:00, run_after=2024-07-06T10:50:18.573262+00:00[0m
[[34m2024-07-06T10:45:19.296+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:45:19.296+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:45:19.297+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:45:19.298+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:45:19.299+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:40:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:45:19.299+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:45:19.303+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:45:20.960+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:40:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:45:25.706+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:40:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:45:27.774+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:40:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:45:27.785+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:40:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:45:25.816293+00:00, run_end_date=2024-07-06 10:45:26.314052+00:00, run_duration=0.497759, state=success, executor_state=success, try_number=1, max_tries=0, job_id=356, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:45:19.297687+00:00, queued_by_job_id=33, pid=37428[0m
[[34m2024-07-06T10:45:32.201+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:45:32.202+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:45:32.202+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:40:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:45:32.204+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:45:32.205+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:40:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:45:32.205+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:45:32.209+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:40:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:45:33.939+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:40:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:45:38.572+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:40:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:45:42.056+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:40:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:45:42.067+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:40:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:45:38.685564+00:00, run_end_date=2024-07-06 10:45:40.473948+00:00, run_duration=1.788384, state=success, executor_state=success, try_number=1, max_tries=0, job_id=357, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:45:32.202932+00:00, queued_by_job_id=33, pid=37505[0m
[[34m2024-07-06T10:45:42.147+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:40:18.573262+00:00: scheduled__2024-07-06T10:40:18.573262+00:00, state:running, queued_at: 2024-07-06 10:45:19.243250+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:45:42.148+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:40:18.573262+00:00, run_id=scheduled__2024-07-06T10:40:18.573262+00:00, run_start_date=2024-07-06 10:45:19.264362+00:00, run_end_date=2024-07-06 10:45:42.148043+00:00, run_duration=22.883681, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:40:18.573262+00:00, data_interval_end=2024-07-06 10:45:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:45:42.156+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:45:18.573262+00:00, run_after=2024-07-06T10:50:18.573262+00:00[0m
[[34m2024-07-06T10:48:11.575+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:50:19.075+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:50:18.573262+00:00, run_after=2024-07-06T10:55:18.573262+00:00[0m
[[34m2024-07-06T10:50:19.131+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:50:19.131+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:50:19.132+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:50:19.134+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:50:19.135+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:45:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:50:19.135+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:50:19.139+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:50:20.913+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:45:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:50:25.698+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:45:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:50:27.797+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:45:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:50:27.810+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:45:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:50:25.793624+00:00, run_end_date=2024-07-06 10:50:26.259958+00:00, run_duration=0.466334, state=success, executor_state=success, try_number=1, max_tries=0, job_id=358, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:50:19.132953+00:00, queued_by_job_id=33, pid=39157[0m
[[34m2024-07-06T10:50:27.902+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:50:27.902+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:50:27.903+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:45:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:50:27.905+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:50:27.906+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:45:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:50:27.906+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:50:27.910+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:45:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:50:29.679+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:45:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:50:34.448+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:45:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:50:37.726+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:45:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:50:37.738+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:45:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:50:34.560881+00:00, run_end_date=2024-07-06 10:50:36.324474+00:00, run_duration=1.763593, state=success, executor_state=success, try_number=1, max_tries=0, job_id=359, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:50:27.903768+00:00, queued_by_job_id=33, pid=39211[0m
[[34m2024-07-06T10:50:42.239+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:45:18.573262+00:00: scheduled__2024-07-06T10:45:18.573262+00:00, state:running, queued_at: 2024-07-06 10:50:19.068532+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:50:42.241+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:45:18.573262+00:00, run_id=scheduled__2024-07-06T10:45:18.573262+00:00, run_start_date=2024-07-06 10:50:19.093610+00:00, run_end_date=2024-07-06 10:50:42.240857+00:00, run_duration=23.147247, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:45:18.573262+00:00, data_interval_end=2024-07-06 10:50:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:50:42.249+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:50:18.573262+00:00, run_after=2024-07-06T10:55:18.573262+00:00[0m
[[34m2024-07-06T10:53:11.644+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T10:55:19.619+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:55:18.573262+00:00, run_after=2024-07-06T11:00:18.573262+00:00[0m
[[34m2024-07-06T10:55:19.665+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:55:19.666+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:55:19.666+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:55:19.668+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:55:19.668+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:50:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T10:55:19.669+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:55:19.671+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:55:21.391+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:50:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T10:55:26.135+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:50:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:55:28.392+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:50:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:55:28.409+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:50:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:55:26.255392+00:00, run_end_date=2024-07-06 10:55:26.752735+00:00, run_duration=0.497343, state=success, executor_state=success, try_number=1, max_tries=0, job_id=360, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 10:55:19.667032+00:00, queued_by_job_id=33, pid=40895[0m
[[34m2024-07-06T10:55:32.943+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:55:32.944+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T10:55:32.944+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:50:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T10:55:32.946+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T10:55:32.947+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:50:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T10:55:32.947+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:55:32.952+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:50:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T10:55:34.692+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:50:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T10:55:39.509+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:50:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T10:55:42.854+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:50:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T10:55:42.867+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:50:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 10:55:39.603212+00:00, run_end_date=2024-07-06 10:55:41.381928+00:00, run_duration=1.778716, state=success, executor_state=success, try_number=1, max_tries=0, job_id=361, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 10:55:32.945061+00:00, queued_by_job_id=33, pid=40976[0m
[[34m2024-07-06T10:55:47.488+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:50:18.573262+00:00: scheduled__2024-07-06T10:50:18.573262+00:00, state:running, queued_at: 2024-07-06 10:55:19.614225+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T10:55:47.489+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:50:18.573262+00:00, run_id=scheduled__2024-07-06T10:50:18.573262+00:00, run_start_date=2024-07-06 10:55:19.633848+00:00, run_end_date=2024-07-06 10:55:47.489293+00:00, run_duration=27.855445, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:50:18.573262+00:00, data_interval_end=2024-07-06 10:55:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T10:55:47.493+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T10:55:18.573262+00:00, run_after=2024-07-06T11:00:18.573262+00:00[0m
[[34m2024-07-06T10:58:11.699+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:00:19.493+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:00:18.573262+00:00, run_after=2024-07-06T11:05:18.573262+00:00[0m
[[34m2024-07-06T11:00:19.552+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:00:19.552+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:00:19.552+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:00:19.555+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:00:19.556+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:55:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:00:19.556+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:00:19.560+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T10:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:00:21.310+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:55:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:00:26.130+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T10:55:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:00:28.226+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T10:55:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:00:28.238+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T10:55:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:00:26.233687+00:00, run_end_date=2024-07-06 11:00:26.704544+00:00, run_duration=0.470857, state=success, executor_state=success, try_number=1, max_tries=0, job_id=362, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:00:19.553583+00:00, queued_by_job_id=33, pid=42302[0m
[[34m2024-07-06T11:00:32.575+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:00:32.576+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:00:32.576+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:55:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:00:32.578+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:00:32.579+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:55:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:00:32.579+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:00:32.587+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T10:55:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:00:34.278+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T10:55:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:00:38.871+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T10:55:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:00:42.042+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T10:55:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:00:42.062+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T10:55:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:00:38.966058+00:00, run_end_date=2024-07-06 11:00:40.530456+00:00, run_duration=1.564398, state=success, executor_state=success, try_number=1, max_tries=0, job_id=363, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:00:32.577362+00:00, queued_by_job_id=33, pid=42350[0m
[[34m2024-07-06T11:00:42.135+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 10:55:18.573262+00:00: scheduled__2024-07-06T10:55:18.573262+00:00, state:running, queued_at: 2024-07-06 11:00:19.478232+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:00:42.136+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 10:55:18.573262+00:00, run_id=scheduled__2024-07-06T10:55:18.573262+00:00, run_start_date=2024-07-06 11:00:19.512276+00:00, run_end_date=2024-07-06 11:00:42.136064+00:00, run_duration=22.623788, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 10:55:18.573262+00:00, data_interval_end=2024-07-06 11:00:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:00:42.140+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:00:18.573262+00:00, run_after=2024-07-06T11:05:18.573262+00:00[0m
[[34m2024-07-06T11:03:11.753+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:05:19.064+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:05:18.573262+00:00, run_after=2024-07-06T11:10:18.573262+00:00[0m
[[34m2024-07-06T11:05:19.119+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:05:19.120+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:05:19.120+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:05:19.122+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:05:19.123+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:00:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:05:19.123+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:05:19.127+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:05:20.872+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:00:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:05:25.589+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:00:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:05:27.862+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:00:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:05:27.876+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:00:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:05:25.714610+00:00, run_end_date=2024-07-06 11:05:26.183598+00:00, run_duration=0.468988, state=success, executor_state=success, try_number=1, max_tries=0, job_id=364, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:05:19.121321+00:00, queued_by_job_id=33, pid=43346[0m
[[34m2024-07-06T11:05:32.718+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:05:32.719+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:05:32.719+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:00:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:05:32.721+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:05:32.722+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:00:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:05:32.722+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:05:32.727+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:00:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:05:34.486+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:00:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:05:39.397+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:00:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:05:42.879+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:00:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:05:42.892+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:00:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:05:39.497242+00:00, run_end_date=2024-07-06 11:05:41.364962+00:00, run_duration=1.86772, state=success, executor_state=success, try_number=1, max_tries=0, job_id=365, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:05:32.720373+00:00, queued_by_job_id=33, pid=43396[0m
[[34m2024-07-06T11:05:47.642+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:00:18.573262+00:00: scheduled__2024-07-06T11:00:18.573262+00:00, state:running, queued_at: 2024-07-06 11:05:19.059395+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:05:47.643+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:00:18.573262+00:00, run_id=scheduled__2024-07-06T11:00:18.573262+00:00, run_start_date=2024-07-06 11:05:19.083639+00:00, run_end_date=2024-07-06 11:05:47.643599+00:00, run_duration=28.55996, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:00:18.573262+00:00, data_interval_end=2024-07-06 11:05:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:05:47.647+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:05:18.573262+00:00, run_after=2024-07-06T11:10:18.573262+00:00[0m
[[34m2024-07-06T11:08:11.814+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:10:19.913+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:10:18.573262+00:00, run_after=2024-07-06T11:15:18.573262+00:00[0m
[[34m2024-07-06T11:10:19.965+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:10:19.966+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:10:19.966+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:10:19.968+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:10:19.969+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:05:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:10:19.969+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:10:19.977+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:10:21.885+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:05:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:10:27.533+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:05:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:10:30.360+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:05:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:10:30.373+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:05:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:10:27.648549+00:00, run_end_date=2024-07-06 11:10:28.227473+00:00, run_duration=0.578924, state=success, executor_state=success, try_number=1, max_tries=0, job_id=366, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:10:19.967108+00:00, queued_by_job_id=33, pid=44395[0m
[[34m2024-07-06T11:10:35.727+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:10:35.728+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:10:35.728+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:05:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:10:35.730+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:10:35.731+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:05:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:10:35.732+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:10:35.736+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:05:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:10:37.665+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:05:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:10:42.992+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:05:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:10:46.920+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:05:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:10:46.935+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:05:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:10:43.115129+00:00, run_end_date=2024-07-06 11:10:45.014658+00:00, run_duration=1.899529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=367, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:10:35.729080+00:00, queued_by_job_id=33, pid=44447[0m
[[34m2024-07-06T11:10:47.026+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:05:18.573262+00:00: scheduled__2024-07-06T11:05:18.573262+00:00, state:running, queued_at: 2024-07-06 11:10:19.907191+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:10:47.026+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:05:18.573262+00:00, run_id=scheduled__2024-07-06T11:05:18.573262+00:00, run_start_date=2024-07-06 11:10:19.928767+00:00, run_end_date=2024-07-06 11:10:47.026795+00:00, run_duration=27.098028, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:05:18.573262+00:00, data_interval_end=2024-07-06 11:10:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:10:47.032+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:10:18.573262+00:00, run_after=2024-07-06T11:15:18.573262+00:00[0m
[[34m2024-07-06T11:13:15.795+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:15:19.915+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:15:18.573262+00:00, run_after=2024-07-06T11:20:18.573262+00:00[0m
[[34m2024-07-06T11:15:19.963+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:15:19.963+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:15:19.963+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:15:19.965+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:15:19.966+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:10:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:15:19.966+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:15:19.970+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:15:21.580+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:10:18.573262+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:15:26.303+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:10:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:15:28.253+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:10:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:15:28.265+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:10:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:15:26.409551+00:00, run_end_date=2024-07-06 11:15:26.884293+00:00, run_duration=0.474742, state=success, executor_state=success, try_number=1, max_tries=0, job_id=368, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:15:19.964225+00:00, queued_by_job_id=33, pid=45356[0m
[[34m2024-07-06T11:15:28.347+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:15:28.347+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:15:28.347+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:10:18.573262+00:00 [scheduled]>[0m
[[34m2024-07-06T11:15:28.349+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:15:28.350+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:10:18.573262+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:15:28.350+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:15:28.353+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:10:18.573262+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:15:29.945+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:10:18.573262+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:15:34.469+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:10:18.573262+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:15:37.792+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:10:18.573262+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:15:37.804+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:10:18.573262+00:00, map_index=-1, run_start_date=2024-07-06 11:15:34.577239+00:00, run_end_date=2024-07-06 11:15:36.402045+00:00, run_duration=1.824806, state=success, executor_state=success, try_number=1, max_tries=0, job_id=369, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:15:28.348462+00:00, queued_by_job_id=33, pid=45386[0m
[[34m2024-07-06T11:15:42.100+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:10:18.573262+00:00: scheduled__2024-07-06T11:10:18.573262+00:00, state:running, queued_at: 2024-07-06 11:15:19.909794+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:15:42.101+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:10:18.573262+00:00, run_id=scheduled__2024-07-06T11:10:18.573262+00:00, run_start_date=2024-07-06 11:15:19.929219+00:00, run_end_date=2024-07-06 11:15:42.101032+00:00, run_duration=22.171813, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:10:18.573262+00:00, data_interval_end=2024-07-06 11:15:18.573262+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:15:42.104+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:15:18.573262+00:00, run_after=2024-07-06T11:20:18.573262+00:00[0m
[[34m2024-07-06T11:18:15.921+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:20:24.107+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:20:23.977575+00:00, run_after=2024-07-06T11:25:23.977575+00:00[0m
[[34m2024-07-06T11:20:24.171+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:15:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:20:24.172+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:20:24.172+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:15:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:20:24.178+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:20:24.179+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:15:23.977575+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:20:24.179+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:15:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:20:24.182+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:15:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:20:25.907+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:15:23.977575+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:20:31.067+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:15:23.977575+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:20:33.339+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:15:23.977575+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:20:33.351+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:15:23.977575+00:00, map_index=-1, run_start_date=2024-07-06 11:20:31.176699+00:00, run_end_date=2024-07-06 11:20:31.675543+00:00, run_duration=0.498844, state=success, executor_state=success, try_number=1, max_tries=0, job_id=370, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:20:24.173058+00:00, queued_by_job_id=33, pid=46315[0m
[[34m2024-07-06T11:20:37.798+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:15:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:20:37.798+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:20:37.799+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:15:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:20:37.801+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:20:37.802+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:15:23.977575+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:20:37.802+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:15:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:20:37.807+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:15:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:20:39.704+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:15:23.977575+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:20:45.244+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:15:23.977575+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:20:49.673+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:15:23.977575+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:20:49.688+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:15:23.977575+00:00, map_index=-1, run_start_date=2024-07-06 11:20:45.350482+00:00, run_end_date=2024-07-06 11:20:47.234841+00:00, run_duration=1.884359, state=success, executor_state=success, try_number=1, max_tries=0, job_id=371, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:20:37.799734+00:00, queued_by_job_id=33, pid=46366[0m
[[34m2024-07-06T11:20:49.768+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:15:23.977575+00:00: scheduled__2024-07-06T11:15:23.977575+00:00, state:running, queued_at: 2024-07-06 11:20:24.100244+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:20:49.769+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:15:23.977575+00:00, run_id=scheduled__2024-07-06T11:15:23.977575+00:00, run_start_date=2024-07-06 11:20:24.127196+00:00, run_end_date=2024-07-06 11:20:49.768869+00:00, run_duration=25.641673, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:15:23.977575+00:00, data_interval_end=2024-07-06 11:20:23.977575+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:20:49.772+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:20:23.977575+00:00, run_after=2024-07-06T11:25:23.977575+00:00[0m
[[34m2024-07-06T11:23:15.974+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:25:24.504+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:25:23.977575+00:00, run_after=2024-07-06T11:30:23.977575+00:00[0m
[[34m2024-07-06T11:25:24.564+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:20:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:25:24.564+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:25:24.565+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:20:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:25:24.567+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:25:24.567+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:20:23.977575+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:25:24.568+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:20:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:25:24.572+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:20:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:25:26.311+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:20:23.977575+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:25:31.830+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:20:23.977575+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:25:34.053+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:20:23.977575+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:25:34.071+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:20:23.977575+00:00, map_index=-1, run_start_date=2024-07-06 11:25:31.934782+00:00, run_end_date=2024-07-06 11:25:32.421338+00:00, run_duration=0.486556, state=success, executor_state=success, try_number=1, max_tries=0, job_id=372, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:25:24.565855+00:00, queued_by_job_id=33, pid=47354[0m
[[34m2024-07-06T11:25:34.149+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:20:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:25:34.149+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:25:34.150+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:20:23.977575+00:00 [scheduled]>[0m
[[34m2024-07-06T11:25:34.151+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:25:34.152+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:20:23.977575+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:25:34.152+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:20:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:25:34.156+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:20:23.977575+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:25:35.696+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:20:23.977575+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:25:40.219+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:20:23.977575+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:25:43.676+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:20:23.977575+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:25:43.690+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:20:23.977575+00:00, map_index=-1, run_start_date=2024-07-06 11:25:40.316139+00:00, run_end_date=2024-07-06 11:25:42.136040+00:00, run_duration=1.819901, state=success, executor_state=success, try_number=1, max_tries=0, job_id=373, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:25:34.150643+00:00, queued_by_job_id=33, pid=47382[0m
[[34m2024-07-06T11:25:48.007+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:20:23.977575+00:00: scheduled__2024-07-06T11:20:23.977575+00:00, state:running, queued_at: 2024-07-06 11:25:24.495024+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:25:48.007+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:20:23.977575+00:00, run_id=scheduled__2024-07-06T11:20:23.977575+00:00, run_start_date=2024-07-06 11:25:24.521769+00:00, run_end_date=2024-07-06 11:25:48.007469+00:00, run_duration=23.4857, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:20:23.977575+00:00, data_interval_end=2024-07-06 11:25:23.977575+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:25:48.010+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:25:23.977575+00:00, run_after=2024-07-06T11:30:23.977575+00:00[0m
[[34m2024-07-06T11:28:16.031+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:30:39.537+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:30:33.764478+00:00, run_after=2024-07-06T11:35:33.764478+00:00[0m
[[34m2024-07-06T11:30:39.598+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:25:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:30:39.598+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:30:39.598+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:25:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:30:39.601+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:30:39.601+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:25:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:30:39.602+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:25:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:30:39.605+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:25:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:30:41.477+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:25:33.764478+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:30:45.950+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:25:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:30:48.115+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:25:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:30:48.129+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:25:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:30:46.064030+00:00, run_end_date=2024-07-06 11:30:46.642415+00:00, run_duration=0.578385, state=success, executor_state=success, try_number=1, max_tries=0, job_id=374, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:30:39.599545+00:00, queued_by_job_id=33, pid=49114[0m
[[34m2024-07-06T11:30:48.231+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:25:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:30:48.231+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:30:48.232+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:25:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:30:48.233+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:30:48.234+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:25:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:30:48.234+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:25:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:30:48.238+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:25:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:30:49.836+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:25:33.764478+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:30:54.253+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:25:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:30:57.445+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:25:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:30:57.461+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:25:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:30:54.359974+00:00, run_end_date=2024-07-06 11:30:56.095742+00:00, run_duration=1.735768, state=success, executor_state=success, try_number=1, max_tries=0, job_id=375, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:30:48.232557+00:00, queued_by_job_id=33, pid=49145[0m
[[34m2024-07-06T11:30:57.548+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:25:33.764478+00:00: scheduled__2024-07-06T11:25:33.764478+00:00, state:running, queued_at: 2024-07-06 11:30:39.528873+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:30:57.549+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:25:33.764478+00:00, run_id=scheduled__2024-07-06T11:25:33.764478+00:00, run_start_date=2024-07-06 11:30:39.553579+00:00, run_end_date=2024-07-06 11:30:57.548914+00:00, run_duration=17.995335, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:25:33.764478+00:00, data_interval_end=2024-07-06 11:30:33.764478+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:30:57.552+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:30:33.764478+00:00, run_after=2024-07-06T11:35:33.764478+00:00[0m
[[34m2024-07-06T11:33:16.087+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:35:34.811+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:35:33.764478+00:00, run_after=2024-07-06T11:40:33.764478+00:00[0m
[[34m2024-07-06T11:35:34.860+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:30:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:35:34.860+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:35:34.860+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:30:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:35:34.862+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:35:34.863+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:30:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:35:34.863+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:30:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:35:34.866+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:30:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:35:36.585+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:30:33.764478+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:35:41.230+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:30:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:35:43.299+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:30:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:35:43.316+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:30:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:35:41.325470+00:00, run_end_date=2024-07-06 11:35:41.783093+00:00, run_duration=0.457623, state=success, executor_state=success, try_number=1, max_tries=0, job_id=376, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:35:34.861199+00:00, queued_by_job_id=33, pid=50090[0m
[[34m2024-07-06T11:35:48.042+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:30:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:35:48.042+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:35:48.042+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:30:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:35:48.044+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:35:48.045+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:30:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:35:48.045+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:30:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:35:48.049+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:30:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:35:49.635+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:30:33.764478+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:35:54.386+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:30:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:35:57.775+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:30:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:35:57.790+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:30:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:35:54.483974+00:00, run_end_date=2024-07-06 11:35:56.291916+00:00, run_duration=1.807942, state=success, executor_state=success, try_number=1, max_tries=0, job_id=377, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:35:48.043419+00:00, queued_by_job_id=33, pid=50141[0m
[[34m2024-07-06T11:36:01.985+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:30:33.764478+00:00: scheduled__2024-07-06T11:30:33.764478+00:00, state:running, queued_at: 2024-07-06 11:35:34.806385+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:36:01.985+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:30:33.764478+00:00, run_id=scheduled__2024-07-06T11:30:33.764478+00:00, run_start_date=2024-07-06 11:35:34.830302+00:00, run_end_date=2024-07-06 11:36:01.985647+00:00, run_duration=27.155345, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:30:33.764478+00:00, data_interval_end=2024-07-06 11:35:33.764478+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:36:01.989+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:35:33.764478+00:00, run_after=2024-07-06T11:40:33.764478+00:00[0m
[[34m2024-07-06T11:38:19.469+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:40:37.310+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:40:33.764478+00:00, run_after=2024-07-06T11:45:33.764478+00:00[0m
[[34m2024-07-06T11:40:37.357+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:35:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:40:37.358+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:40:37.358+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:35:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:40:37.360+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:40:37.360+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:35:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:40:37.361+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:35:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:40:37.364+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:35:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:40:38.929+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:35:33.764478+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:40:43.204+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:35:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:40:45.096+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:35:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:40:45.102+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:35:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:40:43.315068+00:00, run_end_date=2024-07-06 11:40:43.759614+00:00, run_duration=0.444546, state=success, executor_state=success, try_number=1, max_tries=0, job_id=378, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:40:37.358799+00:00, queued_by_job_id=33, pid=50850[0m
[[34m2024-07-06T11:40:45.184+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:35:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:40:45.185+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:40:45.185+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:35:33.764478+00:00 [scheduled]>[0m
[[34m2024-07-06T11:40:45.187+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:40:45.187+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:35:33.764478+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:40:45.187+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:35:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:40:45.191+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:35:33.764478+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:40:46.763+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:35:33.764478+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:40:51.068+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:35:33.764478+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:40:54.408+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:35:33.764478+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:40:54.423+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:35:33.764478+00:00, map_index=-1, run_start_date=2024-07-06 11:40:51.167341+00:00, run_end_date=2024-07-06 11:40:52.915421+00:00, run_duration=1.74808, state=success, executor_state=success, try_number=1, max_tries=0, job_id=379, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:40:45.185976+00:00, queued_by_job_id=33, pid=50867[0m
[[34m2024-07-06T11:40:54.501+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:35:33.764478+00:00: scheduled__2024-07-06T11:35:33.764478+00:00, state:running, queued_at: 2024-07-06 11:40:37.301640+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:40:54.505+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:35:33.764478+00:00, run_id=scheduled__2024-07-06T11:35:33.764478+00:00, run_start_date=2024-07-06 11:40:37.326282+00:00, run_end_date=2024-07-06 11:40:54.505559+00:00, run_duration=17.179277, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:35:33.764478+00:00, data_interval_end=2024-07-06 11:40:33.764478+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:40:54.514+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:40:33.764478+00:00, run_after=2024-07-06T11:45:33.764478+00:00[0m
[[34m2024-07-06T11:43:20.624+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:45:39.238+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:45:38.815206+00:00, run_after=2024-07-06T11:50:38.815206+00:00[0m
[[34m2024-07-06T11:45:39.296+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:40:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:45:39.296+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:45:39.296+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:40:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:45:39.298+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:45:39.299+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:40:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:45:39.299+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:40:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:45:39.302+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:40:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:45:40.857+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:40:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:45:45.213+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:40:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:45:47.181+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:40:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:45:47.195+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:40:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:45:45.302347+00:00, run_end_date=2024-07-06 11:45:45.771097+00:00, run_duration=0.46875, state=success, executor_state=success, try_number=1, max_tries=0, job_id=380, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:45:39.297096+00:00, queued_by_job_id=33, pid=51482[0m
[[34m2024-07-06T11:45:51.530+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:40:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:45:51.531+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:45:51.531+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:40:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:45:51.533+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:45:51.534+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:40:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:45:51.534+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:40:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:45:51.538+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:40:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:45:53.179+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:40:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:45:57.616+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:40:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:46:00.907+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:40:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:46:00.921+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:40:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:45:57.719825+00:00, run_end_date=2024-07-06 11:45:59.557454+00:00, run_duration=1.837629, state=success, executor_state=success, try_number=1, max_tries=0, job_id=381, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:45:51.531885+00:00, queued_by_job_id=33, pid=51515[0m
[[34m2024-07-06T11:46:00.991+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:40:38.815206+00:00: scheduled__2024-07-06T11:40:38.815206+00:00, state:running, queued_at: 2024-07-06 11:45:39.231452+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:46:00.991+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:40:38.815206+00:00, run_id=scheduled__2024-07-06T11:40:38.815206+00:00, run_start_date=2024-07-06 11:45:39.256257+00:00, run_end_date=2024-07-06 11:46:00.991745+00:00, run_duration=21.735488, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:40:38.815206+00:00, data_interval_end=2024-07-06 11:45:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:46:01.000+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:45:38.815206+00:00, run_after=2024-07-06T11:50:38.815206+00:00[0m
[[34m2024-07-06T11:48:20.677+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:50:39.023+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:50:38.815206+00:00, run_after=2024-07-06T11:55:38.815206+00:00[0m
[[34m2024-07-06T11:50:39.070+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:45:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:50:39.070+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:50:39.070+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:45:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:50:39.072+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:50:39.073+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:45:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:50:39.073+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:45:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:50:39.078+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:45:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:50:40.754+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:45:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:50:45.361+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:45:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:50:47.320+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:45:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:50:47.340+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:45:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:50:45.462318+00:00, run_end_date=2024-07-06 11:50:45.916960+00:00, run_duration=0.454642, state=success, executor_state=success, try_number=1, max_tries=0, job_id=382, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:50:39.071593+00:00, queued_by_job_id=33, pid=52083[0m
[[34m2024-07-06T11:50:47.421+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:45:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:50:47.422+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:50:47.422+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:45:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:50:47.424+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:50:47.424+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:45:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:50:47.425+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:45:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:50:47.428+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:45:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:50:48.978+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:45:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:50:53.330+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:45:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:50:56.592+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:45:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:50:56.601+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:45:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:50:53.456128+00:00, run_end_date=2024-07-06 11:50:55.149438+00:00, run_duration=1.69331, state=success, executor_state=success, try_number=1, max_tries=0, job_id=383, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:50:47.422808+00:00, queued_by_job_id=33, pid=52100[0m
[[34m2024-07-06T11:51:00.798+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:45:38.815206+00:00: scheduled__2024-07-06T11:45:38.815206+00:00, state:running, queued_at: 2024-07-06 11:50:39.017585+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:51:00.799+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:45:38.815206+00:00, run_id=scheduled__2024-07-06T11:45:38.815206+00:00, run_start_date=2024-07-06 11:50:39.038112+00:00, run_end_date=2024-07-06 11:51:00.799175+00:00, run_duration=21.761063, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:45:38.815206+00:00, data_interval_end=2024-07-06 11:50:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:51:00.803+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:50:38.815206+00:00, run_after=2024-07-06T11:55:38.815206+00:00[0m
[[34m2024-07-06T11:53:24.365+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T11:55:39.302+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:55:38.815206+00:00, run_after=2024-07-06T12:00:38.815206+00:00[0m
[[34m2024-07-06T11:55:39.351+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:50:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:55:39.351+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:55:39.351+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:50:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:55:39.353+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:55:39.354+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:50:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T11:55:39.354+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:50:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:55:39.357+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:50:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:55:41.071+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:50:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T11:55:45.802+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:50:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:55:47.739+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:50:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:55:47.753+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:50:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:55:45.910638+00:00, run_end_date=2024-07-06 11:55:46.367283+00:00, run_duration=0.456645, state=success, executor_state=success, try_number=1, max_tries=0, job_id=384, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 11:55:39.352411+00:00, queued_by_job_id=33, pid=52726[0m
[[34m2024-07-06T11:55:51.760+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:50:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:55:51.760+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T11:55:51.761+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:50:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T11:55:51.763+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T11:55:51.764+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:50:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T11:55:51.764+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:50:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:55:51.767+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:50:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T11:55:53.336+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:50:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T11:55:57.845+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:50:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T11:56:01.039+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:50:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T11:56:01.055+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:50:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 11:55:57.949584+00:00, run_end_date=2024-07-06 11:55:59.672698+00:00, run_duration=1.723114, state=success, executor_state=success, try_number=1, max_tries=0, job_id=385, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 11:55:51.761839+00:00, queued_by_job_id=33, pid=52756[0m
[[34m2024-07-06T11:56:01.131+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:50:38.815206+00:00: scheduled__2024-07-06T11:50:38.815206+00:00, state:running, queued_at: 2024-07-06 11:55:39.297039+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T11:56:01.132+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:50:38.815206+00:00, run_id=scheduled__2024-07-06T11:50:38.815206+00:00, run_start_date=2024-07-06 11:55:39.319040+00:00, run_end_date=2024-07-06 11:56:01.132007+00:00, run_duration=21.812967, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:50:38.815206+00:00, data_interval_end=2024-07-06 11:55:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T11:56:01.135+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T11:55:38.815206+00:00, run_after=2024-07-06T12:00:38.815206+00:00[0m
[[34m2024-07-06T11:58:24.411+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:00:39.352+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:00:38.815206+00:00, run_after=2024-07-06T12:05:38.815206+00:00[0m
[[34m2024-07-06T12:00:39.415+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:55:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:00:39.416+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:00:39.416+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:55:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:00:39.418+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:00:39.419+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:55:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:00:39.419+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:55:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:00:39.422+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T11:55:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:00:41.127+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:55:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:00:45.638+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T11:55:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:00:47.621+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T11:55:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:00:47.634+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T11:55:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:00:45.731893+00:00, run_end_date=2024-07-06 12:00:46.186683+00:00, run_duration=0.45479, state=success, executor_state=success, try_number=1, max_tries=0, job_id=386, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:00:39.417320+00:00, queued_by_job_id=33, pid=53337[0m
[[34m2024-07-06T12:00:51.843+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:55:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:00:51.844+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:00:51.844+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:55:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:00:51.846+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:00:51.847+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:55:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:00:51.847+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:55:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:00:51.851+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T11:55:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:00:53.473+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T11:55:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:00:58.001+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T11:55:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:01:01.089+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T11:55:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:01:01.106+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T11:55:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:00:58.101376+00:00, run_end_date=2024-07-06 12:00:59.752431+00:00, run_duration=1.651055, state=success, executor_state=success, try_number=1, max_tries=0, job_id=387, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:00:51.845297+00:00, queued_by_job_id=33, pid=53364[0m
[[34m2024-07-06T12:01:05.199+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 11:55:38.815206+00:00: scheduled__2024-07-06T11:55:38.815206+00:00, state:running, queued_at: 2024-07-06 12:00:39.345444+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:01:05.200+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 11:55:38.815206+00:00, run_id=scheduled__2024-07-06T11:55:38.815206+00:00, run_start_date=2024-07-06 12:00:39.377940+00:00, run_end_date=2024-07-06 12:01:05.200412+00:00, run_duration=25.822472, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 11:55:38.815206+00:00, data_interval_end=2024-07-06 12:00:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:01:05.204+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:00:38.815206+00:00, run_after=2024-07-06T12:05:38.815206+00:00[0m
[[34m2024-07-06T12:03:24.468+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:05:39.515+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:05:38.815206+00:00, run_after=2024-07-06T12:10:38.815206+00:00[0m
[[34m2024-07-06T12:05:39.561+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:00:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:05:39.561+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:05:39.561+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:00:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:05:39.563+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:05:39.564+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:00:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:05:39.564+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:00:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:05:39.567+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:00:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:05:41.313+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:00:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:05:45.938+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:00:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:05:47.958+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:00:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:05:47.973+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:00:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:05:46.036554+00:00, run_end_date=2024-07-06 12:05:46.584798+00:00, run_duration=0.548244, state=success, executor_state=success, try_number=1, max_tries=0, job_id=388, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:05:39.562382+00:00, queued_by_job_id=33, pid=54651[0m
[[34m2024-07-06T12:05:52.216+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:00:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:05:52.216+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:05:52.217+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:00:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:05:52.219+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:05:52.219+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:00:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:05:52.220+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:00:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:05:52.225+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:00:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:05:53.947+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:00:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:05:58.520+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:00:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:06:01.627+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:00:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:06:01.643+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:00:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:05:58.614529+00:00, run_end_date=2024-07-06 12:06:00.141487+00:00, run_duration=1.526958, state=success, executor_state=success, try_number=1, max_tries=0, job_id=389, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:05:52.217817+00:00, queued_by_job_id=33, pid=54685[0m
[[34m2024-07-06T12:06:01.714+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:00:38.815206+00:00: scheduled__2024-07-06T12:00:38.815206+00:00, state:running, queued_at: 2024-07-06 12:05:39.508475+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:06:01.715+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:00:38.815206+00:00, run_id=scheduled__2024-07-06T12:00:38.815206+00:00, run_start_date=2024-07-06 12:05:39.530477+00:00, run_end_date=2024-07-06 12:06:01.715411+00:00, run_duration=22.184934, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:00:38.815206+00:00, data_interval_end=2024-07-06 12:05:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:06:01.719+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:05:38.815206+00:00, run_after=2024-07-06T12:10:38.815206+00:00[0m
[[34m2024-07-06T12:08:27.244+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:10:39.519+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:10:38.815206+00:00, run_after=2024-07-06T12:15:38.815206+00:00[0m
[[34m2024-07-06T12:10:39.572+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:05:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:10:39.573+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:10:39.573+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:05:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:10:39.575+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:10:39.576+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:05:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:10:39.576+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:05:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:10:39.579+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:05:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:10:41.350+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:05:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:10:46.004+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:05:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:10:48.259+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:05:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:10:48.277+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:05:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:10:46.119509+00:00, run_end_date=2024-07-06 12:10:46.612825+00:00, run_duration=0.493316, state=success, executor_state=success, try_number=1, max_tries=0, job_id=390, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:10:39.574097+00:00, queued_by_job_id=33, pid=55299[0m
[[34m2024-07-06T12:10:52.921+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:05:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:10:52.922+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:10:52.922+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:05:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:10:52.924+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:10:52.925+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:05:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:10:52.925+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:05:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:10:52.929+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:05:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:10:54.672+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:05:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:10:59.232+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:05:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:11:02.527+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:05:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:11:02.542+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:05:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:10:59.331899+00:00, run_end_date=2024-07-06 12:11:01.071618+00:00, run_duration=1.739719, state=success, executor_state=success, try_number=1, max_tries=0, job_id=391, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:10:52.923271+00:00, queued_by_job_id=33, pid=55334[0m
[[34m2024-07-06T12:11:06.751+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:05:38.815206+00:00: scheduled__2024-07-06T12:05:38.815206+00:00, state:running, queued_at: 2024-07-06 12:10:39.504660+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:11:06.752+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:05:38.815206+00:00, run_id=scheduled__2024-07-06T12:05:38.815206+00:00, run_start_date=2024-07-06 12:10:39.536970+00:00, run_end_date=2024-07-06 12:11:06.752509+00:00, run_duration=27.215539, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:05:38.815206+00:00, data_interval_end=2024-07-06 12:10:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:11:06.761+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:10:38.815206+00:00, run_after=2024-07-06T12:15:38.815206+00:00[0m
[[34m2024-07-06T12:13:27.293+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:15:39.118+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:15:38.815206+00:00, run_after=2024-07-06T12:20:38.815206+00:00[0m
[[34m2024-07-06T12:15:39.179+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:10:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:15:39.179+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:15:39.179+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:10:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:15:39.181+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:15:39.182+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:10:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:15:39.183+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:10:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:15:39.186+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:10:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:15:40.960+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:10:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:15:45.459+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:10:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:15:47.409+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:10:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:15:47.425+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:10:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:15:45.568439+00:00, run_end_date=2024-07-06 12:15:46.021593+00:00, run_duration=0.453154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=392, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:15:39.180478+00:00, queued_by_job_id=33, pid=55934[0m
[[34m2024-07-06T12:15:51.541+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:10:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:15:51.541+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:15:51.541+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:10:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:15:51.543+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:15:51.544+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:10:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:15:51.544+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:10:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:15:51.548+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:10:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:15:53.112+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:10:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:15:57.561+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:10:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:16:00.852+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:10:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:16:00.866+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:10:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:15:57.657447+00:00, run_end_date=2024-07-06 12:15:59.394498+00:00, run_duration=1.737051, state=success, executor_state=success, try_number=1, max_tries=0, job_id=393, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:15:51.542256+00:00, queued_by_job_id=33, pid=55960[0m
[[34m2024-07-06T12:16:00.956+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:10:38.815206+00:00: scheduled__2024-07-06T12:10:38.815206+00:00, state:running, queued_at: 2024-07-06 12:15:39.110912+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:16:00.957+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:10:38.815206+00:00, run_id=scheduled__2024-07-06T12:10:38.815206+00:00, run_start_date=2024-07-06 12:15:39.141242+00:00, run_end_date=2024-07-06 12:16:00.957424+00:00, run_duration=21.816182, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:10:38.815206+00:00, data_interval_end=2024-07-06 12:15:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:16:00.962+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:15:38.815206+00:00, run_after=2024-07-06T12:20:38.815206+00:00[0m
[[34m2024-07-06T12:18:29.105+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:20:39.798+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:20:38.815206+00:00, run_after=2024-07-06T12:25:38.815206+00:00[0m
[[34m2024-07-06T12:20:39.850+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:15:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:20:39.850+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:20:39.850+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:15:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:20:39.852+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:20:39.853+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:15:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:20:39.853+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:15:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:20:39.857+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:15:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:20:41.635+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:15:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:20:46.427+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:15:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:20:48.426+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:15:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:20:48.441+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:15:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:20:46.527095+00:00, run_end_date=2024-07-06 12:20:46.997557+00:00, run_duration=0.470462, state=success, executor_state=success, try_number=1, max_tries=0, job_id=394, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:20:39.851311+00:00, queued_by_job_id=33, pid=56503[0m
[[34m2024-07-06T12:20:52.961+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:15:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:20:52.962+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:20:52.962+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:15:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:20:52.964+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:20:52.965+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:15:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:20:52.965+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:15:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:20:52.969+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:15:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:20:54.680+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:15:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:20:59.696+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:15:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:21:03.141+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:15:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:21:03.158+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:15:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:20:59.809419+00:00, run_end_date=2024-07-06 12:21:01.613234+00:00, run_duration=1.803815, state=success, executor_state=success, try_number=1, max_tries=0, job_id=395, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:20:52.963443+00:00, queued_by_job_id=33, pid=56534[0m
[[34m2024-07-06T12:21:08.070+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:15:38.815206+00:00: scheduled__2024-07-06T12:15:38.815206+00:00, state:running, queued_at: 2024-07-06 12:20:39.791944+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:21:08.071+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:15:38.815206+00:00, run_id=scheduled__2024-07-06T12:15:38.815206+00:00, run_start_date=2024-07-06 12:20:39.813294+00:00, run_end_date=2024-07-06 12:21:08.071257+00:00, run_duration=28.257963, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:15:38.815206+00:00, data_interval_end=2024-07-06 12:20:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:21:08.076+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:20:38.815206+00:00, run_after=2024-07-06T12:25:38.815206+00:00[0m
[[34m2024-07-06T12:23:29.151+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:25:39.913+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:25:38.815206+00:00, run_after=2024-07-06T12:30:38.815206+00:00[0m
[[34m2024-07-06T12:25:39.963+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:20:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:25:39.963+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:25:39.963+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:20:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:25:39.965+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:25:39.966+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:20:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:25:39.966+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:20:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:25:39.975+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:20:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:25:41.633+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:20:38.815206+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:25:46.365+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:20:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:25:48.306+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:20:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:25:48.321+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:20:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:25:46.466276+00:00, run_end_date=2024-07-06 12:25:46.921863+00:00, run_duration=0.455587, state=success, executor_state=success, try_number=1, max_tries=0, job_id=396, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:25:39.964465+00:00, queued_by_job_id=33, pid=57042[0m
[[34m2024-07-06T12:25:52.442+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:20:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:25:52.443+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:25:52.443+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:20:38.815206+00:00 [scheduled]>[0m
[[34m2024-07-06T12:25:52.444+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:25:52.445+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:20:38.815206+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:25:52.445+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:20:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:25:52.448+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:20:38.815206+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:25:54.000+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:20:38.815206+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:25:58.503+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:20:38.815206+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:26:01.695+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:20:38.815206+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:26:01.712+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:20:38.815206+00:00, map_index=-1, run_start_date=2024-07-06 12:25:58.596781+00:00, run_end_date=2024-07-06 12:26:00.304646+00:00, run_duration=1.707865, state=success, executor_state=success, try_number=1, max_tries=0, job_id=397, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:25:52.443691+00:00, queued_by_job_id=33, pid=57070[0m
[[34m2024-07-06T12:26:05.817+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:20:38.815206+00:00: scheduled__2024-07-06T12:20:38.815206+00:00, state:running, queued_at: 2024-07-06 12:25:39.908393+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:26:05.818+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:20:38.815206+00:00, run_id=scheduled__2024-07-06T12:20:38.815206+00:00, run_start_date=2024-07-06 12:25:39.928401+00:00, run_end_date=2024-07-06 12:26:05.818389+00:00, run_duration=25.889988, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:20:38.815206+00:00, data_interval_end=2024-07-06 12:25:38.815206+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:26:05.822+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:25:38.815206+00:00, run_after=2024-07-06T12:30:38.815206+00:00[0m
[[34m2024-07-06T12:28:29.198+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:30:44.607+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:30:43.420772+00:00, run_after=2024-07-06T12:35:43.420772+00:00[0m
[[34m2024-07-06T12:30:44.655+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:25:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:30:44.655+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:30:44.655+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:25:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:30:44.657+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:30:44.658+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:25:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:30:44.658+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:25:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:30:44.662+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:25:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:30:46.217+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:25:43.420772+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:30:50.708+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:25:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:30:52.562+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:25:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:30:52.583+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:25:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:30:50.798706+00:00, run_end_date=2024-07-06 12:30:51.238238+00:00, run_duration=0.439532, state=success, executor_state=success, try_number=1, max_tries=0, job_id=398, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:30:44.656376+00:00, queued_by_job_id=33, pid=57613[0m
[[34m2024-07-06T12:30:52.677+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:25:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:30:52.678+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:30:52.678+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:25:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:30:52.680+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:30:52.681+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:25:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:30:52.681+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:25:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:30:52.685+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:25:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:30:54.205+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:25:43.420772+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:30:58.462+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:25:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:31:01.513+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:25:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:31:01.520+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:25:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:30:58.576012+00:00, run_end_date=2024-07-06 12:31:00.080604+00:00, run_duration=1.504592, state=success, executor_state=success, try_number=1, max_tries=0, job_id=399, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:30:52.679147+00:00, queued_by_job_id=33, pid=57635[0m
[[34m2024-07-06T12:31:01.580+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:25:43.420772+00:00: scheduled__2024-07-06T12:25:43.420772+00:00, state:running, queued_at: 2024-07-06 12:30:44.600703+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:31:01.581+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:25:43.420772+00:00, run_id=scheduled__2024-07-06T12:25:43.420772+00:00, run_start_date=2024-07-06 12:30:44.622173+00:00, run_end_date=2024-07-06 12:31:01.580936+00:00, run_duration=16.958763, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:25:43.420772+00:00, data_interval_end=2024-07-06 12:30:43.420772+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:31:01.583+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:30:43.420772+00:00, run_after=2024-07-06T12:35:43.420772+00:00[0m
[[34m2024-07-06T12:33:29.255+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:35:46.520+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:35:43.420772+00:00, run_after=2024-07-06T12:40:43.420772+00:00[0m
[[34m2024-07-06T12:35:46.568+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:30:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:35:46.568+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:35:46.569+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:30:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:35:46.571+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:35:46.571+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:30:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:35:46.571+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:30:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:35:46.575+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:30:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:35:48.187+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:30:43.420772+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:35:52.667+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:30:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:35:54.585+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:30:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:35:54.597+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:30:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:35:52.756670+00:00, run_end_date=2024-07-06 12:35:53.216612+00:00, run_duration=0.459942, state=success, executor_state=success, try_number=1, max_tries=0, job_id=400, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:35:46.569715+00:00, queued_by_job_id=33, pid=58235[0m
[[34m2024-07-06T12:35:58.955+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:30:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:35:58.955+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:35:58.956+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:30:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:35:58.958+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:35:58.958+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:30:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:35:58.959+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:30:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:35:58.963+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:30:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:36:00.608+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:30:43.420772+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:36:05.031+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:30:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:36:08.226+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:30:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:36:08.233+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:30:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:36:05.124135+00:00, run_end_date=2024-07-06 12:36:06.832802+00:00, run_duration=1.708667, state=success, executor_state=success, try_number=1, max_tries=0, job_id=401, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:35:58.956770+00:00, queued_by_job_id=33, pid=58268[0m
[[34m2024-07-06T12:36:08.294+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:30:43.420772+00:00: scheduled__2024-07-06T12:30:43.420772+00:00, state:running, queued_at: 2024-07-06 12:35:46.513823+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:36:08.295+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:30:43.420772+00:00, run_id=scheduled__2024-07-06T12:30:43.420772+00:00, run_start_date=2024-07-06 12:35:46.534525+00:00, run_end_date=2024-07-06 12:36:08.294950+00:00, run_duration=21.760425, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:30:43.420772+00:00, data_interval_end=2024-07-06 12:35:43.420772+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:36:08.298+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:35:43.420772+00:00, run_after=2024-07-06T12:40:43.420772+00:00[0m
[[34m2024-07-06T12:38:29.312+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:40:44.858+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:40:43.420772+00:00, run_after=2024-07-06T12:45:43.420772+00:00[0m
[[34m2024-07-06T12:40:44.911+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:35:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:40:44.911+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:40:44.912+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:35:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:40:44.913+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:40:44.914+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:35:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:40:44.914+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:35:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:40:44.918+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:35:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:40:46.571+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:35:43.420772+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:40:51.135+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:35:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:40:53.107+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:35:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:40:53.124+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:35:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:40:51.233023+00:00, run_end_date=2024-07-06 12:40:51.696530+00:00, run_duration=0.463507, state=success, executor_state=success, try_number=1, max_tries=0, job_id=402, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:40:44.912640+00:00, queued_by_job_id=33, pid=58864[0m
[[34m2024-07-06T12:40:53.202+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:35:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:40:53.203+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:40:53.203+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:35:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:40:53.205+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:40:53.205+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:35:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:40:53.206+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:35:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:40:53.209+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:35:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:40:54.770+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:35:43.420772+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:40:59.090+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:35:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:41:02.453+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:35:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:41:02.471+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:35:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:40:59.185178+00:00, run_end_date=2024-07-06 12:41:00.921435+00:00, run_duration=1.736257, state=success, executor_state=success, try_number=1, max_tries=0, job_id=403, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:40:53.203896+00:00, queued_by_job_id=33, pid=58884[0m
[[34m2024-07-06T12:41:06.671+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:35:43.420772+00:00: scheduled__2024-07-06T12:35:43.420772+00:00, state:running, queued_at: 2024-07-06 12:40:44.852292+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:41:06.672+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:35:43.420772+00:00, run_id=scheduled__2024-07-06T12:35:43.420772+00:00, run_start_date=2024-07-06 12:40:44.873490+00:00, run_end_date=2024-07-06 12:41:06.672052+00:00, run_duration=21.798562, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:35:43.420772+00:00, data_interval_end=2024-07-06 12:40:43.420772+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:41:06.675+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:40:43.420772+00:00, run_after=2024-07-06T12:45:43.420772+00:00[0m
[[34m2024-07-06T12:43:32.548+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:45:44.100+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:45:43.420772+00:00, run_after=2024-07-06T12:50:43.420772+00:00[0m
[[34m2024-07-06T12:45:44.151+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:40:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:45:44.151+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:45:44.152+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:40:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:45:44.154+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:45:44.154+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:40:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:45:44.155+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:40:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:45:44.158+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:40:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:45:45.718+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:40:43.420772+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:45:50.223+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:40:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:45:52.107+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:40:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:45:52.120+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:40:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:45:50.318740+00:00, run_end_date=2024-07-06 12:45:50.759996+00:00, run_duration=0.441256, state=success, executor_state=success, try_number=1, max_tries=0, job_id=404, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:45:44.152714+00:00, queued_by_job_id=33, pid=59459[0m
[[34m2024-07-06T12:45:56.638+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:40:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:45:56.639+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:45:56.639+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:40:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:45:56.641+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:45:56.642+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:40:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:45:56.642+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:40:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:45:56.646+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:40:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:45:58.221+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:40:43.420772+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:46:02.606+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:40:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:46:06.254+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:40:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:46:06.277+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:40:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:46:02.703215+00:00, run_end_date=2024-07-06 12:46:04.352062+00:00, run_duration=1.648847, state=success, executor_state=success, try_number=1, max_tries=0, job_id=405, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:45:56.640262+00:00, queued_by_job_id=33, pid=59484[0m
[[34m2024-07-06T12:46:06.399+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:40:43.420772+00:00: scheduled__2024-07-06T12:40:43.420772+00:00, state:running, queued_at: 2024-07-06 12:45:44.093314+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:46:06.399+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:40:43.420772+00:00, run_id=scheduled__2024-07-06T12:40:43.420772+00:00, run_start_date=2024-07-06 12:45:44.115288+00:00, run_end_date=2024-07-06 12:46:06.399689+00:00, run_duration=22.284401, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:40:43.420772+00:00, data_interval_end=2024-07-06 12:45:43.420772+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:46:06.408+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:45:43.420772+00:00, run_after=2024-07-06T12:50:43.420772+00:00[0m
[[34m2024-07-06T12:48:42.730+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:50:44.597+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:50:43.420772+00:00, run_after=2024-07-06T12:55:43.420772+00:00[0m
[[34m2024-07-06T12:50:44.683+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:45:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:50:44.684+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:50:44.685+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:45:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:50:44.690+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:50:44.691+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:45:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-07-06T12:50:44.692+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:45:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:50:44.703+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'extract_data', 'scheduled__2024-07-06T12:45:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:50:47.409+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:45:43.420772+00:00/task_id=extract_data permission to 509
[[34m2024-07-06T12:50:52.641+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.extract_data scheduled__2024-07-06T12:45:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:50:54.852+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='extract_data', run_id='scheduled__2024-07-06T12:45:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:50:54.867+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=extract_data, run_id=scheduled__2024-07-06T12:45:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:50:52.769491+00:00, run_end_date=2024-07-06 12:50:53.480574+00:00, run_duration=0.711083, state=success, executor_state=success, try_number=1, max_tries=0, job_id=406, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-07-06 12:50:44.686595+00:00, queued_by_job_id=33, pid=60638[0m
[[34m2024-07-06T12:51:00.332+0000[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:45:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:51:00.333+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG data_extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-06T12:51:00.334+0000[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:45:43.420772+00:00 [scheduled]>[0m
[[34m2024-07-06T12:51:00.340+0000[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task validate_data because previous state change time has not been saved[0m
[[34m2024-07-06T12:51:00.341+0000[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:45:43.420772+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-06T12:51:00.342+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:45:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:51:00.348+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'data_extract_dag', 'validate_data', 'scheduled__2024-07-06T12:45:43.420772+00:00', '--local', '--subdir', 'DAGS_FOLDER/data_extract_dag.py'][0m
[[34m2024-07-06T12:51:04.839+0000[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /root/MLOpsProject/services/airflow/dags/data_extract_dag.py[0m
Changing /root/MLOpsProject/services/airflow/logs/dag_id=data_extract_dag/run_id=scheduled__2024-07-06T12:45:43.420772+00:00/task_id=validate_data permission to 509
[[34m2024-07-06T12:51:13.218+0000[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: data_extract_dag.validate_data scheduled__2024-07-06T12:45:43.420772+00:00 [queued]> on host ice-lazurite960[0m
[[34m2024-07-06T12:51:18.380+0000[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='data_extract_dag', task_id='validate_data', run_id='scheduled__2024-07-06T12:45:43.420772+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-06T12:51:18.396+0000[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=data_extract_dag, task_id=validate_data, run_id=scheduled__2024-07-06T12:45:43.420772+00:00, map_index=-1, run_start_date=2024-07-06 12:51:13.407216+00:00, run_end_date=2024-07-06 12:51:15.884167+00:00, run_duration=2.476951, state=success, executor_state=success, try_number=1, max_tries=0, job_id=407, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-07-06 12:51:00.335513+00:00, queued_by_job_id=33, pid=60809[0m
[[34m2024-07-06T12:51:27.697+0000[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun data_extract_dag @ 2024-07-06 12:45:43.420772+00:00: scheduled__2024-07-06T12:45:43.420772+00:00, state:running, queued_at: 2024-07-06 12:50:44.571184+00:00. externally triggered: False> successful[0m
[[34m2024-07-06T12:51:27.700+0000[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=data_extract_dag, execution_date=2024-07-06 12:45:43.420772+00:00, run_id=scheduled__2024-07-06T12:45:43.420772+00:00, run_start_date=2024-07-06 12:50:44.626476+00:00, run_end_date=2024-07-06 12:51:27.700343+00:00, run_duration=43.073867, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 12:45:43.420772+00:00, data_interval_end=2024-07-06 12:50:43.420772+00:00, dag_hash=51905d986c5c12bb4b4b2a2129f28eee[0m
[[34m2024-07-06T12:51:27.715+0000[0m] {[34mdag.py:[0m3722} INFO[0m - Setting next_dagrun for data_extract_dag to 2024-07-06T12:50:43.420772+00:00, run_after=2024-07-06T12:55:43.420772+00:00[0m
[[34m2024-07-06T12:53:49.722+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T12:53:49.746+0000[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2024-07-06T12:58:35.816+0000] {manager.py:543} INFO - DAG data_extract_dag is missing and will be deactivated.
[2024-07-06T12:58:35.825+0000] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-07-06T12:58:35.834+0000] {manager.py:557} INFO - Deleted DAG data_extract_dag in serialized_dag table
[[34m2024-07-06T13:01:08.294+0000[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-06T13:01:08.330+0000[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
